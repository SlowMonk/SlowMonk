<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Woongwon Lee</title>
    <description>Reinforce Yourself
</description>
    <link>http://localhost:4003/</link>
    <atom:link href="http://localhost:4003/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Mon, 04 May 2020 16:21:05 +0900</pubDate>
    <lastBuildDate>Mon, 04 May 2020 16:21:05 +0900</lastBuildDate>
    <generator>Jekyll v3.8.6</generator>
    
      <item>
        <title>추천 시스템 Basics</title>
        <description>&lt;h1 id=&quot;추천-시스템-basics&quot;&gt;추천 시스템 Basics&lt;/h1&gt;
&lt;h2 id=&quot;추천-시스템이란&quot;&gt;추천 시스템이란&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;특정 사용자&lt;/strong&gt;가 &lt;strong&gt;관심을 가질 만한 정보&lt;/strong&gt;를 추천하는 것이다. Information filtering의 일종이라고 볼 수 있다. 여기서 말하는 &lt;strong&gt;관심&lt;/strong&gt;은 정의하기 나름이다. 아이템의 종류가 동영상이라면 동영상을 시청하는 것이 관심의 표현일 것이다. 아이템의 종류가 쇼핑에 관련된 제품이라면 구매나 장바구니에 넣는 것이 관심을 나타낸다. 얼마나 관심을 가지는지도 정의하기 나름이 된다. 유튜브의 추천 시스템은 다음 그림과 같이 메인피드에서 내가 볼 만한 동영상을 보여준다. 유튜브에서는 동영상을 시청한 시간을 관심의 척도로 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-6bef25bc-9a2e-45a7-bb61-4e0a639b6082.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.idownloadblog.com/2016/04/26/youtube-new-homepage-design/&quot;&gt;https://www.idownloadblog.com/2016/04/26/youtube-new-homepage-design/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;특정 서비스에서 아이템의 수가 점점 많아지면서 사용자가 정보를 찾기가 어려워진다. 수많은 정보 속에서 사용자가 원하는 정보를 찾을 수 있는 방법에는 두 가지가 있다. &lt;strong&gt;검색&lt;/strong&gt;은 query와 document 사이의 관계를 분석하는 것이라면 &lt;strong&gt;추천&lt;/strong&gt;은 user와 item 사이의 관계를 분석하는 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;검색&lt;/strong&gt; : 사용자가 (관심 있는 아이템에 대한) query를 던지면 query와 관련이 많은 document list를 반환&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;추천&lt;/strong&gt; : 사용자의 이전 행동을 기반으로 관심 있을만한  아이템 리스트를 반환&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;query를 user라고 보고 document를 item이라고 본다면 검색과 추천은 상당히 비슷하다고 볼 수 있다. 실제로 learning to rank 라는 task는 검색과 추천 모두에 활용할 수 있다. 유튜브의 예를 들면 검색 버튼을 통해 원하는 동영상을 찾을 수도 있고 메인 피드에서 재미 있을만한 동영상을 클릭해서 볼 수 있다. 혹은 검색을 통해 나온 아이템을 선택하면 그 아이템과 비슷한 아이템을 추천할 수도 있다. 이렇게 검색과 추천은 서로 비슷하면서도 함께 사용해서 사용자가 좋아할만한 아이템을 더 잘 찾을 수 있게 도와준다.&lt;/p&gt;

&lt;p&gt;추천 시스템을 크게 나누자면 다음 두 개가 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;content-based filtering&lt;/strong&gt; : user나 item은 자체의 특징으로 표현된다. 예를 들어 item이 음악이라면 음악의 제목, 가수, 장르 등이 item을 표현하게 된다. 사용자가 만약 재즈 음악을 들었다면 content-based filtering 은 사용자에게 다른 재즈 음악을 추천할 것이다. 실제로는 이렇게 간단하게 item을 표현하지는 않고 수백개 정도의 feature(?)로 표현한다. collaborative filtering은 user와 item 간의 행동을 분석하는 방법이라고 하면 content-based filtering은 item 자체끼리 비슷한지 비교하는 방법이다. Content-based filering에서 가장 많이 사용되는 방법은 TF-IDF 이다 (자연어처리 쪽에서 널리 사용되는 문장을 vector로 표현하는 방법이다). item을 하나의 vector로 표현한 다음에 clustering 알고리즘을 통해 item을 몇 종류의 cluster로 나눈다. 아래 그림 같이 user가 콜라를 선택했다면 콜라와 같은 cluster에 있는 Duff라는 음류수를 추천하는 것이다. content-based filtering은 과거의 user가 봤던 유형의 item만 추천하기 때문에 한계가 많다. 하지만 user-item iteraction 정보가 없어도 추천이 바로 가능하므로 cold-start 문제는 없다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-fd0d3412-8a14-4346-abde-23cad44cde7a.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/&quot;&gt;http://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;collaborative filtering&lt;/strong&gt; : 줄여서 CF라고 한다. CF는 user-item의 상호작용 정보를 통해 user가 좋아할 만한 item을 추천한다. 기본적인 개념은 user-item interaction 정보를 통해 비슷한 사용자를 찾고 내가 보지 않았지만 비슷한 사용자가 봤던 item을 추천하는 것이다. 이렇게 비슷한 user끼리 서로 어떤 item을 좋아할지 협력해서 filtering하기 때문에 이름이 collaborative filtering이다. CF은 content-based filtering과는 다르게 user나 item 자체의 특성을 추천에 사용하지 않는다. 아래 그림에서 피자와 샐러드라는 item을 user가 샀다고 하더라도 user에게는 그저 item1, item2일 뿐이다. CF는 item 자체의 ‘content’를 모르고 추천하기 때문에 더 많은 user-item interaction 데이터를 필요로 한다. user-user, item-item, user-item similiarity를 계산하기 위한 방법에는 KNN, pearson correlation 등이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-47e94b86-b443-443b-97ef-859fd0b6750f.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/&quot;&gt;http://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;hybrid recommendation system : content-based filtering과 collaborative filtering은 장단점이 있기 때문에 두 방법을 합쳐서 사용하는 연구가 많이 진행됐다. 예를 들면 다음과 같이 두 모델에서 나온 추천을 섞어서 사용자에게 전달하는 방법도 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-aecf588d-3028-4275-aca2-97c71efb58ce.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;참고 자료&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://khanrc.tistory.com/entry/%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9CRecommendation-System&quot;&gt;추천 시스템(Recommendation System)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://ko.wikipedia.org/wiki/%EC%B6%94%EC%B2%9C_%EC%8B%9C%EC%8A%A4%ED%85%9C&quot;&gt;추천 시스템&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://dataconomy.com/2015/03/an-introduction-to-recommendation-engines/&quot;&gt;An Introduction to Recommendation Engines&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;collaborative-filtering&quot;&gt;Collaborative Filtering&lt;/h2&gt;

&lt;p&gt;추천 시스템은 최근에 위에서 언급한 CF 위주로 많은 발전을 이루었다. 누군가 추천 시스템을 만든다고 하면 우선 CF을 사용하라고 할 것이다. CF는 user-item interaction 데이터를 활용해서 추천을 하는 것이다. 하지만 user가 item에 interaction 하는지 (feedback) 데이터마다 다를 수 있다. 간단하게는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;explicit feedback&lt;/strong&gt; : item에 대한 user의 선호도를 직접적으로 알 수 있는 데이터이다. 영화 평점, 맛집 별점과 같은 피드백 데이터를 이야기한다. user가 직접 별점을 계속 매겨야하므로 데이터셋의 크기가 보통 작다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;implicit feedback&lt;/strong&gt; : 서비스에서 user한 행동을 기록한 것을 말한다. 예를 들면 뉴스 사이트에서 뉴스를 클릭한 기록이 될 수 있다. 클릭을 통해서 직접적으로 item에 대한 user의 선호도를 알 수 없기 때문에 implicit feedback이라고 한다. explicit feedback에 비해 데이터의 품질은 낮으나 데이터셋의 양이 보통 비교가 안될만큼 크기 때문에 최신 추천 시스템은 implicit feedback 데이터를 통해 추천하는 방향으로 발전했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-f435adb2-5a77-4b3e-9024-1de5485e9863.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://jinyi.me/2018/06/Recommendation-System-Miscellus/&quot;&gt;https://jinyi.me/2018/06/Recommendation-System-Miscellus/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CF의 학습에 주어지는 데이터의 형태는 user-item matrix이다. 다음 그림이 user-item matrix의 예시이다. 만약 데이터의 형태가 explicit feedback이라면 user가 영화에 대해 줬던 rating 값을 적는다. 이렇게 matrix를 만들고 나면 (?)로 비어있는 곳들이 있는데 이 비어있는 곳을 채우는 것이 CF의 목표라고 볼 수 있다. 따라서 이 문제를 matrix completion이라고 부르기도 한다. 빈 공간을 채웠다면 채운 숫자에 따라 사용자에게 추천할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-56357041-7c29-49bc-a455-a8f556c45f81.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.researchgate.net/figure/Example-of-user-item-ratings-matrix-in-a-movie-recommendation-scenario_fig2_300646445&quot;&gt;https://www.researchgate.net/figure/Example-of-user-item-ratings-matrix-in-a-movie-recommendation-scenario_fig2_300646445&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;만약 implicit feedback라면 rating이 아니라 interaction이 있었다면 1이라고 적는다. interaction이 없던 곳은 0으로 채운다.  Explicit feedback 데이터셋에서는 rating 자체가 이미 item에 대한 선호도를 담고 있기 때문에 rating 안에 긍정적인 feedback과 부정적인 feedback 모두를 담고 있다. 하지만 implicit feedback 데이터셋에서는 오직 positive feedback 정보 밖에 없다. interaction이 일어난 positive feedback에 대해서만 학습한다면 user의 선호도를 학습하기가 어렵다. 따라서 implicit feedback 데이터셋에서는 ‘missing data’를 모두 잠정적인 negative feedback으로 보고 0으로 채운다. 따라서 implicit feedback 데이터셋으로 CF를 학습할 때는 matrix의 빈 공간을 채우는 것이 학습의 목표가 아니라 주어진 matrix에서 사용자의 선호 패턴을 알아내는 것이 목표가 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-c06c3086-9140-4e8c-9d3f-eaecd218a0ab.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;구체적으로 어떻게 CF가 학습 하는지 알아보기 전에 CF가 공통적으로 안고 있는 문제는 다음과 같다. 이 이외에도 많은 문제가 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;cold-start problem&lt;/strong&gt; : CF를 기반으로 추천하려면 user나 item이나 충분한 수의 interaction 데이터가 있어야한다. 따라서 새로운 user가 추천을 받거나 새로운 item이 잘 추천되기 위해서는 여러 개의 iteraction이 쌓일 때까지 기다려야 한다. 즉, user-item matrix에서 새로운 user라는 row가 추가되거나 새로운 item이라는 새로운 column이 추가되었을 때 추천을 할 수 없는 문제이다. 이것을 cold-start 문제라고 부른다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;sparsity&lt;/strong&gt; : 실제 서비스에서는 user와 item이 수백만일 수 있는데 이러면 matrix는 상당히 sparse 해진다. Sparsity는 전체 matrix에서 비어있는 칸이 얼마나 되는지로 측정한다. CF는 sparsity가 0.97 정도 이하일 때 잘 작동한다는 이야기가 있다. Sparsity는 인위적으로 줄일 수도 있다. 예를 들어 일정 개수 이상 interaction이 있는 user와 item만 사용하는 식으로 sparsity를 줄일 수 있다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;diversity&lt;/strong&gt; : CF는 user의 past history를 사용해서 추천을 하기 때문에 비슷한 item만 계속 추천할 수도 있다. 당장은 user가 좋아하는 item을 보여주면 user가 좋아하겠지만 금방 실증날 것이다. 유튜브에서 강아지 동영상을 봤는데 그 이후로 메인 피드에 강아지만 나온다고 생각해보면 이해가 빠를 것이다. 길게 봤을 때 user의 retention에 안 좋은 영향을 준다. 따라서 추천 시스템은 user의 취향을 반영하면서도 다양한 item을 보여줘야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CF는 두 가지로 나뉘는데 memory-based와 model-based 방법이다. 하나씩 살펴보자.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;memory-based method&lt;/strong&gt; : 메모리 기반이라고 하면 표현이 좀 안 와닿을 수 있다. 데이터로 주어지는 user-item matrix를 그대로 다 활용해서 특정 user가 특정 item에 줄 rating을 예측하는 방법이다.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;model-based method :&lt;/strong&gt; user-item matrix를 통해 model을 학습하고 예측은 이 모델을 사용해서 하는 방법이다. CF에서는 제일 유명한 방법인 matrix factorization이 model-based 방법에 해당한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;memory-based-method&quot;&gt;Memory-based method&lt;/h3&gt;

&lt;p&gt;user-item matrix가 주어졌고 특정 user i가 아직 interaction을 하지 않은 item j에 대한 rating을 예측하고 싶다고 해보자. memory-based 방법에서 기본적인 단계는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;user i의 평균 예측 점수 구하기 (사람마다 점수 주는 평균이 다르므로)&lt;/li&gt;
  &lt;li&gt;user i와 다른 user 들과 similarity를 구하기&lt;/li&gt;
  &lt;li&gt;user i와 가장 유사한 k명 추출하기&lt;/li&gt;
  &lt;li&gt;다른 user 들이 item j에 대해 매긴 점수를 weighted sum&lt;/li&gt;
  &lt;li&gt;user i와 위에서 계산한 weighted sum을 통해 최종 rating 예측&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;memory-based 방법을 예시를 통해 살펴보자. 예시는 다음 블로그에서 가져왔다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://buildingrecommenders.wordpress.com/2015/11/18/overview-of-recommender-algorithms-part-2/&quot;&gt;Overview of Recommender Algorithms - Part 2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;다음과 같이 사용자가 책에 대해 rating을 explicit하게 매긴 데이터가 주어졌다고 해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-90c7ca5f-c9da-4b20-a0a1-945509aa43b5.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;user-based CF에서는 user 사이의 similarity를 구해야 한다. user 사이의 similarity를 구하기 위한 방법에는 여러가지가 있는데 그 중에서 가장 대표적인 것이 pearson correlation이다. 두 명의 user가 함께 점수를 매긴 rating 정보를 통해 similarity를 구할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-48ac373b-7d4a-499c-b42d-77b022d22801.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Pearson correlation이 아니라 cosine similarity를 통해서 유사도를 구할 수도 있다. Cosine similarity는 다음과 같은 식으로 구할 수 있다. Cosine similarity는 user나 item의 평균 rating을 계산하지 않아도 되기 때문에 계산적인 측면에서 pearson correlation 보다 더 효율적이다. 하지만 특정 사용자의 점수 기준이 너무 높거나 낮거나 또는 특정 아이템이 받는 점수가 평균적으로 너무 높거나 낮거나 할 때 유사도가 잘 계산이 안될 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-39db3088-bc63-4006-badb-d9954e780757.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 similarity 식을 이용해서 추천을 하고자 하는 사용자와 다른 사용자 사이의 similarity를 구하고 정렬한다. 이렇게 다른 사용자들을 similarity에 따라 정렬을 했다면 그 중에서 가장 similarity가 높은 사용자들을 추려낸다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-ce620a4c-d8f5-4da0-b6f8-7f83580a1e04.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;가장 비슷한 사용자를 추려냈다면 다음 식을 통해 rating을 예측할 수 있다. k는 nomaralizing factor라고 보면 되고 simil이 user 사이의 similarity 값이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-3d4b9910-c311-423f-b8b9-62cf6cf7b7b8.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-016e5dbf-ffc7-44fa-a10b-5c9e39be0bb7.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예를 들어 가장 비슷한 사용자 2명만을 이용해서 추천한다고 해보자. 이 사용자 2명이 본 책 중에 이미 추천 받을 사용자가 본 책을 빼고 rating을 예측한다. 다음 그림에서는 간단하게 하기 위해서 평균 rating 정보는 생략했다. 예측한 rating에 따라 가장 사용자가 좋아할 만한 책을 알아낼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-b09adff9-d709-4ac1-93d4-b85438d707f6.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;memory-based method는 matrix의 sparsity가 커지면 커질수록 잘 동작하지 않는 경향이 있다. 따라서 model-based method 쪽으로 연구는 이어지게 된다.&lt;/p&gt;

&lt;h3 id=&quot;model-based-method&quot;&gt;Model-based method&lt;/h3&gt;

&lt;p&gt;user-item matrix를 통해 model을 학습하는 방법이다. Matrix로부터 model이 학습하고 나면 추천을 할 때는 model만 있으면 된다. model-based CF는 memory-based CF에 비해 scalability와 추천 속도에서 장점이 있다. 대신 데이터셋 전체를 사용해서 추천 하는 것이 아니라 model만을 사용하기 때문에 추천이 정확하지 않을 수 있다는 단점이 있다. Model의 종류는 여러가지가 있다. Bayesian Network, Decision Tree, Latent Factor Models, MDP 등 다양하다. 여기서 살펴볼 것은 Matrix Factorization이라는 방법으로 Latent Factor Model의 일종이다.&lt;/p&gt;

&lt;h2 id=&quot;matrix-factorization&quot;&gt;Matrix Factorization&lt;/h2&gt;

&lt;p&gt;MF는 다음 논문을 참고했다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf&quot;&gt;https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MF는 latent factor 모델의 일종이다. Latent factor 모델이 하고자 하는 것은 item과 user를 20-100 정도의 vector로 표현하고자 하는 것이다. 이 vector는 데이터에서 나타나는 rating 패턴에 의해 학습되는 것이다. content-based 방법에서 item을 제목이나 장르, 가수 등으로 표현했다면 latent factor 모델에서는 interaction pattern으로부터 학습된, 기계가 이해하는 특징으로 표현한다. Latent factor는 interaction 정보를 포함하기 때문에 풍부하게 item이나 user를 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;Content-based method와는 또 다른 것이 item과 user의 latent factor는 vector의 dimension을 공유한다는 것이다. 다음 예제를 보면 이해가 더 쉬울 것이다. 다음 그림은 latent factor 모델을 단순화시킨 그림이다. 사람과 영화 모두 serious 축과 성별 축 상에서의 값으로 표현한다. 따라서 latent factor 모델에서는 user와 item의 latent factor가 각 dimension 마다 같은 의미를 지닌다. Latent-factor model에서는 user와 item factor의 dot-product 값을 측정하고 그 값이 크면 추천한다. 같은 공간 상에 있기 때문에 가능한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-87d5c1c7-f6ef-44a6-ab29-fde9a30d609d.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 논문에서 Matrix Factorization에 대해 다음과 같이 설명한다. 이 설명에서도 보면 user와 item을 같은 latent factor space 상에 mapping 한다고 표현한다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf&quot;&gt;https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Matrix factorization models map both users and items to a joint latent factor space of dimensionality f, such that user-item interactions are modeled as inner products in that space
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Matrix Factorization을 식과 그림으로 나타내는 것은 어렵지 않다. item latent factor matrix를 q라고 하고 user latent factor matrix를 p라고 한다면 user u의 item i에 대한 rating은 다음과 같은 식으로 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-a70070cd-d4dd-46ce-96ea-05837e925d1b.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림으로 보자면 원래 user-item matrix를 user의 latent factor matrix와 item의 latent factor matrix로 쪼개는 것이라고 볼 수 있다(matrix factorization 자체가 행렬을 인수분해 한다는 뜻이니까). 이 때, q와 p를 곱한 값으로 원래 user-item matrix를 만들어내야 하는데 그것이 MF 학습의 핵심이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-946921cd-6d3f-4713-aff7-2267f622ba79.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://buildingrecommenders.wordpress.com/2015/11/18/overview-of-recommender-algorithms-part-2/&quot;&gt;https://buildingrecommenders.wordpress.com/2015/11/18/overview-of-recommender-algorithms-part-2/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;MF의 objective function은 다음과 같이 정의할 수 있다. 목표는 원래 rating을 예측하는 것으로 loss function은 MSE error를 사용한다. MF 중에서 MSE error가 아닌 cross-entropy를 사용하는 경우도 있지만 일단 MSE error만 생각하자. 아래 식에서 k는 rating을 알고 있는 user, item pair를 의미한다. 만약 MSE error만 최소화하는 것으로 학습하면 쉽게 overfitting 될 것이다. 따라서 L2 regularization을 사용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-85a08543-f738-45c1-8d30-3626fee8aaeb.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 objective function을 최소화하는 p와 q를 찾기 위한 방법이 2가지가 있다.&lt;/p&gt;

&lt;h3 id=&quot;sgdstochastic-gradient-descent&quot;&gt;SGD(stochastic gradient descent)&lt;/h3&gt;

&lt;p&gt;머신러닝을 하는 사람이라면 SGD에 대해 잘 알고 있을 것이다. error를 왼쪽과 같이 쓴다면 p와 q는 오른쪽 식과 같은 식으로 업데이트 할 수 있다(위의 loss function을 편미분해서 뺀 것이다).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-ea7f2b35-e501-417e-8314-b5ed1103a746.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-2c39105a-e84a-4360-b3bf-018532efa446.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;alsalternating-least-square&quot;&gt;ALS(alternating least square)&lt;/h3&gt;

&lt;p&gt;아래 objective function에서 p와 q를 모르기 때문에 convex optimization이 아니다. 이 objective function을 convex하게 바꿔주는 방법이 ALS이다. 방법은 간단하다. p와 q 둘 중 하나를 고정하면 전체 식은 convex하게 된다. 따라서 p의 값(만약 q를 고정했다고 하면)은 optimal하게 결정된다. p와 q를 번갈아가면서 고정하고 업데이트를 하는 식으로 학습을 진행한다. 다음 objective function이 수렴할 때까지 그 과정을 반복하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-85a08543-f738-45c1-8d30-3626fee8aaeb.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SGD가 ALS보다 간단하고 빠른 경향이 있지만 ALS는 parallel하게 계산할 수 있다는 장점이 있다. ALS에 대한 자세한 설명은 ‘Collaborative Filtering for Implicit Feedback Datasets’ 논문을 참고하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;additional-bias&quot;&gt;Additional bias&lt;/h3&gt;

&lt;p&gt;MF는 user factor와 item factor의 단순 dot product로 rating을 표현하려고 한다. 하지만 특정 사용자는 평균적으로 모든 item에 높은 점수를 줄 수도 있다. MF의 objective function은 이런 점을 잘 반영하지 못한다. 따라서 user와 item에 대해 bias를 사용한다. user u가 item i에 매긴 rating에 대한 바이어스는 오른쪽 식과 같이 표현할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-18bbb271-54aa-4b94-b30c-7ca53a18ea29.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-90e81317-aaf6-47f9-a541-0aead105ab60.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;user bias와 item bias 모두 학습 대상이기 때문에 objective function에서 포함해서 학습한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-8de4c4ed-58da-42c3-9728-a870d16ec871.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;code-review-of-sgd-mf&quot;&gt;Code Review of SGD MF&lt;/h3&gt;

&lt;p&gt;SGD를 이용해서 학습하는 기본적인 MF에 대한 코드는 다음 글에 잘 소개되어 있다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://yamalab.tistory.com/92&quot;&gt;[Recommender System] - Python으로 Matrix Factorization 구현하기&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;P는 user latent matrix이고 Q는 item latent matrix이다. num_users는 사용자 수이고 num_items는 아이템의 수를 말한다. k는 latent factor의 dimension을 말한다. P와 Q는 normal distribution에 따라 parameter initialization을 해준다. R은 user-item matrix를 의미한다. 위에서 말한 것처럼 user, item, 그리고 모든 rating에 대한 bias를 사용한다. user와 item에 대한 bias는 user 수, item 수의 크기를 가진다. 또한 global bias는 상수이다. 원하는 epoch 만큼 학습을 하는데 각 epoch에서 0이 아닌 모든 rating에 대해서 loss를 미분해서 업데이트한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        training Matrix Factorization : Update matrix latent weight and bias
    
        참고: self._b에 대한 설명
        - global bias: input R에서 평가가 매겨진 rating의 평균값을 global bias로 사용
        - 정규화 기능. 최종 rating에 음수가 들어가는 것 대신 latent feature에 음수가 포함되도록 해줌.
    
        :return: training_process
        &quot;&quot;&quot;&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# init latent features
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_num_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# init biases
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_b_P&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_num_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_b_Q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_R&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# train while epochs
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_training_process&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    
            &lt;span class=&quot;c1&quot;&gt;# rating이 존재하는 index를 기준으로 training
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_num_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_training_process&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
            &lt;span class=&quot;c1&quot;&gt;# print status
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_verbose&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Iteration: %d ; cost = %.4f&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;self.gradient_descent를 통해서 p와 q 그리고 bias 들을 업데이트한다. 업데이트는 user, item pair 하나 하나에 대해 gradient를 구해서 한다. gradient_descent 함수는 다음과 같다. 만약 prediction을 구했다고 하면 prediction과 rating의 차이를 error로 정의한다. bias를 업데이트 할 때는 loss function을 bias로 미분해서 gradient를 구해서 업데이트한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-8de4c4ed-58da-42c3-9728-a870d16ec871.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gradient_descent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rating&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        graident descent function
    
        :param i: user index of matrix
        :param j: item index of matrix
        :param rating: rating of (i,j)
        &quot;&quot;&quot;&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# get error
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rating&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# update biases
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_b_P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_reg_param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_b_P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_b_Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_reg_param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_b_Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# update latent feature
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;dp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dp&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_learning_rate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dq&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;prediction은 다음 수식을 코드로 구현한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-90e81317-aaf6-47f9-a541-0aead105ab60.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        get predicted rating: user_i, item_j
        :return: prediction of r_ij
        &quot;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;b_Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:].&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;T&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;user latent factor와 item latent factor의 gradient는 다음과 같이 구한다. loss 식을 p와 q에 대해서 편미분해서 구한 것이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;gradient&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
        gradient of latent feature for GD
    
        :param error: rating - prediction error
        :param i: user index
        :param j: item index
        :return: gradient of latent feature tuple
        &quot;&quot;&quot;&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;dp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_reg_param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;dq&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_P&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_reg_param&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_Q&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:])&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dq&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;cost는 단지 실제 rating과 prediction의 차이가 얼마나 나는지 보는 것으로서 원래 Loss 식에서 regularization term을 뺀 값이라고 볼 수 있다. 매 epoch 마다 cost를 측정하면서 cost가 수렴하면 학습을 멈춘다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yi&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nonzero&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_complete_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;zip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;R&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sqrt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cost&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 15 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/recomm/2019/08/15/recommendation_system_basics/</link>
        <guid isPermaLink="true">http://localhost:4003/recomm/2019/08/15/recommendation_system_basics/</guid>
        
        
        <category>recomm</category>
        
      </item>
    
      <item>
        <title>Neural Collaborative Filtering 논문 리뷰</title>
        <description>&lt;h1 id=&quot;neural-collaborative-filtering&quot;&gt;Neural Collaborative Filtering&lt;/h1&gt;

&lt;h3 id=&quot;주요-내용&quot;&gt;주요 내용&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;논문이 풀고자 하는 문제는 implicit feedback을 토대로 item을 user에게 추천하는 것&lt;/li&gt;
  &lt;li&gt;deep learning을 matrix factorization에서 user-item interaction 부분에 적용하는 것&lt;/li&gt;
  &lt;li&gt;dot-product(GMF)와 MLP의 장점을 모두 살린 네트워크 구조 사용&lt;/li&gt;
  &lt;li&gt;loss function으로 MSE가 아닌 binary cross-entropy 사용&lt;/li&gt;
  &lt;li&gt;point-wise loss + negative sampling 사용&lt;/li&gt;
  &lt;li&gt;BPR과 eALS를 large-margin으로 outperform&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;→ 다른 유튜브 논문이나 wide and deep 논문에서는 여러 feature를 어떻게 잘 조합하느냐에 집중했음. 이 논문에서는 정말 collaborative filtering 세팅에 집중해서 다른 feature 없이 deep learning을 사용해 잘 학습하는 것에 집중함. 은근 참고할 내용이 많아서 꼭 한 번 읽어봐야 함.&lt;/p&gt;

&lt;h3 id=&quot;implicit-data-학습&quot;&gt;implicit data 학습&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;추천 학계에서는 그동안 주로 explicit feedback으로 학습해왔는데 트렌드는 implicit feedback임&lt;/li&gt;
  &lt;li&gt;implicit feedback은 자동적으로 데이터가 수집되고 데이터가 풍부하다는 게 장점&lt;/li&gt;
  &lt;li&gt;단점은 negative feedback의 부재. 사실 이 문제를 어떻게 해결하느냐가 implicit feedback 추천에서 제일 중요함. 사실 여기서 데이터는 positive vs negative가 아니라 observed vs unobserved 임. 중요함.&lt;/li&gt;
  &lt;li&gt;두 가지 objective function이 있는데 point-wise loss와 pair-wise loss가 있음&lt;/li&gt;
  &lt;li&gt;BPR(Bayesian Personalized Ranking)에서는 pair-wise loss를 사용함.&lt;/li&gt;
  &lt;li&gt;point-wise loss를 사용하는 Matrix factorization 모델 중에서 가장 SOTA는 eALS임. BPR 보다 좋음.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;dot-product의-문제&quot;&gt;dot-product의 문제&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;잘 와닿지는 않지만 다음 그림으로 설명. user-item interaction matrix에서 user 끼리의 유사도를 측정함. u1, u2, u3에 대해 유사도에 따라 벡터를 그려보면 (이걸 latent factor라고 가정) (b) 그림과 같이 나옴. 새로 u4가 들어와서 u1, u2, u3와 유사도를 측정해보면 u1 &amp;gt; u3 &amp;gt; u2 순으로 u4와 유사함. 하지만 p4를 어디에 놔도 p3보다 p2가 더 가깝기 때문에 ranking loss가 커질 수 밖에 없음. dot-product가 linear 하기 때문에 발생하는 문제인데 그래서 non-linear인 neural net를 쓰겠다는 논리를 펼치는 것.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-c783035f-3863-4023-920c-c397538a1254.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;사실 위 문제는 latent factor의 dimension이 낮기 때문에 발생하는 문제인데 이 문제를 해결하기 위해 dimension을 높이면 오히려 overfitting이 발생함 → 그래서 latent factor의 dimension을 높이는 것이 아니라 user factor와 item factor 간의 상관관계를 풍부하게 표현하는 방향으로 생각을 바꿈.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;neural-collaborative-filtering-framework&quot;&gt;Neural Collaborative Filtering Framework&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;NCF는 user latent factor와 item latent factor간의 상관관계를 표현하는데 MLP를 사용한다는 관점에서 다음과 같은 구조로 표현할 수 있음&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-a3e99bbb-bc1c-4f0a-92e9-b15746d9e4d8.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;user embedding과 item embedding을 latent factor로 보고 그 두 개의 vector를 뉴럴넷에 넣어서 이래저래 하는 것을 NCF라고 함.&lt;/li&gt;
  &lt;li&gt;원래 WMF에서는 loss function으로 MSE를 사용했는데 그건 output를 gaussian distribution으로 봤기 때문임. 하지만 정답은 1 또는 0 밖에 없기 때문에 가우시안이 아닌 probabilitic function을 사용하는 것이 좋음 (?? 이거 사실 이해 못함). 따라서 sigmoid function으로 output을 내보내고 loss function으로는 binary cross-entropy를 사용&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-59cbfede-af99-4ea2-8bdf-55560f21b836.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;학습은 SGD로&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;generalized-matrix-factorization&quot;&gt;Generalized Matrix Factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;줄여서 GMF. MF의 일반화 버전으로 MF에서는 단순 dot-product로만 output을 예측했다면 GMF에서는 element마다의 weight를 학습함. p와 q가 embedding인데 두 embedding을 element-wise로 곱한 다음에 weight를 곱함. 그리고 non-linear activation function을 사용해서 모델이 user-item interaction을 더 풍부하게 표현할 수 있도록 함. h가 uniform vector이고 a가 1이면 그게 바로 MF&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-d6c1628c-a573-4c7d-b13b-90154c5a3b4b.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;multi-layer-perceptron&quot;&gt;Multi-Layer Perceptron&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;MLP라 해서 별거 있는 건 아니고 GMF가 dot-product + 뉴럴넷이라면 MLP는 concat + 뉴럴넷이다.&lt;/li&gt;
  &lt;li&gt;GMF 보다는 MLP가 좀 더 user-item interaction을 학습하는 입장에서 flexible함.&lt;/li&gt;
  &lt;li&gt;보통 랭킹 모델의 구조라고 볼 수 있음&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-8cca6436-3f4c-401e-970c-3cf5f2b0e308.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;neural-matrix-factorization&quot;&gt;Neural Matrix Factorization&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;위에서 언급한 GMF와 MLP가 각자 다른 특성을 가지고 있는데 이걸 합쳐버리면 더 복잡한 user-item interaction도 표현할 수 있지 않을까?&lt;/li&gt;
  &lt;li&gt;그래서 나온게 다음과 같은 모델 구조. 이렇게 하지 않고 MLP와 GMF가 같은 embedding을 공유할 수 있는데 그렇게 하면 동일한 embedding 사이즈로 고정된다는 단점이 있음. MLP와 GMF를 각각 최적화 했을 때 다른 embedding size를 가질 수 있으므로 분리하는게 더 이상적임&lt;/li&gt;
  &lt;li&gt;수식과 그림은 다음과 같음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-d7f35132-1ec9-4e1e-9dfd-360c68c40ad9.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-b2293c08-61b8-4503-b79a-b33a23aebfc0.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;마지막에 GMF와 MLP 부분을 합칠 때 weight를 사용해서 trade-off 를 조절할 수 있음. 근데 결국 0.5로 사용함..&lt;/li&gt;
  &lt;li&gt;GMF와 MLP를 각각 pretrain 한 다음에(ADAM으로 학습) 합쳐서 NMF로 학습(SGD로 학습)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;experiment&quot;&gt;Experiment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;학습에 사용한 건 movielense 데이터와 pinterest 데이터.&lt;/li&gt;
  &lt;li&gt;핀터레스트 데이터의 경우 20개의 핀을 본 사용자만 데이터에 포함함.&lt;/li&gt;
  &lt;li&gt;처음 하이퍼 파라메터를 조정하기 위해 사용자 당 하나의 데이터만 추출해서 데이터셋을 만들었음&lt;/li&gt;
  &lt;li&gt;하나의 positive 당 4개의 negative를 뽑아서 학습에 사용함&lt;/li&gt;
  &lt;li&gt;파라메터 초기화는 가우시안 분포로.&lt;/li&gt;
  &lt;li&gt;핀터레스트 데이터에서 NDCG로 측정할 때는 BPR이 eALS보다 잘 하는 경향이 있음. pair-wise loss를 사용할 경우에 ranking을 잘 하는 경향이 있음&lt;/li&gt;
  &lt;li&gt;NeuMF가 모든 경우에 SOTA&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-c233dbc9-4dd3-4ce4-914d-b4b2472341a9.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;point-wise loss의 경우 negative sampling ratio를 조절할 수 있다는 장점이 있음. sampling ratio를 조절하면서 실험한 결과는 다음과 같음. 3에서 6 사이가 optimal 함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-563f9a1b-dbe8-483c-b2fc-7eae566f2acb.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MLP에서 레이어를 늘리면 더 잘 학습함. 딥러닝 방법이 잘 맞는다는 이야기.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/Untitled-953dc5f7-f6df-4c58-994e-b8846407953f.png&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;코드&quot;&gt;코드&lt;/h3&gt;

&lt;p&gt;NMF 코드는 논문 저자가 Keras로 구현해놓음. 다음 부분이 모델 부분. NMF 그림과 동일함. GMF의 embedding 차원은 8이고 MLP의 embedding 차원은 32임. MLP의 뉴럴넷 차원은 [64,32,16,8]임.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/NeuMF.py&quot;&gt;https://github.com/hexiangnan/neural_collaborative_filtering/blob/master/NeuMF.py&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mf_dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;reg_mf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;assert&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;#Number of layers in the MLP
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;# Input variables
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'int32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'user_input'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'int32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'item_input'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Embedding layer
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;MF_Embedding_User&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mf_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mf_embedding_user'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_regularizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg_mf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;MF_Embedding_Item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mf_dim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mf_embedding_item'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_regularizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg_mf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   
    
        &lt;span class=&quot;n&quot;&gt;MLP_Embedding_User&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_users&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;mlp_embedding_user&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_regularizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;MLP_Embedding_Item&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Embedding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_dim&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mlp_embedding_item'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init_normal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_regularizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;   
        
        &lt;span class=&quot;c1&quot;&gt;# MF part
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;mf_user_latent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MF_Embedding_User&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mf_item_latent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MF_Embedding_Item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mf_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mf_user_latent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mf_item_latent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'mul'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# element-wise multiply
&lt;/span&gt;    
        &lt;span class=&quot;c1&quot;&gt;# MLP part 
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;mlp_user_latent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MLP_Embedding_User&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mlp_item_latent&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MLP_Embedding_Item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mlp_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mlp_user_latent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_item_latent&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'concat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;W_regularizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;l2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reg_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;layer%d&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;mlp_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mlp_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;c1&quot;&gt;# Concatenate MF and MLP parts
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;#mf_vector = Lambda(lambda x: x * alpha)(mf_vector)
&lt;/span&gt;        &lt;span class=&quot;c1&quot;&gt;#mlp_vector = Lambda(lambda x : x * (1-alpha))(mlp_vector)
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;predict_vector&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;merge&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mf_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mlp_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'concat'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;c1&quot;&gt;# Final prediction layer
&lt;/span&gt;        &lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sigmoid'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'lecun_uniform'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;prediction&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; 
                      &lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;prediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;다음은 negative sampling 하는 부분. 사용자와 positive 아이템을 하나 뽑고 그 사용자와 interaction 하지 않은 아이템들을 가져옴. 이렇게 전체 데이터셋을 만들고 나서 256 batch size로 뽑아서 학습에 사용함.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_train_instances&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_negatives&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],[],[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;num_users&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keys&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# positive instance
&lt;/span&gt;            &lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;c1&quot;&gt;# negative instances
&lt;/span&gt;            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;xrange&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_negatives&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;has_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
                    &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;u&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;user_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;item_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;loss function과 optimizer는 다음과 같음&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;adagrad&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adagrad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rmsprop&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RMSprop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;learner&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lower&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;adam&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Adam&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;learning_rate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'binary_crossentropy'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Thu, 15 Aug 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/recomm/2019/08/15/neural_collaborative_filtering/</link>
        <guid isPermaLink="true">http://localhost:4003/recomm/2019/08/15/neural_collaborative_filtering/</guid>
        
        
        <category>recomm</category>
        
      </item>
    
      <item>
        <title>카오스멍키를 읽으며</title>
        <description>&lt;h1 id=&quot;카오스멍키를-읽으며&quot;&gt;카오스멍키를 읽으며&lt;/h1&gt;
&lt;h3 id=&quot;어떤-내용을-담고-있는가&quot;&gt;어떤 내용을 담고 있는가&lt;/h3&gt;

&lt;p&gt;카오스멍키는 기술적인 뜻을 가진다. 넷플릭스가 만들어서 오픈소스로 공개한 서버 엔지니어링 측면에서 안정성을 테스트하는 방법인데 어떻게 보면 좀 악랄한 방법이라고 볼 수 있다. 카오스 멍키(=미친 원숭이)가 데이터 센터에서 뛰어다니며 서버를 마구 망치고 있어도 서비스는 그대로 돌아가야 함을 의미한다. 하지만 저자가 사용한 카오스멍키는 이런 엔지니어링적인 의미는 아니다. 서버실을 우리가 사는 세상이라고 하면 카오스멍키는 저자가 몸을 담고 혹은 몸을 담았던 스타트업을 의미한다. 스타트업은 이리 저리 마구 선을 뽑는 원숭이와 같이 세상의 질서를 무시하며 무섭게 돌진한다. 하지만 이 책의 제목은 그닥 중요하지 않다. 이 뜻을 찾아보며 저자에게 당했다라는 생각도 든다. 바이럴이 되도록 일부러 이런 생소한 단어를 제목으로 쓴게 아닌가 싶기도 하다.&lt;/p&gt;

&lt;p&gt;이 책은 저자가 월스트리트의 골드먼삭스에서 일할 때부터 페이스북의 문을 나올 때까지의 이야기를 담고 있다. 하지만 단순히 개인의 경험에 대한 자서전이라고 하기에는 실리콘밸리의 스타트업에 대해 이야기 해주는 바가 많다. 아니 이보다 더 적나라하게 솔직한 정보를 주는 책이 있을까 싶으며 저자가 걱정되는 책이다(실명을 거론하며 거침없이 상대방을 까기 때문에). 창업, 투자, 소송 인수 등의 이야기를 직설적으로 이야기한다. 그동안 스타트업에 대해 환상을 가지고 있으며 깨부수기 가장 좋은 책이다. 예나 지금이나 주변에서 스타트업을 하라고 부추기는 친구가 있다면 좋은 친구가 아닐지 모른다. 그만큼 스타트업은 힘들며 마치 진흙탕을 나뒹구는 것 같다.&lt;/p&gt;

&lt;p&gt;하지만 심지어 이런 정보들도 이 책에서 가장 중요한 내용은 아니다. 이 책이 주는 가장 중요한 정보는 바로 &lt;strong&gt;에드테크에 대한 이야기와 페이스북의 광고 시스템에 대한 이야기&lt;/strong&gt;이다. 플랫폼 회사에서 일하고 있다면 꼭 필요한 바로 그 정보이다. 물론 이런 정보를 안다고 해서 자신의 회사에서 바로 극적인 변화를 일으킬 수 있는 것은 아니다. 하지만 데이터가 신성시되는 업계에서 일하다보면 당연히 모르고 지나칠 바로 그 질문 “그래서 데이터에서 어떻게 돈을 벌 수 있는데?”을 마주하게 한다. 페이스북도 어려워했다. 우리는 어떨까?&lt;/p&gt;

&lt;p&gt;저자가 페이스북에서 근무할 당시 페이스북 사용자는 거의 10억에 달했다. 10억의 사용자가 있으면 저절로 돈이 벌려야 하는 것 아닌가? 물론 말 그대로 10억을 곱하면 큰 숫자가 되기 때문에 페이스북은 돈을 많이 벌고 있었다. 하지만 지금의 페이스북 제국을 만들 정도는 아니었다. 어떻게 페이스북은 현재 마케터가 가장 선호하는 광고 플랫폼이 되었을까? 어떻게 실리콘밸리에서 가장 뛰어난 광고 시스템을 구축할 수 있었을까? 이것을 이해하려면 광고 업계의 발전에 대해서 알아야 한다. 분명 저자가 먹기 좋게 정보를 제공하고 있지만 한 번 봐서는 알아 듣기가 어려웠다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;광고를-더-똑똑하게-타겟팅과-리타겟팅&quot;&gt;광고를 더 똑똑하게: 타겟팅과 리타겟팅&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;기존의 광고에서 ‘타켓팅’이라는 개념은 존재하지 않았다.&lt;/strong&gt; 신문 하단에 있는 광고나 고속도로의 광고판이나 황금시간대에 엄청나게 비싼 돈을 주고 만든 갤럭시 핸드폰 광고는 모두 같다. 조준하지 않고 쏘는 기관총과 같이 최대한 많은 사람에게 노출하는 것이 목표이다. 광고에 관심이 있을 사람에게 노출하는 타겟팅은 물론 쏟아부은 돈에 비해 얼만큼 효과가 있었는지 측정도 거의 불가능 했다. ‘돈을 썼으니 사람들이 더 알아주겠지’라고 생각하는 것 밖에는 방법이 없는 것이다. 물론 그 중에는 대박이 터져서 효과를 확 느낄 수 있는 경우도 있겠지만 일반적인 경우는 아니다.&lt;/p&gt;

&lt;p&gt;그러한 광고계의 원시 시대로부터 어떻게 현재에 이르게 되었을까? 페이스북에 들어가보면 알겠지만 방금 봤던 상품이 페이스북 광고로 나오는 경우를 꽤 봤을 것이다. 이런 기능을 &lt;strong&gt;리타겟팅&lt;/strong&gt;이라고 한다. 많은 일반 사용자들을 소름끼치게 하면서 클릭하게 만드는 이런 기술이 어떻게 가능할까? 이 기술을 리타겟팅이라 부르는 이유는 특정 쇼핑몰을 방문했다가 이탈한 소비자를 다시 타겟팅해서 쇼핑몰로 데려올 수 있기 때문이다. 리타겟팅에 대해 더 궁금하다면 다음 글을 읽어보면 된다. 글에서 소개한 리타게팅의 예시를 보면 훨씬 이해하기 쉽다&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;토요일 아침 침대에 누워 검은색 스니커즈를 온라인으로 구매하려고 태블릿으로 검색하고 있다고 상상해보겠습니다. 마침 마음에 드는 상품을 찾았고 장바구니에 담았습니다. (물론 장바구니에 담지 않을 수도 있겠지만, 이미 온라인 행동 패턴상으로는 해당 상품에 대한 관심이 있다는 것은 알 수 있습니다). 쇼핑을 하던 중 친구와의 점심 약속 때문에 문자를 주고받다가 구매 완료를 잊고 외출 준비를 합니다. 몇 시간 후 친구와 점심을 먹고 커피숍에서 스마트폰으로 뉴스를 찾아봅니다. 최신 기사를 스크롤하던 중 검은색 스니커즈가 나오는 광고를 보게 됩니다. 그런데 신기하게도 광고 속의 스니커즈는 아침에 봤던 바로 그 웹사이트에서 판매하고 있는 상품입니다. 이것이 바로 리타게팅입니다&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;a href=&quot;https://www.criteo.com/kr/insights/%EB%A6%AC%ED%83%80%EA%B2%8C%ED%8C%85%EC%97%90-%EB%8C%80%ED%95%B4-%EC%95%8C%EA%B3%A0-%EC%8B%B6%EC%9C%BC%EC%84%B8%EC%9A%94-%EB%A6%AC%ED%83%80%EA%B2%8C%ED%8C%85%EC%9D%98-%EC%9B%90%EB%A6%AC%EB%A5%BC/&quot;&gt;리타게팅의 원리에 대해 알아보기&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;리타겟팅이 가능한 이유는 바로 쿠키 때문이다. 궁금하다면 다음 글을 읽어보길 바란다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://garius.tistory.com/50&quot;&gt;리타겟팅 광고- 쿠키란?(유저,사이트 리타겟팅)&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;리타겟팅은 타겟팅의 여러가지 종류 중의 하나이다. &lt;strong&gt;단순한 배너 광고를 넘어 이렇게 똑똑한 광고가 가능하게 된 것은 바로 데이터의 활용 때문이다&lt;/strong&gt;. 리타겟팅의 경우 광고주가 가지고 있는 정보를 페이스북이 활용한 케이스이다. 그 이외에도 페이스북에서는 실제 사용자 정보를 토대로 한 타겟팅이 가능하다. 단순히 사용자의 나이, 성별 등을 이용해서 타겟팅을 할 수도 있고 페이스북에서 사용자가 입력하거나 좋아요를 누른 기록을 통해 사용자의 관심사를 파악해서 타겟팅을 할 수 있다. 그렇게 타게팅된 광고는 페이스북의 뉴스피드에 뜬다. 사실 이런 이야기는 저자가 책을 쓴 이후의 이야기이다. 2013년부터 서서히 페이스북은 뉴스피드에 광고를 넣기 시작했다. 중독성 강한 뉴스피드에 광고를 넣음으로서(이전에 저커버그가 뉴스피드를 신성시해서 광고를 넣지 못하게 했던 것을 생각하면 아이러니하다) 현재의 페이스북 제국이 형성된 것이다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;페이스북을-현재-페이스북으로&quot;&gt;페이스북을 현재 페이스북으로&lt;/h3&gt;

&lt;p&gt;페이스북의 현재 사용자는 어림잡아 20억명 정도다. 저자가 글을 쓸 당시에는 10억명 정도였는데 이 정도의 사용자가 있으면 아무리 광고 시스템이 엉망이어도 돈이 많이 벌린다는 사실이 더 골치 아팠던 것 같다. 이 책에서는 페이스북이 이 수많은 사용자가 있는 플랫폼에서 돈을 벌기 위해 어떤 노력을 해왔는지 이야기한다. 하지만 결국 구세주는 모바일 기기의 보편화이다. 그 전에 간단히 페이스북이 어떤 노력을 했는지 살펴보자. 우리도 비슷한 진통을 겪을 수 있으니.&lt;/p&gt;

&lt;p&gt;페이스북은 초반에 광고는 창에서 오른쪽 위 구석 잘 안 보이는 곳에 위치하고 있었다. 마크 저커버그가 뉴스피드에 광고를 넣는 것을 싫어했기 때문에 최대한 광고는 안 보이도록 조치한 것이다. 그래서 페이스북 초반에 업체를 홍보하기 위해 가능했던 방법은 &lt;strong&gt;좋아요를 구매&lt;/strong&gt;하는 것이다. 왜냐하면 좋아요를 통해서 혹은 공유를 통해서만 사용자들의 뉴스피드에 노출될 수 있었기 때문이다. 하지만 그 효과는 낮았고 초반에 돈을 많이 쓰던 기업들도 곧 형편없는 광고라는 것을 깨달았다. 셰릴은 회의 석상에서 다음과 같이 말했다고 한다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“그만둬요! 스타벅스는 이제 더 이상 좋아요 때문에 연간 1,000만 달러를 쓰진 않을 거에요. 아무도 이제 그렇게 하지 않을 거라고요!”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;페이스북이 시도했던 다른 광고 방법은 &lt;strong&gt;스폰서드 스토리&lt;/strong&gt;이다. 다르게 말하면 친구가 추천하는 광고랄까. 즉, 그냥 광고를 집어넣는다면 잘 클릭을 안하지만 나의 친구가 좋아요를 눌렀다는 정보가 함께 나타나면 클릭을 하게 된다는 것이다. 실제로도 스폰서드 스토리의 클릭율은 기존 광고에 비해 40-60 퍼센트 높아졌다. 그래봤자 0.11 퍼센트 밖에 안되지만 말이다. 페이스북의 기대와는 다르게 스폰서드 스토리를 망했다. 스폰서드 스토리가 궁금하면 다음 글을 참고하자.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.bloter.net/archives/123361&quot;&gt;페북 ‘친구 추천 광고’, 정작 친구는 몰랐다&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;이제 여기서부터 저자와 관련있는 내용이 나온다. 바로 CA와 FBX다. 이 두 가지는 결국 &lt;strong&gt;페이스북 사용자와 외부 데이터를 연결하려는 노력이다&lt;/strong&gt;. 저자는 페이스북에서 타겟팅과 관련된 일을 했는데 타겟팅이란 결국 유저를 부르고 대답하는 과정이라고 할 수 있다. 현실 세계에서 사람은 이름이 있어서 부르면 대답하지만 온라인 세계에서 사용자는 여러 가지의 이름을 가진다. 이 이름들이 같은 이름인지 아닌지를 판단하는 기술이 바로 커스텀 어디언스, 줄여서 CA이다. 저자가 그토록 반대했던 광고 시스템이다. CA는 기존에 있던 페이스북의 타겟팅 기능을 활용했다고 한다. CA를 이해하려면 데이터 온보딩에 대해서 알아야 한다.&lt;/p&gt;

&lt;p&gt;데이터 온보딩이란 가지고 있는 데이터를 사용자의 이름에 해당하는 특정 ID로 연결시키는 작업이다. 여러가지 사이트에 빈 공간을 구매하고 사용자가 그 사이트에 방문할 때마다 쿠키에 정보를 남긴다. 그러면 해당 사용자의 여러가지 정보를 모을 수 있다. 이 정보를 토대로 &lt;strong&gt;페이스북 밖에서 활동하는 사용자와 페이스북 사용자의 고유 ID를 연결시키는 것이다&lt;/strong&gt;. 당시 페이스북의 온보딩 기능인 CA는 90 %의 정확도로 동일 인물을 맞췄다고 한다. 따라서 광고주는 소유하고 있는 정보를 페이스북에 제공하면 해당 사용자의 고유 ID를 알 수 있는 것이다. 광고주가 가지고 있는 데이터를 통해 타겟팅을 하면 해당 ID의 페이스북 페이지에 광고가 뜨는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/mtzdhspnpp2qt2q/Screenshot%202019-05-08%2022.50.47.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;데이터 온보딩 예시: 출처 &lt;a href=&quot;https://clearcode.cc/blog/first-party-data-onboarding/&quot;&gt;https://clearcode.cc/blog/first-party-data-onboarding/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;하지만 CA가 기존 페이스북 광고 시스템을 이용한만큼 외부의 광고주들은 반드시 페이스북을 통해서만 광고를 집행할 수 있었다. 즉 페이스북에서만 광고를 결제한다는 것이다. 그래서 저자는 책에서 ‘폐쇄형’이라고 이야기한다. 하지만 이미 광고 세계에서는 프로그래매틱 광고가 트렌드가 되어있었다. 페이스북 광고 시스템만 이용하게 하기에는 무리가 있는 것이다. 이 쯤에서 저자가 주장했던 아이디어가 나온다. 바로 페이스북 익스체인지, 줄여서 FBX이다. 이후 1년동안 이 두 아이디어는 경쟁을 하다가 결국 CA가 채택되고 저자는 페이스북에서 쫒겨나게 된다.&lt;/p&gt;

&lt;p&gt;FBX는 CA와 달리 개방형이다. 페이스북 외부에 존재하던 에드 익스체인지에 페이스북 광고를 붙이는 것이다. 이렇게 하면 페이스북 광고 자체도 에드 익스체인지에 존재하는 여러 AD Network 중에 하나가 된다. 공정하게 다른 광고들과 경쟁한달까. 페이스북은 자체 사용자 데이터에 대해 그 가치를 높게 평가하면서도 다른 광고 시스템과 경쟁하기는 싫어했던 것 같다. 이렇게 하면 외부에 존재하던 광고주들이 자사의 광고 타게팅 기술을 그대로 적용시킬 수 있다. 이 때문에 FBX는 전에는 존재하지 않았던 매출을 페이스북에게 가져다준다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/r6ojeuzefxvfdqs/Screenshot%202019-05-08%2022.45.34.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;에드 익스체인지 그림: 출처 &lt;a href=&quot;http://blog.pandora.tv/2018/05/ad-tech%EC%95%A0%EB%93%9C%ED%85%8C%ED%81%AC-ad-network%EC%95%A0%EB%93%9C%EB%84%A4%ED%8A%B8%EC%9B%8C%ED%81%AC%EC%99%80-ad-exchange%EC%95%A0%EB%93%9C%EC%9D%B5%EC%8A%A4%EC%B2%B4%EC%9D%B8%EC%A7%80/&quot;&gt;http://blog.pandora.tv/2018/05/ad-tech애드테크-ad-network애드네트워크와-ad-exchange애드익스체인지/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;하지만 언제나 그렇듯 회사의 방향성은 여러 가지 다양한 조건에 따라 결정이 된다. 책에서 이야기하는 복잡한 상황에 따라 FBX 대신 CA가 살아남게 된다. 그렇다면 현재 페이스북의 광고 시스템은 뭘까? CA일까? 아니다. CA도 FBX도 아닌 제 3의 제품이 페이스북을 살렸다. 개념은 완전 간단하다. 바로 모바일 페이스북 인벤토리에 광고를 넣는 것이다. (여기에 에드 익스체인지를 붙인건지 잘 모르겠다. 모바일에서도 CA 기술 같은 게 유효한건지??) 이는 큰 세계의 흐름의 변화에 따른 성공이다. 모바일 시장이 급속도로 성장하면서 이전에 웹에서와는 전혀 다른 광고 시장이 형성된 것이다. 웹에서는 브라우저 안에서의 사용자 행동을 추적하는 것이 가능했다. 하지만 앱 세계에서 사용자가 이 앱 저 앱으로 옮겨다니면 그 행동을 추적할 수 있는 방법이 없다. 하나의 앱은 독립적인 브라우저 같은 역할을 하는 것이다.&lt;/p&gt;

&lt;p&gt;이런 상황에서 페이스북이 가진 사용자 데이터는 마케터에게는 소중한 데이터가 된 것이다. 이 기회를 놓치지 않고 페이스북은 빠르게 태세 전환을 해서 모바일 광고에 힘을 쏟아부었다. 그게 현재의 페이스북을 만든 것이다.&lt;/p&gt;

&lt;h3 id=&quot;책의-중요한-부분들&quot;&gt;책의 중요한 부분들&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;58 page: 프로그래매틱 광고에 대한 소개&lt;/li&gt;
  &lt;li&gt;76 page: 스타트업이 성공하기 위해 이뤄야할 기적&lt;/li&gt;
  &lt;li&gt;116 page: 구글이 돈을 버는 방식&lt;/li&gt;
  &lt;li&gt;120 page: 구글과 소상공인 사이의 체울 수 없는 간극&lt;/li&gt;
  &lt;li&gt;302 page: 전환 추척 시스템에 대한 이야기&lt;/li&gt;
  &lt;li&gt;369 page: 데이터를 돈으로, 그리고 CPM에 대해&lt;/li&gt;
  &lt;li&gt;390 page: 키튼, 단순한 텍스트를 의미론적인 범주로 (머신러닝 관련)&lt;/li&gt;
  &lt;li&gt;398 page: 데이터를 기반으로 수익을 올리는 것이 왜 어려운가&lt;/li&gt;
  &lt;li&gt;403 page: 최적화와 타겟팅에 대해&lt;/li&gt;
  &lt;li&gt;412 page: 광고 검토 및 품질 관리에 대해 (머신러닝 관련)&lt;/li&gt;
  &lt;li&gt;433 page: 페이스북 광고에 대한 오해&lt;/li&gt;
  &lt;li&gt;483 page: 문제의 수익화 제품인 스폰서드 스토리&lt;/li&gt;
  &lt;li&gt;493 page: 스폰서드 스토리의 실패&lt;/li&gt;
  &lt;li&gt;506 page: 페이스북의 강력한 기기통합 능력&lt;/li&gt;
  &lt;li&gt;512 page: 페이스북의 온보딩&lt;/li&gt;
  &lt;li&gt;520 page: 페이스북 CA와 FBX&lt;/li&gt;
  &lt;li&gt;553 page: 페이스북 FBX에 대한 상세설명&lt;/li&gt;
  &lt;li&gt;564 page: FBX와 아마존&lt;/li&gt;
  &lt;li&gt;576 page: 광고주 관점에서의 FBX와 CA&lt;/li&gt;
  &lt;li&gt;589 page: 광고 서버에 대한 설명&lt;/li&gt;
  &lt;li&gt;604 page: FBX의 기술적 난제&lt;/li&gt;
  &lt;li&gt;611 page: 긴 전쟁의 끝, CA의 승리&lt;/li&gt;
  &lt;li&gt;632 page: 페이스북 광고의 성공 이야기&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 08 May 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/reading/2019/05/08/monkey/</link>
        <guid isPermaLink="true">http://localhost:4003/reading/2019/05/08/monkey/</guid>
        
        
        <category>reading</category>
        
      </item>
    
      <item>
        <title>BERT 논문 리뷰</title>
        <description>&lt;p&gt;논문 링크: &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;https://arxiv.org/pdf/1810.04805.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;참고 코드 링크: &lt;a href=&quot;https://github.com/codertimo/BERT-pytorch&quot;&gt;https://github.com/codertimo/BERT-pytorch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/8p2vpf3rhjcngrx/Screenshot%202019-05-08%2023.00.58.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;abstract&quot;&gt;Abstract&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;모든 layer에서 left, right context를 모두 보는 deep bidirectinal 모델&lt;/li&gt;
  &lt;li&gt;pretrain 한 BERT 모델을 여러 task에 fine-tuning 했더니 SOTA&lt;/li&gt;
  &lt;li&gt;task-specific한 구조를 디자인 할 필요없음&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;NLP에는 sentence-level로 처리하는 task가 있고 token-level로 처리하는 task가 있다. 이 task를 잘하기 위해 pre-training 하는 전략이 잘 먹혔다. Pre-training 하는 방법에는 크게 feature-based 방법과 fine-tuning 방법이 있다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;feature-based 방법은 task-specific한 architecture를 사용하는 방법이다. pretrain 한 representation을 additional feature로 넣는 방법이다. 논문에서도 그닥 언급을 안하는 게 BERT와 방향성이 달라서인지 feature-based 방법이 별로여서인지 잘 모르겠다.&lt;/li&gt;
  &lt;li&gt;fine-tuning 방법이 BERT가 사용하는 방법인데 이전 SOTA였던 GPT도 이 방법을 사용한다. 이 방법은 task-specific한 parameter를 최소화해서 finetuning하는 방법이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;GPT가 이 논문에서 개선했다고 말하고 싶은 주요 대상이다. GPT는 unidirectional 한 모델을 사용한다. 즉 모든 토큰은 이전 토큰만 참고할 수 있는 것이다. auto-regressive하다고 이해하면 더 이해가 빠를 것이다. 다음 그림에서 OpenAI GPT를 보면 각 layer에서는 이전 input에 대한 정보만 참조하는 것을 볼 수 있다. 하지만 BERT의 경우 이전, 이후의 정보를 모두 활용한다. unidirectional은 token-level에서 큰 단점이 된다.(어떤 근거로?)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/wnf6wyhr4xqw3sg/Screenshot%202019-05-08%2023.01.29.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT의 핵심은 다음 세 가지이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;unidirectional이 아닌 bidirectional&lt;/li&gt;
  &lt;li&gt;masked language model과 next sentence prediction task를 이용한 pretraining&lt;/li&gt;
  &lt;li&gt;대규모 데이터셋을 통한 pretraining —&amp;gt; task-specific한 데이터셋에 대해 finetuning&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;그래서 얼마나 잘하나보면 다음과 같이 겁나 잘한다. 모든 GLUE task에서 SOTA를 달성했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/wb10rowfny0ituj/Screenshot%202019-05-08%2023.02.08.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SQUAD 데이터셋에서도 마찬가지로 SOTA&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/paglisu9mc38tmb/Screenshot%202019-05-08%2023.02.29.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Named Entity Recognition task인 CoNLL-2003 데이터셋에서도 SOTA&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/zz3s5a1x7m6ys8t/Screenshot%202019-05-08%2023.02.58.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;SWAG(Sentence-pair completion task) 에서도 SOTA&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/xfwzeqtfe7vy70p/Screenshot%202019-05-08%2023.03.13.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;bert&quot;&gt;BERT&lt;/h2&gt;

&lt;p&gt;모델 구조는 위 그림과 같다. 이 논문을 보려면 transformer는 기본적으로 알고 있어야한다. BERT의 네트워크 구조는 deep 하게 transformer를 쌓은 것이다. 여기서 이 transformer가 모든 layer에서 왼쪽 오른쪽 정보를 모두 활용하는 것이다. BERT의 구조를 표현하는 기호는 L(네트워크 깊이, transformer layer를 몇 개를 쌓았나), H(hidden size), A(self-attention head) 이다. 논문에서는 2개의 네트워크 구조를 제안한다. 작은 놈과 큰 놈&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;BERTBASE: L=12, H=768, A=12, Total Parameters=110M&lt;/li&gt;
  &lt;li&gt;BERTLARGE: L=24, H=1024, A=16, Total Parameters=340M&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;데이터는 다음과 같이 전처리하면 된다. BERT를 학습시킬 때 next sentence 인지 아닌지 판단하는 걸로 학습하기 때문에 하나의 입력 데이터는 두 개의 문장으로 이루어진다. 이 때, 두 개의 문장은 꼭 실제 문장 단위가 아니어도 된다. 각 문장이 token으로 바꾸는 게 tokenizer가 하는 일이다. 단어를 있는 그대로 모두 embedding으로 바꾸면 너무 네트워크 크기가 커진다. 따라서 token의 개수를 줄이는 과정이 필요한데 여기서는 WordPeice를 통해 token의 개수를 30,000개로 제한한다. 문장 길이는 최대 512 토큰으로 제한한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;word piece model 참고 링크: &lt;a href=&quot;https://lovit.github.io/nlp/2018/04/02/wpm/&quot;&gt;https://lovit.github.io/nlp/2018/04/02/wpm/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/smw4jee8m2wy9zd/Screenshot%202019-05-08%2023.03.45.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 입력 데이터를 뉴럴넷에 집어넣기 위해 representation으로 바꿔야한다. 보통 NLP에서 하듯이 embedding으로 변환하는 과정이다. Input representation으로는 하나의 토큰 자리에 세 가지 embedding이 들어간다. 첫 번째는 token embedding으로 일반적인 word embedding을 떠올리면 될 것 같다. 그리고 두 번째는 segment embedding이다. 두 개의 문장을 입력으로 넣는데 뉴럴넷이 두 개의 문장이라는 것을 구분할 방법이 필요하다. 따라서 첫 번째 문장인지 두 번째 문장인지에 대해 embedding을 한다. 그리고 세 번째는 position embedding 인데 transformer에서 position encoding 할 때와는 다르게 각 위치에 대한 embedding을 학습한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/zwtnj68rbay95j6/Screenshot%202019-05-08%2023.04.15.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;코드로 본다면 다음과 같이 세 embedding을 구하고 합친다. 합치는 연산은 단순 합이다. embedding dimension은 모두 동일하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/037h4xykpl8c6j7/Screenshot%202019-05-08%2023.05.15.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;input 토큰 중 첫번째 토큰은 [cls] 토큰으로 classification task일 때 사용하는 token 이다. 문장 사이와 input의 마지막에는 [sep] 토큰을 사용한다. input 이후의 모델 구조는 transformer이기 때문에 여기서는 설명을 생략한다. 간단히 embedding을 통과한 이후에 transformer로 들어간다라고 생각하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/739qc6pymetmpql/Screenshot%202019-05-08%2023.05.34.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;masked-lm&quot;&gt;Masked LM&lt;/h2&gt;

&lt;p&gt;BERT 모델을 pretrain 할 때는 두 가지 task로 학습한다. 그 중에 첫 번째가 masked lm이다. 왜 이 두 가지 task를 정했는지는 모르겠다. bidirectional한 모델에 맞는 task를 고른거지 싶다. 이 task는 input sequence의 15%를 random으로 mask를 씌우고 그 자리에 있던 원래 token을 맞추는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/1vzx64dtw3aevnk/Screenshot%202019-05-08%2023.05.53.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 때, 단순히 다 mask를 씌우지 않는다. 다음과 같이 세 가지 방법을 사용한다. 이렇게 하는 이유는 pretrain 한 다음에 finetuning을 하기 때문이다. pretrain 할 때는 항상 입력에 mask가 있고 finetuning을 할 때는 mask가 전혀 없다. 이러한 차이가 있기 때문에 그 자리를 항상 mask로 바꾸는 게 아니라 다른 단어로 채우는 것이다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Randomly 80% of tokens, gonna be a &lt;code class=&quot;highlighter-rouge&quot;&gt;[MASK]&lt;/code&gt; token&lt;/li&gt;
  &lt;li&gt;Randomly 10% of tokens, gonna be a &lt;code class=&quot;highlighter-rouge&quot;&gt;[RANDOM]&lt;/code&gt; token(another word)&lt;/li&gt;
  &lt;li&gt;Randomly 10% of tokens, will be remain as same. But need to be predicted.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;코드로 보면 다음과 같다. 15 퍼센트의 확률로 위 3가지의 방법으로 그 자리를 채운다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/1vzx64dtw3aevnk/Screenshot%202019-05-08%2023.05.53.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;BERT 모델의 출력으로 sequence 길이만큼이 나올텐데 그 중에서 mask가 있는 자리의 출력값만 사용한다. fully connected layer를 사용해서 vocab 크기 만큼의 softmax 출력을 내보낸다.&lt;/p&gt;

&lt;h2 id=&quot;next-sentence-prediction&quot;&gt;Next Sentence Prediction&lt;/h2&gt;

&lt;p&gt;두 번째 pretrain task는 다음 sentence를 예측하는 것이다. 데이터셋에서 각 샘플은 2개의 문장으로 구성되어있다. 50 퍼센트의 확률로 2개의 문장에서 뒤의 문장을 다른 문장으로 바꿔치기 한다. 그 다음에 다음 문장이 맞는지 안맞는지로 binary classification 하는 것이다. 이 task를 하는 이유는 pretrain model을 token-level의 task에만 활용할 것이 아니라 sentence-level에도 활용하고 싶기 때문이다. 이 task를 통해 BERT 모델은 문장 사이의 연관성에 대해서 학습할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/iq02l1bs9oululi/Screenshot%202019-05-08%2023.06.34.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;pre-training-procedure&quot;&gt;Pre-training Procedure&lt;/h2&gt;

&lt;p&gt;학습 데이터로는 BooksCorpus와 English Wikipedia를 사용했다. Masked LM loss와 Next Sentence Prediction loss를 더해서 학습한다. 구체적인 사항은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;We train with batch size of 256 sequences&lt;/li&gt;
  &lt;li&gt;for 1,000,000 steps, which is approximately 40
epochs over the 3.3 billion word corpus.&lt;/li&gt;
  &lt;li&gt;We use Adam with learning rate of 1e-4, β1 = 0.9,
β2 = 0.999, L2 weight decay of 0.01,&lt;/li&gt;
  &lt;li&gt;learning rate warmup over the first 10,000 steps, and linear
decay of the learning rate.&lt;/li&gt;
  &lt;li&gt;We use a dropout probability of 0.1 on all layers.&lt;/li&gt;
  &lt;li&gt;We use a gelu activation (Hendrycks and Gimpel, 2016)&lt;/li&gt;
  &lt;li&gt;Training of BERT BASE was performed on 4 Cloud TPUs in Pod configuration (16 TPU chips total).5 Training of BERT LARGE was performed on 16 Cloud TPUs (64 TPU chips total). Each pre-training took 4 days to complete.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;fine-tuning-procedure&quot;&gt;Fine-tuning Procedure&lt;/h2&gt;

&lt;p&gt;BERT 모델은 token-level의 task에도 sentence-level의 task에도 활용할 수 있다. sentence-level의 task는 sentence classification이다. token-level task는 question answering, Named entity recognition이다. classification을 할 때는 맨 첫번째 자리의 transformer의 output을 활용한다. 그리고 question answering의 경우는 첫 번째 문장에 question, 두 번째 문장에 paragraph를 넣어서 입력 데이터를 만든다. 정답을 만드는 과정은 다음 수식과 같다. 네트워크가 맞춰야 하는 것은 answer가 paragraph에서 어디서 시작해서 어디에서 끝나는 지이다. 따라서 start vector와 end vector만 학습하면 된다. start vector는 transformer의 dimension의 크기를 가진다. paragraph의 각 위치 마다의 output인 T와 start vector를 dot product해서 softmax를 취한다. end vector도 마찬가지로 probability를 구하고 정답과 cross-entropy를 취해서 학습한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/1prhl3jrs0ckyyo/Screenshot%202019-05-08%2023.07.13.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;참고-자료&quot;&gt;참고 자료&lt;/h2&gt;

&lt;p&gt;레딧 bert 관련 토론: &lt;a href=&quot;https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/&quot;&gt;https://www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Wed, 08 May 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/nlp/2019/05/08/BERT/</link>
        <guid isPermaLink="true">http://localhost:4003/nlp/2019/05/08/BERT/</guid>
        
        
        <category>nlp</category>
        
      </item>
    
      <item>
        <title>2018년을 마무리하며, 2019년을 맞이하며 드는 생각들</title>
        <description>&lt;h2 id=&quot;2018년을-마무리하며-2019년을-맞이하며-드는-생각들&quot;&gt;2018년을 마무리하며, 2019년을 맞이하며 드는 생각들&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;항상 새로운 환경과 새로운 업무를 하는 것은 두려움으로 다가온다. 하지만 조금의 두려움을 이겨내고 나면 나와 마주칠 수 있는 시간이 된다. 두려워하지말고 자신을 좀 더 믿자.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;조직이라는 것에 대해서는 항상 생각을 하는 것이 좋다. 조직은 마치 생명과 같아서 꾸준한 관심을 가지지 않으면 죽기 마련이다. 생동감 넘치는 조직을 만들기 위해서는 문화라는 밥을 먹어야한다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;문화를 형성하는 것은 상당히 어렵다. 내가 생각하는 문화의 모습을 상세히 적어보고 그렇게 할 수 있는 사람을 찾아라.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;당장 필요에 의해 움직이다보면 자연스레 나의 바닥이 드러난다. 관계는 점점 피상적으로 될 것이다. 당장의 이익이 아닌 좀 더 먼 미래의 가치에 의해 움직이는 사람이 되는 것이 좋다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;아무 유익이 안될 것 같다고 생각할지라도 사랑으로 움직이는 것이 중요하다. 결국 사람은 사랑을 바탕으로 인격을 형성하고 그 인격이 정체성이라는 집을 짓는다. 사랑으로 움직이지 않는 순간 순수한 자신을 잃어버리게 될 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;일 뿐만 아니라 다른 것들도 세상에 중요한 것이 많다. 나이가 들고 경험이 늘면서 배워야 할 것은 ‘적절함’ 또는 ‘균형’이다. 항상 생각하는 것이지만 어렵다는 것도 변함이 없다. 뭔가 커다란 돌파구를 만들려면 균형을 깨고 하나에만 쏟아부어야 할 수도 있다. 그렇다고 해서 다른 것들을 포기하면 안된다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모든 일을 할 수 없고 모든 사람에게 사랑받을 수 없다. 인생에서 뭐가 중요한지를 잘 정리하고 항상 되새김질 해야한다. 무엇이 중요한지가 정해져있으면 그 때 그 때 판단하기가 수월해 진다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사업 자체로 세상을 더 좋은 곳으로 만들수도 있고 사업을 통해 번 돈으로 사회를 좋은 곳으로 만들 수 있다. 어느 것이 옳다할 수 없다. 어느 것이 더 쉽다 할 수도 없다. 자신의 마음의 소리에 귀를 기울이고 그에 따라 움직여라.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;공감하는 능력을 꾸준히 올려야한다. 신경쓰지 않다보면 자기중심적인 사고를 하게 될 것이다. 사람은 혼자 살 수 있는 존재가 아니다. 누군가로부터 받는 것도 좋지만 주는 기쁨이 더 큰 기쁨이다. 물질적으로 줄 수도 있지만 공감으로 마음에 선물을 줄 수도 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사람은 착할 수도 있고 악할 수도 있다는 사실을 깨달아야한다. 흑백논리는 통하지 않는다. 나 자신도 그럴 수 있다. 사람을 판단하기 전에 사람의 복잡성을 이해할 필요가 있다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;쉽게 시작하지마라. 쉽게 시작하면 쉽게 그만두게 된다. 무엇이든 한다면 제대로 할 마음가짐이 필요하다. 그 일 하나 하나를 할 때마다 평판을 쌓아가는 것이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;필요한 조건들을 채우기 보다는 필요한 마음가짐을 갖춰라. 필요한 조건은 변하기 마련이지만 마음가짐과 태도는 항상 동일하다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;대표처럼 생각하고 일하려고 노력해라. 그런 사람은 어디에서나 꼭 필요한 사람이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;무슨 일을 하든지 주변 사람을 도우려고 노력해라. 성공한다면 어떻게 사람들을 도울 것인지 미리 생각해라. 사람을 돕는 것은 또한 나를 돕는 일이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;나를 전혀 모르는 사람들과의 만남을 주기적으로 가져라. 내가 속해있는 조직, 내가 쌓아올린 성과가 아닌 나라는 사람 자체에 대해 생각할 시간이 필요하다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 01 Jan 2019 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/writing/2019/01/01/writing-practice-13/</link>
        <guid isPermaLink="true">http://localhost:4003/writing/2019/01/01/writing-practice-13/</guid>
        
        <category>paper</category>
        
        
        <category>writing</category>
        
      </item>
    
      <item>
        <title>CIFAR-10 정복 시리즈 3: Shake-Shake</title>
        <description>&lt;h2 id=&quot;cifar-10-정복하기-시리즈-소개&quot;&gt;CIFAR-10 정복하기 시리즈 소개&lt;/h2&gt;
&lt;p&gt;CIFAR-10 정복하기 시리즈에서는 딥러닝이 CIFAR-10 데이터셋에서 어떻게 성능을 높여왔는지 그 흐름을 알아본다. 또한 코드를 통해서 동작원리를 자세하게 깨닫고 실습해볼 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/07/start_cifar10/&quot;&gt;CIFAR-10 정복 시리즈 0: 시작하기&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/09/resnet/&quot;&gt;CIFAR-10 정복 시리즈 1: ResNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/&quot;&gt;CIFAR-10 정복 시리즈 2: PyramidNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/25/shake_shake/&quot;&gt;CIFAR-10 정복 시리즈 3: Shake-Shake&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;관련 코드 링크
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/dnddnjs/pytorch-cifar10&quot;&gt;pytorch cifar10 github code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cifar-10-정복-시리즈-3-shake-shake&quot;&gt;CIFAR-10 정복 시리즈 3: Shake-Shake&lt;/h2&gt;
&lt;p&gt;이전 포스트에서는 ResNet의 구조를 변형시킨 모델을 알아봤다. PyramidNet은 학습할 때 error rate가 거의 0이라고 볼 수 있다. 학습 error rate에 비해 테스트 error rate는 여전히 높기 때문에 regularization에 대해 생각해봐야한다. CIFAR은 학습 데이터양이 적은데 비해 네트워크의 representation power는 높다. 따라서 &lt;strong&gt;overfit&lt;/strong&gt;이 일어나기 쉽다. CIFAR에서의 overfit 문제를 해결하고자 하는 것이 &lt;strong&gt;Shake-Shake regularization&lt;/strong&gt;&lt;sup id=&quot;fnref:0&quot;&gt;&lt;a href=&quot;#fn:0&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;이다. Shake-Shake는 네트워크의 forward pass와 backward pass에서 noise를 주는 방식이다. 하지만 Shake-Shake는 &lt;strong&gt;ResNeXt&lt;/strong&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;의 구조의 네트워크에만 적용할 수 있다. 이 post에서는 Shake-Shake를 살펴보도록 하겠다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#resnext&quot;&gt;ResNeXt&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#fractalnet&quot;&gt;FractalNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#shake-shake&quot;&gt;Shake-Shake&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#code-review&quot;&gt;Code Review&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#squeeze-and-excitation&quot;&gt;Squeeze and Excitation&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;딥러닝에서 regularization은 overfit을 방지하기 위한 방법으로 많이 사용되고 있다. 그동안 사용되어왔던 regularization 효과를 가지는 방법들로는 weight decay, dropout, batch-normalization, SGD 등이 있다. PyramidNet 포스트에서 살펴봤던 ResDrop 또한 regularization에 해당한다. 네트워크 자체는 점점 강력해지지만 generalization 성능은 그만큼 따라오지 않기 때문에 이 이외에 추가적인 노력이 이어졌다. 기존 residual block는 2 branch로 구성되어있다. 한 branch는 idenity mapping이고 다른 branch는 nonlinear computation이 이뤄진다. &lt;strong&gt;ResNeXt&lt;/strong&gt;는 이런 기본적인 구성을 벗어나서 2개의 branch 이상의 n개의 branch를 사용한다. &lt;strong&gt;FractalNet&lt;/strong&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;의 경우도 ResNeXt와 유사하게 여러 개의 subpath를 사용한다. FractalNet은 drop path라는 regularization 방법을 사용한다. &lt;strong&gt;Shake-Shake&lt;/strong&gt;는 ResNeXt와 drop path를 적절히 합친 것이라고 볼 수 있다. 따라서 Shake-Shake를 살펴보기 전에 ResNeXt와 FractalNet을 간단히 살펴보겠다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;resnext&quot;&gt;ResNeXt&lt;/h2&gt;
&lt;p&gt;ResNeXt는 기본적으로 &lt;strong&gt;multi-branch ResNet이&lt;/strong&gt;라고 보면 된다. 기존에 residual block을 design할 때 activation의 순서를 바꿔보거나(pre-activation ResNet) 혹은 convolution의 filter 수를 변화시켰다(WideResNet, PyramidNet). 하지만 ResNeXt는 그 이외에 &lt;strong&gt;cardinality&lt;/strong&gt;라는 개념을 소개한다. 다음 그림에서 왼쪽이 일반적인 residual block이다. 오른쪽이 ResNeXt의 residual block이다. Shortcut connection은 그대로 하나이지만 residual 부분이 여러개인 것을 볼 수 있다. Cardinality는 residual의 개수이다. 여러 path의 output은 summation으로 합친다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/whhjpdfwzkcoahu/Screenshot%202018-11-23%2019.58.46.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1611.05431.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ResNeXt의 multi-branch는 GoogLeNet의 &lt;strong&gt;Inception module&lt;/strong&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;과 상당히 유사하다. 다음 그림의 Inception module이다. ResNeXt의 residual block은 Inception module과 다르게 각 path마다 모두 동일한 구조를 지니며 dimension이 모두 같다. Inception module은 hyper parameter가 많기 때문에 디자인하기 어렵다면 ResNeXt는 단순히 몇 개의 path를 사용하는지만 설정하기 때문에 상당히 간편하다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/frvr8g5vaojayw7/Screenshot%202018-11-23%2020.04.08.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1409.4842.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;fractalnet&quot;&gt;FractalNet&lt;/h2&gt;
&lt;p&gt;FractalNet은 Residual을 학습시키는 기존의 ResNet 변형체들과 다른 방식이다. FractalNet은 Residual을 학습하는 방식을 사용하지 않아도 네트워크를 깊게 쌓을 수 있다는 것을 보여준다. 아래 그림이 FractalNet의 fractal block의 구조를 보여준다. 가장 왼쪽은 fractal block을 형성하는 기본적인 방법을 보여준다. 보통 &lt;code class=&quot;MathJax_Preview&quot;&gt;f_C&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f_C&lt;/script&gt;로는 convolution을 사용하는데 확장할 때는 하나의 convolution이 오른쪽에 두 개로 합쳐진다. 그 다음 왼쪽에 다른 하나의 convolution을 붙이고 그 출력들을 join 연산을 통해 합친다. 이렇게 만든 fractal block은 가운데 그림과 같다. Residual block에서 볼 수 있는 residual과 identity mapping의 구조는 볼 수 없다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/kngx40hcgf8bg45/Screenshot%202018-11-23%2020.25.40.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1605.07648.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;기존 residual block에서는 2 branch가 identity mapping과 residual learning 이라는 각자의 역할을 수행했다. 하지만 fractal block의 경우 여러 path가 존재하는데 서로 중복된 역할을 할 수 있다. Dropout이 이러한 &lt;strong&gt;co-adaptation&lt;/strong&gt; 문제를 해결하려고 하나의 neuron 단위에 적용되었다. Fractal block에서는 &lt;strong&gt;co-adapatation&lt;/strong&gt; 문제를 해결하기 위해 path를 drop 해버리는 drop path를 사용한다. Drop path의 작동하는 예시는 다음 그림과 같다. Drop path는 두 가지 방식으로 작동한다. Local 방식은 다음 그림에서 형광색에 해당하는 join layer에서 랜덤하게 인풋을 drop해버린다. Global 방식은 두 번째, 네 번째 그림에서 보듯이 전체 block 내부에서 하나의 path만 선택한다. 이렇게 path를 drop해버리는 것으로 regularization 효과를 볼 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/ednae8p9cag0hin/Screenshot%202018-11-23%2020.26.01.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1605.07648.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음은 FractalNet의 실험결과이다. 20 layers에 38.6M 사이즈의 FractalNet을 보면 CIFAR-10에서 augmentation이 없을 경우 10.18 %의 error rate를 얻는 것을 볼 수 있다. 하지만 drop-path와 dropout을 사용할 경우 3% 정도의 성능이 향상된다. Data augmentation을 적용한 CIFAR-10에 대해서도 0.6 % 정도의 성능 향상을 볼 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/2tqzbx4lxti64g5/Screenshot%202018-11-23%2023.53.07.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1605.07648.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;shake-shake&quot;&gt;Shake-Shake&lt;/h2&gt;
&lt;p&gt;Shake-Shake는 ResNeXt와 Drop-path를 합친 것이라고 볼 수 있다. ResNeXt에서 여러 branch의 output을 합칠 때 단순히 summation으로 합친다. 하지만 Shake-Shake에서는 stochastic affine transform을 통해서 합치겠다는 것이 아이디어이다. 다음 그림이 Shake-Shake의 작동 방식을 알려준다. ResNeXt의 경우 32개의 branch까지도 사용했는데 Shake-Shake에서는 2개의 branch만 사용한다. 이 2개의 branch를 사용해서 regularization 하는 것이 핵심이다. Shake-Shake는 forward pass에서 한 번, backward pass에서 한 번 &lt;strong&gt;stochastic affine transform&lt;/strong&gt;을 수행한다. 이 affine transform은 일종의 &lt;strong&gt;augmentation&lt;/strong&gt;이라고 볼 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/t2ijf2ahf5dkxa1/Screenshot%202018-10-13%2019.32.12.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1705.07485.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
Shake-Shake에서 특정 block의 forward는 다음 수식과 같다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;는 확률변수로서 0에서 1 사이의 랜덤한 숫자이다. 하나의 path는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;를 곱하고 다른 하나의 path는 &lt;code class=&quot;MathJax_Preview&quot;&gt;1-\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1-\alpha&lt;/script&gt;를 곱해서 더한 것이 residual이 된다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;는 학습할 때 매 mini-batch마다 새로 뽑는다. Backward pass에서도 비슷하게 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;라는 0에서 1 사이의 확률변수를 뽑아서 두 개의 다른 path로 가는 gradient에 그 값을 곱해준다. 위 그림에서 두 번째가 backward pass를 의미한다. 세 번째는 test 할 때를 말하는 것인데 test 할 때는 두 개의 path에 0.5씩 곱한 다음에 더한다. Shake-Shake는 drop path와 같이 하나의 path를 없애버리는 방식이 아니라 두 개의 path를 랜덤하게 섞어버리는 방식을 사용했다. 이러한 방식을 Shake-Shake 만의 novelty라고 볼 수 있다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;x_{i+1} = x_i + \alpha_i F(x_i, W_i^{(1)}) + (1- \alpha_i )F(x_i, W_i^{(2)})&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{i+1} = x_i + \alpha_i F(x_i, W_i^{(1)}) + (1- \alpha_i )F(x_i, W_i^{(2)})&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Forward pass와 backward pass에서는 각각 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;라는 random number를 뽑아야한다. 이 때 새로운 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha, \beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha, \beta&lt;/script&gt;를 뽑는 방법에는 여러 가지가 있다. Pass 할 때마다 새로운 random number를 뽑는 것을 “Shake”라고 하며 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;를 따로 따로 pass마다 새로 뽑는 것을 &lt;strong&gt;Shake-Shake&lt;/strong&gt;라고 한다. Shake-Shake 방식이 제일 성능이 좋기 때문에 논문의 이름이 Shake-Shake Regularization이 된 것이다. 다음 표는 여러가지 random number 추출 방식에 따른 성능 비교를 보여준다. Shake-Shake Image 방식이 &lt;strong&gt;2.86%&lt;/strong&gt;로 가장 높은 성능을 달성한 것을 볼 수 있다. Level이라는 것이 있는데 Batch는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha, \beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha, \beta&lt;/script&gt;를 하나의 mini-batch 안에서 공유하겠다는 것이고 Image는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha, \beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha, \beta&lt;/script&gt;를 mini-batch안의 image마다 다르게 사용하겠다는 것을 뜻한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/1cso45sjfpj7aap/Screenshot%202018-11-24%2000.17.41.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1705.07485.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Shake-Shake 모델은 3개의 stage를 가지는데 각 stage는 4개의 residual block으로 구성된다. 따라서 네트워크 전체의 깊이는 26이 된다. 위 표에서 Model 부분에 26 2x96d라고 써져있는데 이건 네트워크가 26층의 깊이를 가지며 2개의 branch를 사용하고 첫 residual block의 width가 96이라는 것을 의미한다. 2점대의 error rate라는 꽤나 인상적인 결과를 보여주는 Shake-Shake의 코드를 한 번 살펴보자.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;code-review&quot;&gt;Code Review&lt;/h2&gt;
&lt;p&gt;코드에서는 Shake-Shake 26 2-32d 모델을 살펴볼 것이다. Shake-Shake의 residual block은 다음과 같다. 각 부분을 따로 살펴보겠다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ShakeBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ShakeBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SkippingBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shake_shake&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ShakeShake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
        &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;        
            &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shake_shake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shake_shake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;skip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;skip&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Residual branch는 따로 class로 정의를 해놓았다. 각각의 branch는 self.residual_branch1과 self.residual_branch2로 정의한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
residual branch의 코드는 다음과 같다. 일반적인 Residual block에서 residual branch에 해당하는 부분만 들어있다. Residual branch는 ReLU-Conv3x3-BN-ReLU-Conv3x3-BN-Mul 으로 구성된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ResidualBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResidualBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                               &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                               &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
다시 ShakeBlock으로 돌아간다. Residual Branch를 통해 정의된 residual branch들은 각각 out1, out2의 출력을 낸다. nn.Module을 상속할 경우 self.training을 통해 학습 중인지 아닌지를 알아낼 수 있다. 만약 학습 중이라면 self.training이 True가 되고 이 때 alpha와 beta를 image 단위로 랜덤하게 뽑아야한다. torch.rand라는 함수를 사용해서 alpha와 beta를 sampling한 다음에 feature dimension에 맞춰준다. out1과 out2에 alpha를 적용하는 함수가 self.shake_shake이며 custom module로 따로 정의되어있다. 이 때 beta도 함께 인자로 넣어주는데 pytorch에서 forward pass에서의 값을 저장해놓고 backpropagation을 하기 때문에 forward pass에서 beta의 정보를 넣어줘야한다. self.shake_shake에서 반환된 out은 shortcut과 더힌다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;residual_branch2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;        
        &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rand&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shake_shake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shake_shake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;self.shake_shake는 ShakeShake라는 클래스를 통해 정의된다. ShakeShake는 Shake-Shake 코드의 핵심이라고 할 수 있다. 이 코드를 작성할 때 pytorch tutorial&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;과 pytorch discuss&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;를 참고했다. 원래 backpropagation 할 때는 forward pass에서 곱해졌던 상수값을 기억해서 gradient에 곱해준다. 하지만 Shake-Shake에서는 forward pass와 backward pass에서 다른 상수값을 사용하기 때문에 이와 같이 custom을 해야 한다. ctx.save_for_backward에 인자로 넣으면 backward 할 때 그 값들을 호출할 수 있다. backward 함수에서 아까 저장했던 tensor를 불러온다. 불러온 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;값을 각각의 branch로 내려가는 두 개의 gradient에 곱해준다. 한 gradient에는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;를 곱하고 한 branch에는 &lt;code class=&quot;MathJax_Preview&quot;&gt;1 - \beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1 - \beta&lt;/script&gt;를 곱해준다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ShakeShake&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;autograd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;save_for_backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;

    &lt;span class=&quot;o&quot;&gt;@&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;staticmethod&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;input1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ctx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;saved_tensors&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grad_input1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_output&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;grad_input2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;beta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_output&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_input1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;grad_input2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ShakeBlock의 forward pass에서 shortcut을 downsampling 하는데 보통 downsample의 방식과는 다르다. 보통 residual block에서 shortcut connection을 down sample 할 때 feature map의 사이즈를 반으로 줄이고 channel 수를 2배로 늘린다. ResNet에서는 max-pooling으로 사이즈를 반으로 줄이고 zero-padding으로 channel 수를 늘렸다. Shake-Shake에서는 특이하게도 입력을 2개의 branch를 만들어서 channel 방향으로 concatenate 한다. 첫 번째 branch는 들어온 입력을 average pooling한 이후에 1x1 convolution을 통과시킨다. 두 번째 branch에서는 입력을 왼쪽 위로 1 step만큼 shift한 이후에 padding을 통해 원래 입력의 feature map size를 유지한다. 그 이후에 1x1 convolution을 통과시킨다. 두 branch의 output인 out1과 out2를 channel 방향으로 concatenate 한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;SkippingBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SkippingBranch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_pool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AvgPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                   &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;shift_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;shift_x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shift_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shift_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Shake-Shake 네트워크의 전체 구조는 ShakeResNet에 정의되어 있다. ResNet의 전체 네트워크 코드와 동일하다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ShakeResNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ShakeResNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c1&quot;&gt;# feature map size = 32x32x32
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# feature map size = 32x32x64
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c1&quot;&gt;# feature map size = 32x32x128
&lt;/span&gt;        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_pool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AvgPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;m&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kaiming_normal_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fan_out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                        &lt;span class=&quot;n&quot;&gt;nonlinearity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;elif&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;isinstance&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;weight&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;constant_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    
        &lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModuleList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
      
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stage3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;일반적인 ResNet과 또 다른 점은 학습 epoch 수이다. Shake-Shake는 forward pass와 backward pass에 일종의 노이즈를 주입하기 때문에 regularization 효과를 보는 대신 학습이 느려진다. 따라서 기존 ResNet과 같이 일정 update step마다 learning rate를 0.1배 하는 것은 맞지 않다. 대신 &lt;strong&gt;cosine annealing&lt;/strong&gt;&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;을 사용한다. Cosine annealing은 learning rate를 cosine 함수의 형태로 decay 하겠다는 것이다. 다음 그림이 cosine annealing에서 learning rate가 iteration에 따라 어떻게 감소하는지를 보여준다. 처음 몇 epoch 동안은 높은 learning rate로 빠르게 local minimum을 찾고 그 이후 learning rate를 decay하면서 minimum에 가까이 다가가고 마지막 epcoh 동안에는 천천히 움직이다가 학습을 마무리한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/6rcn2w05zye3yhz/Screenshot%202018-11-26%2000.13.33.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
    https://towardsdatascience.com/https-medium-com-reina-wang-tw-stochastic-gradient-descent-with-restarts-5f511975163
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Cosine annealing은 코드로 다음과 같이 구현할 수 있다. PyTorch의 lr_scheduler에서 custom learning rate scheduling을 할 수 있는 LambdaLR을 사용한다. 결국 cosin_annealing 함수를 호출하는 것이다. 이 함수에서는 lr_max에서 lr_min 까지 decay하는 함수의 형태를 정의하고 있다. Shake-Shake에서 첫 learning rate 곧 lr_max는 0.2이고 annealing을 1800 epoch 동안 수행한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_cosine_annealing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr_max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr_min&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lr_max&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr_min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cos&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;cosine_annealing_scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr_scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LambdaLR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;lr_lambda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_cosine_annealing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# since lr_lambda computes multiplicative factor
&lt;/span&gt;            &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scheduler&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Shake-Shake 네트워크의 학습 과정은 다음과 같다. 다른 네트워크에 비해 1800 epoch을 학습하기 때문에 학습이 오래 걸린다. Best test error는 3.79%이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/1ptvuux24wr5qwj/Screenshot%202018-11-27%2014.42.56.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;squeeze-and-excitation&quot;&gt;Squeeze and Excitation&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;참고문헌&quot;&gt;참고문헌&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:0&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1705.07485.pdf &lt;a href=&quot;#fnref:0&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1611.05431.pdf &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1605.07648.pdf &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1409.4842.pdf &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;https://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;https://discuss.pytorch.org/t/why-input-is-tensor-in-the-forward-function-when-extending-torch-autograd/9039 &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1608.03983.pdf &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Thu, 25 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/cifar10/2018/10/25/shake_shake/</link>
        <guid isPermaLink="true">http://localhost:4003/cifar10/2018/10/25/shake_shake/</guid>
        
        <category>dl</category>
        
        
        <category>cifar10</category>
        
      </item>
    
      <item>
        <title>CIFAR-10 정복 시리즈 2: PyramidNet</title>
        <description>&lt;h2 id=&quot;cifar-10-정복하기-시리즈-소개&quot;&gt;CIFAR-10 정복하기 시리즈 소개&lt;/h2&gt;
&lt;p&gt;CIFAR-10 정복하기 시리즈에서는 딥러닝이 CIFAR-10 데이터셋에서 어떻게 성능을 높여왔는지 그 흐름을 알아본다. 또한 코드를 통해서 동작원리를 자세하게 깨닫고 실습해볼 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/07/start_cifar10/&quot;&gt;CIFAR-10 정복 시리즈 0: 시작하기&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/09/resnet/&quot;&gt;CIFAR-10 정복 시리즈 1: ResNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/&quot;&gt;CIFAR-10 정복 시리즈 2: PyramidNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/25/shake_shake/&quot;&gt;CIFAR-10 정복 시리즈 3: Shake-Shake&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;관련 코드 링크
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/dnddnjs/pytorch-cifar10&quot;&gt;pytorch cifar10 github code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cifar-10-정복-시리즈-2-pyramidnet&quot;&gt;CIFAR-10 정복 시리즈 2: PyramidNet&lt;/h2&gt;
&lt;p&gt;이전 포스트에서는 ResNet에 대해서 알아봤다. ResNet이 등장한 2015년 이후로 ResNet 기반의 새로운 네트워크가 많이 나왔다. 이번 포스트에서는 ResNet 이후 모델 구조에 관한 논문 중에 대표적인 3개 논문을 살펴볼 것이다. &lt;strong&gt;WideResNet&lt;/strong&gt;&lt;sup id=&quot;fnref:0&quot;&gt;&lt;a href=&quot;#fn:0&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;은 ResNet의 깊이가 깊어지면 학습이 어렵다는 점을 해결하고자 깊게 쌓는 것이 아니라 넓게 쌓는 방법을 제안했다. &lt;strong&gt;DenseNet&lt;/strong&gt;&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;은 ResNet의 shortcut connection을 주의깊게 보고 더 dense한 connection 방법을 제안했다. &lt;strong&gt;PyramidNet&lt;/strong&gt;&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;은 ResNet의 down sampling 부분에 주목했다. ResNet에서 donw sampling을 특정 layer에서 수행하는 것이 아니라 모든 layer에서 나눠서 하는 것이 PyramidNet의 아이디어이다. 자 이제 하나씩 살펴보자.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#wideresnet&quot;&gt;WideResNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#densenet&quot;&gt;DenseNet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#pyramidnet&quot;&gt;PyramidNet&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;wideresnet&quot;&gt;WideResNet&lt;/h2&gt;

&lt;p&gt;ResNet은 1000 layer 이상의 네트워크도 학습이 되도록 했다. 하지만 100 layer 대의 ResNet 보다 1000 layer 이상의 네트워크가 오히려 성능이 안좋아지는 문제가 생겼다. 물론 이전 포스트에서 Kaiming He의 후속 논문인 &lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt;&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;에서 이 문제를 어느정도 해결했다는 것을 봤다. Wide ResNet은 이와 같은 activation 순서에 대한 연구를 통해 네트워크의 &lt;strong&gt;depth&lt;/strong&gt;를 늘리는 것이 아닌 다른 방법을 제안하고 있다. 네트워크의 &lt;strong&gt;width&lt;/strong&gt;를 키움으로서 네트워크의 성능을 올리는 것이다.&lt;/p&gt;

&lt;p&gt;Wide ResNet 저자가 이러한 생각을 한 이유는 identity mapping에 대한 생각 때문이다. 논문에서는 Identity mapping이 ResNet의 강점인 동시에 약점이라고 주장한다. Identity mapping 즉 shortcut connection은 아무 파라메터가 없다. 따라서 gradient가 shortcut connection을 따라서 흐를 경우 네트워크는 아무것도 배우지 않을 가능성이 있다. 그 결과 깊은 ResNet의 layer 들 중에서 일부만 유용한 representation을 학습하거나 여러 layer들이 정보를 조금씩 나눠서 가지고 있을 수 있다.&lt;/p&gt;

&lt;p&gt;이것은 마치 fully-connected layer에서 생기는 co-adaptation 문제와 유사하다. Fully-connected layer에서는 이 문제를 해결하기 위해 &lt;strong&gt;dropout&lt;/strong&gt;&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;을 사용한다. ResNet에서도 따라서 일부 layer에 유용한 respresentation이 몰려있는 현상이 발생하기 때문에 &lt;strong&gt;ResDrop&lt;/strong&gt;&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;과 같은 논문이 나왔다. Dropout이 랜덤하게 특정 뉴런을 사용안했다면 ResDrop은 특정 residual block을 랜덤하게 사용안한다. 다음 그림은 ResDrop의 작동방식을 보여준다. 네트워크의 뒤로 갈수록 50%에 가까운 확률로 residual learning을 하는 weight를 비활성화한다. &lt;strong&gt;Stochastic depth&lt;/strong&gt;를 사용하면 CIFAR-10 데이터에서 1% 이상의 accuracy가 오른다. 따라서 깊은 네트워크의 layer 중에 일부만 유용한 representation을 학습했다는 주장을 하는 것이다. 따라서 단순히 깊이를 늘리는 것은 학습 성능을 올리는데 좋은 방법이 아닐 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/k3ac39dapahvasa/Screenshot%202018-11-20%2023.02.25.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1603.09382.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
ResNet은 네트워크를 깊게 쌓기 위해 residual block의 convolution filter의 개수를 최소화했다. 심지어 bottleneck block을 사용함으로서 더 적은 filter를 사용하였다. Wide ResNet에서 말하는 width는 이 convolution filter 개수를 의미한다. Wide ResNet은 깊이를 늘리는 것에는 관심이 없기 때문에 residual block의 convolution filter 수를 늘렸다. 기존의 residual block이 아래 그림의 제일 왼쪽이라면 논문에서 제안하는 wide residual block은 오른쪽 두 개와 같다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/f5dwsef7crx97f7/Screenshot%202018-10-12%2018.46.05.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1605.07146.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;CIFAR 데이터셋에 적용된 ResNet의 경우 입력 이미지 사이즈가 32 –&amp;gt; 16 –&amp;gt; 8으로 줄면서 width는 16 –&amp;gt; 32 –&amp;gt; 64로 늘어난다. Wide residual block에서는 이 width를 정수배로 늘린다. WideResNet은 16부터 40의 깊이를 가진다. 깊이가 얕을수록 residual block의 width는 커진다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/7h5whxvdthu8y18/Screenshot%202018-10-12%2018.15.28.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1605.07146.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;WideResNet 논문에서 n=40, 28, 22, 16 그리고 k = 1, 2, 4, 8, 10, 12에 대해 실험했다. 그 결과는 다음과 같다. CIFAR10에 대해서는 n=28 / k=10이 제일 좋은 정확도를 가진다. n이 28이면 conv2, conv3, conv4의 block의 수는 (28-4) / 6 = 4이다. conv2의 width는 16x10 = 160이고 conv3의 width는 32x10인 320이다. conv4의 width는 64x10인 640이다. WideResNet-28-10의 경우 CIFAR-10에서 4.17%의 error rate를 가진다. 이 때 파라메터의 수는 36.5M인데 ResNet-1001이 10.2M인 것과 비교하면 3배 이상 parameter가 많다. ResNet-1001과 비슷한 파라메터 수를 가지는 WideResNet-16-8과 비교했을 때 ResNet-1001과 거의 동일한 성능을 가진다. ResNet-1001의 CIFAR-10에서의 error rate는 4.92%이고 WideResNet-16-8은 4.81%이다. 또한 기존 ResNet-1001보다 WideResNet-28-10이 파라메터 수가 3배 이상 많음에도 accuracy가 더 높은 것을 보아 WideResNet이 기존 ResNet보다 학습이 잘 되는 것을 확인할 수 있다.&lt;/p&gt;

&lt;figure&gt;
   &lt;img src=&quot;https://www.dropbox.com/s/8kaggunevtssp8r/Screenshot%202018-10-12%2021.02.35.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1605.07146.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;여기서 좀 더 나아가 WideResNet에 dropout을 적용하면 State-of-the-art를 달성할 수 있었다. WideResNet의 경우 width를 넓히면서 residual block의 parameter 수가 많아졌다. 따라서 추가적인 regularization이 필요할 수 있는데 실제로 dropout을 convolutional layer 사이에 넣으면 성능이 향상된다. 위에서 4.17% error를 가지는 WideResNet-28-10에 dropout을 적용하면 3.89%의 error rate를 달성할 수 있다. Dropout rate는 0.3을 사용하였다.&lt;/p&gt;

&lt;figure&gt;
   &lt;img src=&quot;https://www.dropbox.com/s/bugro9g0uxbx40t/Screenshot%202018-10-12%2021.13.13.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1605.07146.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
간단히 wide residual block의 forward pass를 보면 다음과 같다. 기억해야할 것은 WideResNet에서는 Pre-activation residual block의 구조를 사용한다. 따라서 batch normalization + relu + conv의 순서로 연산을 진행한다. 첫번째 conv 이후에 dropout을 사용한다. 이제 DenseNet을 살펴보자.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;densenet&quot;&gt;DenseNet&lt;/h2&gt;

&lt;p&gt;WideResNet이 네트워크의 width에 초점을 맞췄다면 DenseNet은 ResNet의 &lt;strong&gt;Shortcut connection&lt;/strong&gt;에 초점을 맞췄다. ResNet과 Highway network, ResDrop 모두 성능을 높이기 위해 앞쪽 layer에서 뒤쪽 layer로 가는 &lt;strong&gt;shortcut connection&lt;/strong&gt;을 사용했다. 이 때 바로 전 layer의 출력만을 다음 layer로 보냈지만 DenseNet은 &lt;strong&gt;이전의 많은 layer의 출력을 한꺼번에 받는다&lt;/strong&gt;. 상당히 간단한 아이디어로 다음 그림을 통해 이해할 수 있다. 다음 그림은 ResNet에서 conv1, conv2, conv3에 해당하는 DenseNet의 DenseBlock을 그림으로 그린 것이다. 기존의 Residual block이 &lt;code class=&quot;MathJax_Preview&quot;&gt;x_l = H_l(x_{l-1}) + x_{l-1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x_l = H_l(x_{l-1}) + x_{l-1}&lt;/script&gt;이라고 한다면 DenseBlock에서 한 layer는 &lt;code class=&quot;MathJax_Preview&quot;&gt;x_l = H_l([x_0, x_1, .... , x_{l-1}])&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x_l = H_l([x_0, x_1, .... , x_{l-1}])&lt;/script&gt; 이라고 표현할 수 있다. Residual block과의 차이점은 DenseBlock 내부에서 이전 layer의 입력을 모두 받는다는 것과 addition이 아닌 concatenation으로 입력을 합친다는 것이다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/qlw9b3vad5osrqa/Screenshot%202018-10-11%2016.16.10.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1608.06993.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
DenseNet의 전체 구조는 다음과 같다. 입력이 들어오면 convolution을 하나 거친 이후에 DenseBlock을 통과한다. Dense Block에서는 이전 Residual block과는 달리 down sampling이 일어나지 않는다. 대신 &lt;strong&gt;transition layer&lt;/strong&gt;를 두어서 여기서 down sampling을 한다. Down sampling으로는 average pooling을 사용한다. Transition layer는 BN + conv1x1 + average pooling으로 구성되어있다. DenseBlock 내부에서는 pre-activation을 사용한다. 즉 layer 하나의 연산은 bn-relu-conv3x3-bn-relu-conv3x3가 된다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/3y5idt67bea7jid/Screenshot%202018-10-11%2017.00.25.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1608.06993.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
DenseNet에서는 한 가지 유의해야할 점이 있다. ResNet에서는 이전 입력과 합칠 때 addition을 사용하기 때문에 feature map의 channel 수는 변하지 않는다. 하지만 DenseNet에서는 이전 입력과 모두 &lt;strong&gt;concatenation&lt;/strong&gt; 을 하기 때문에 feature map의 channel 수가 점점 늘어난다. 만일 DenseBlock 안에서 l번째 layer라고 한다면 이 layer의 입력은 &lt;code class=&quot;MathJax_Preview&quot;&gt;k_0 + k \times (l-1)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k_0 + k \times (l-1)&lt;/script&gt;의 channel 수를 가진다. &lt;code class=&quot;MathJax_Preview&quot;&gt;k_0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k_0&lt;/script&gt;는 DenseBlock의 입력 channel을 의미하고 k는 각 convolution의 channel을 의미한다. Layer의 입력으로는 이전 입력이 모두 concatenated되서 들어오기 때문에 channel 수가 계속 달라지지만 layer 내부에서는 k의 channel로 유지되는 것이다. DenseNet의 특성 상 layer의 width는 상당히 작게 유지될 수 있다. 논문에서는 12, 24, 40 정도를 k 값으로 사용한다.&lt;/p&gt;

&lt;p&gt;CIFAR에서 DenseNet은 3개의 Dense block을 사용한다. 그 이외에는 기존 ResNet 구조와 거의 동일하다. DenseNet의 성능은 다음과 같다. 표에서 C10+는 augmentation 한 것을 의미한다. DenseNet (k=12)를 보면 네트워크의 깊이는 40이며 파라메터 수는 1.0M이다. Error rate는 5.24 %인데 동일한 성능의 WideResNet과 비교하면 2배정도 파라메터가 적다. DenseNet 저자는 이렇게 성능이 향상된 이유로 &lt;strong&gt;Deep supervision&lt;/strong&gt;의 가능성을 제시했다. 이전 layer의 출력이 뒤로 모두 전달이 되는데 결국 loss function이 모든 layer에 적용될 수 있다. 이는 각 layer마다 supervision의 영향력을 직접적으로 걸어주는 역할을 한다고 해석할 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/05baxltjfhiqqux/Screenshot%202018-10-11%2017.16.51.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1608.06993.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;pyramidnet&quot;&gt;PyramidNet&lt;/h2&gt;

&lt;p&gt;PyramidNet은 ResNet의 down sampling에서 일어나는 급격한 width의 변화에 초점을 맞췄다. 일반적인 ResNet 구조의 네트워크들은 feature map size를 반으로 줄이면서 feature map channels는 2배로 늘린다. PyramidNet은 모든 layer에서 channel 수가 변하도록 해서 특정 layer에 집중되어있던 width의 변화를 전체 네트워크로 분산시켰다. 이러한 생각을 하게 된 것은 ResDrop의 연구결과 때문이다.&lt;/p&gt;

&lt;p&gt;ResNet은 &lt;strong&gt;“Residual networks behave like ensembles of relatively shallow networks”&lt;/strong&gt;&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt; 논문에서 언급한 것처럼 일종의 얕은 네트워크들의 앙상블처럼 행동한다. 따라서 ResDrop에서 특정 layer들을 없애도 전체 성능에 영향이 별로 없던 것이다. 하지만 down sample이 일어나는 layer를 없앴을 경우 다른 layer에 비해 큰 폭으로 성능 저하가 일어났다. 다음 그림이 ResNet의 특정 layer를 없앨 경우 성능이 어떻게 변하는지를 보여준다. 파란색 수직선이 down sample이 일어나는 layer이다. 왼쪽이 Pre-activation ResNet인데 down sample이 일어날 때 2% 정도 성능이 저하되는 것을 볼 수 있다. 오른쪽이 PyramidNet에서 같은 실험을 한 것인데 모든 layer에서 성능 저하가 거의 동일한 것을 볼 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/mjlw97g7e1cte5i/Screenshot%202018-11-21%2023.33.55.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
      https://arxiv.org/pdf/1610.02915.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;PyramidNet에서 residual block은 다음 그림에서 (d)나 (e)와 같다. 기존에는 convolution filter의 수가 down sample이 아닌 곳에서는 모두 같았다면 PyramidNet에서는 모든 residual block에서 convolution filter 수가 증가한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/fm7yui43ojdt5rt/Screenshot%202018-10-24%2015.36.15.png?dl=1&quot; /&gt; 
  &lt;figcaption&gt;
      https://arxiv.org/pdf/1610.02915.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;PyramidNet에서 layer의 width를 늘리는 방법에는 두 가지가 있다. (1) additive 방식으로 전체 layer 동안 얼마나 width를 늘릴지 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;를 정한 다음에 이전 width에 비해서 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha / N&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha / N&lt;/script&gt;만큼 늘리는 것이다. (2) multiplicative 방식으로 지수배로 늘리는 방식이다. 다음 그림에서 가운데가 multiplicative 방식이며 오른쪽이 additive 방식이다. 성능은 additive 방식이 좋은 편인데 초기 layer 들의 width가 multiplicative 방식보다 더 큰 경향이 있기 때문이다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/ehjxn6g7lyjhxd9/Screenshot%202018-11-21%2023.45.57.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
      https://arxiv.org/pdf/1610.02915.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;PyramidNet에서는 residual block 내부 구조가 Pre-activation ResNet과 좀 다르다. Pre-activation ResNet의 block 내부 구조 자체도 개선의 여지가 있다고 생각했기 때문에 논문에서 여러 구조로 실험을 해봤다. Residual block 안에 ReLU가 너무 많으면 성능이 안좋아지는 경향이 있다. 따라서 residual block 안에서 첫번째 ReLU를 생략한다. 다음 그림의 (b)와 (d)에 해당에 해당한다. 하지만 첫번째 ReLU를 생략하면 두 개의 convolution 사이에 non-linearity가 없어서 representation power가 약해진다. 따라서 Batch Normalization layer를 하나 더 추가해주는 것이 좋다. 따라서 PyramidNet에서는 (d)를 Residual block의 구조로 사용한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/lv6lvozm1uzgm4h/Screenshot%202018-10-24%2021.15.39.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
      https://arxiv.org/pdf/1610.02915.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;모든 layer에서 width의 변화가 있기 때문에 shortcut connection도 단순히 identity mapping이 될 수 없다. Width가 변하는 경우는 1x1 convolution을 쓰거나 zero-padding을 써야한다. 모든 layer에 1x1 convolution을 shortcut connection으로 사용하면 parameter의 수 증가의 문제도 있고 적용했을 때 결과가 별로 좋지 않다. 따라서 PyramidNet에서는 zero-padding 방법을 사용한다. CIFAR 데이터셋에 적용했던 ResNet 또한 down sampling이 일어날 때 zero-padding을 사용했던 것을 생각하면 동일한 방법임을 알 수 있다. 다음 표는 여러가지 shortcut connection 방식을 비교한 것이다. Projection이 1x1 convolution을 의미하는데 모두 projection을 한 것보다 zero-padding만 한 것이 2% 이상 성능이 좋다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/j5evvkrjmonyfcl/Screenshot%202018-11-21%2023.58.57.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
      https://arxiv.org/pdf/1610.02915.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음은 PyramidNet의 학습 결과표이다. PyramidNet은 CIFAR-10에서 최대 3.31 %의 error rate를 달성한 것을 볼 수 있다. PyramidNet의 학습은 ResNet 학습과 거의 동일하다. Augmentation은 동일하고 SGD with momentum으로 네트워크를 업데이트했다. SGD의 처음 learning rate는 0.1로 시작해서 150 epoch에서 0.01로 225 epoch에서 0.001로 감소시켰다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/3y88bc0n16mmisf/Screenshot%202018-10-24%2021.24.21.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
      https://arxiv.org/pdf/1610.02915.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;간단히 PyramidNet의 모델 구조를 코드로 살펴보자. 코드는 https://github.com/dnddnjs/pytorch-cifar10/tree/pyramid/pyramidnet 에 있다. PyramidNet의 residual block은 다음과 같다. 위에서 살펴봤듯이 기존 pre-actiovation residual block과는 다르게 relu가 하나밖에 없으며 대신 batch normalization이 3개가 있다. Shortcut connection으로는 이전 post에서 소개했던 IdentityPadding을 사용한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ResidualBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResidualBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;      
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                                &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;    
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IdentityPadding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;        
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
       
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;IdentityPadding은 다음과 같다. 한가지 특이한 점은 IdentityPadding이 항상 적용되고 있기 때문에 down sampling이 일어날 때(stride가 2일 때) average pooling을 통해 shortcut을 지나는 feature map의 사이즈를 줄이는 것이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;IdentityPadding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IdentityPadding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AvgPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ceil_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;
            
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;PyramidNet 클래스의 &lt;strong&gt;init&lt;/strong&gt; 부분은 다음과 같다. 전체 네트워크의 깊이가 110이라면 num_layer는 18층이다. 각 block마다 width를 얼마나 늘려야하는지는 self.add_rate로 정의해놓았다. Additive 방식이며 self.get_layers 함수 내에서 사용된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# num_layers = (110 - 2)/6 = 18
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addrate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;alpha&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1.0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                       &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# feature map size = 32x32
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# feature map size = 16x16
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# feature map size = 8x8
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layer3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avgpool&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;AvgPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_classes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;get_layers는 다음과 같다. 계속 out_channels를 in_channels보다 addrate만큼 증가시킨다. 이 때, channel 수로는 정수만 사용해야하므로 round와 int를 사용한다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;addrate&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; 
                                 &lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt; 
                                 &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;PyramidNet을 학습한 그래프는 다음과 같다. 최고 error rate는 4.81%를 기록했다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/lwdujgn4uuunuwj/Screenshot%202018-11-23%2000.12.55.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;참고문헌&quot;&gt;참고문헌&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:0&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1605.07146.pdf &lt;a href=&quot;#fnref:0&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1608.06993.pdf &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1610.02915.pdf &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;https://arxiv.org/abs/1603.05027 &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1207.0580.pdf &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1603.09382.pdf &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1605.06431.pdf &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Wed, 24 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/cifar10/2018/10/24/pyramidnet/</link>
        <guid isPermaLink="true">http://localhost:4003/cifar10/2018/10/24/pyramidnet/</guid>
        
        <category>dl</category>
        
        
        <category>cifar10</category>
        
      </item>
    
      <item>
        <title>왜 일하는가</title>
        <description>&lt;ul&gt;
  &lt;li&gt;저자 : 이나모리 가즈오&lt;/li&gt;
  &lt;li&gt;경력 : 교세라 창업자, KDDI 창업, 일본항공 회장&lt;/li&gt;
  &lt;li&gt;내용 : 왜 일하는지, 일을 통해 무엇을 깨달을 수 있는지, 열심히 일해서 앞으로 어떻게 될지 알려주기 위한 책&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/jatdh3osu95zdw2/Screenshot%202018-10-19%2017.20.35.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;행복한-나를-꿈꾼다면&quot;&gt;행복한 나를 꿈꾼다면&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;젊을 때 수없이 좌절함
    &lt;ul&gt;
      &lt;li&gt;중학교 시험 떨어짐&lt;/li&gt;
      &lt;li&gt;대학 진학이 마음대로 안됌&lt;/li&gt;
      &lt;li&gt;대기업은 다 떨어짐&lt;/li&gt;
      &lt;li&gt;추천으로 들어간 곳은 하루하루 유지하기도 벅참&lt;/li&gt;
      &lt;li&gt;자신을 비하함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;생각을 바꿈
    &lt;ul&gt;
      &lt;li&gt;지금 하고 있는 일을 즐기자&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;지금 하고 있는 일에 더 적극적으로 가능한 무아지경에 이를 때까지 부딪쳐보라&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;1-왜-일하는갸&quot;&gt;1. 왜 일하는갸?&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;도대체 무엇을 위해 일하는가
    &lt;ul&gt;
      &lt;li&gt;지금 당신이 일하는 것은 스스로를 단련하고, 마음을 갈고닦으며, 삶의 가치를 발견하기 위한 가장 중요한 행위&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;평생동안 궁궐을 지어온 도편수의 말
    &lt;ul&gt;
      &lt;li&gt;아무리 볼품없는 나무라도 그 안에는 영혼이 살고 있습니다. 모든 나무에는 저마다 영혼이 살고 있습니다. 그 영혼들이 제게 말을 걸어옵니다. 그 영혼의 소리에 귀 기울이지 않고는 그 나무를 자르거나 다듬을 수가 없습니다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;천년을 버텨온 고목처럼 무수한 고난을 이겨내며 자기 일에 최선을 다하는 사람, 풍성한 삶을 일구고 훌륭한 인격을 키워낸 사람. 그의 말 한마디 한마디는 그의 인생과 마음 가짐을 그대로 보여주었다.&lt;/li&gt;
  &lt;li&gt;서양과 달리 일본은 일에서 그 이상의 기쁨과 보람, 긍지를 느낀다. 나아가 일은 살아가는 존엄한 행위라고 생각해왔다. 서양과 달리 일본 사람들은 직업에 관계없이 모두가 아침부터 밤까지 불평 없이 쉬지 않고 일했다. 일회용품을 만드는 별 볼일없는 기술자라도 자신의 기술을 갈고닦아 훌륭한 일회용품을 만드는데 자부심과 가치를 느꼈다. 그리고 일이란 기술을 갈고닦아 연마할 뿐 아니라 마음을 수행하는 과정이며, 자아를 실현하고 인격을 높이는 수단이라고 여겼다.&lt;/li&gt;
  &lt;li&gt;변명과 불평불만을 입에 달고 지내던 나를 버리고, 아무 생각도 하지 않은 채 눈앞에 놓인 일에만 집중하기로 결심했다. 그러자 전에는 느끼지 못했던 도전의식이 우러났고, 치열하게 싸워보고 싶은 욕구가 솟아올랐다.&lt;/li&gt;
  &lt;li&gt;지금은 물질적으로 풍요로운 시대지만, 정신적인 나태함이라는 부작용을 낳고 말았다. 물질적 풍요에만 젖어 산다는 것이 인생에 어떤 결과를 초래할지 심각하게 생각해보기 바란다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-일을-사랑하는가&quot;&gt;2. 일을 사랑하는가&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;어떻게 오랫동안 꾸준히 일할 수 있었나? 그것은 내가 하는 일이 좋아지도록 스스로를 다스렸기 때문&lt;/li&gt;
  &lt;li&gt;지금 하는 일 외에 방법이 없다면, 지금 하는 일에만 집중하자&lt;/li&gt;
  &lt;li&gt;마침내 어느 순간부터는 그 일을 내가 하고 있다는 자부심으로 마음이 벅찼다&lt;/li&gt;
  &lt;li&gt;일을 사랑하는 것이야말로 내가 평생을 이어올 수 있었던 원동력이자 힘들고 어려운 고비를 헤쳐 나갈 수 있는 힘이었다.&lt;/li&gt;
  &lt;li&gt;자기에게 주어진 일을 좋아하는 것부터 시작하라. 자기가 좋아하는 일을 추구하는 것은 유토피아를 찾는 것과 같다.&lt;/li&gt;
  &lt;li&gt;작은 일을 기뻐할 줄 알고, 감동할 수 있는 것이야말로 세상에서 가장 멋진 일이야.&lt;/li&gt;
  &lt;li&gt;자신이 만든 제품을 안고 잘 만큼 자신이 만든 제품에 애정을 갖고 자신의 일에 집중하지 않는 한 일의 참맛을 알지 못한다.&lt;/li&gt;
  &lt;li&gt;어떤 일이든 그 일을 이루어내려면 스스로 활활 타올라야 한다. 스스로 타오르기 위해서는 왜 그 일을 해야 하는지 이유가 분명하고, 자신이 하는 일을 더없이 좋아해야 하며, 그 일로 이루고자 하는 목표가 확고해야 한다.&lt;/li&gt;
  &lt;li&gt;주변만 빙빙 도는 사람은 절대로 일의 진정한 기쁨을 느낄 수 없다. 자신이 일의 중심에 서서 적극적으로 리드하고 주위 사람들을 감싸 안을 때 비로소 자신이 해낸 일의 참맛을 맛볼 수 있으며, 그 일에 더욱 전력 질주할 수 있다.&lt;/li&gt;
  &lt;li&gt;회사 물정에 어둡더라도 회사를 위한 일이라면 서슴없이 의견을 제안하는 사람은, 그가 아무리 어리고 경험이 부족하더라도 그 조직의 중심에 있는 사람이며, 그 조직의 리더가 될 자격이 충분하다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-어디로-가는가&quot;&gt;3. 어디로 가는가&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;꿈만으로는 결코 그곳에 이르지 못한다. 꿈이 아무리 근사해도 꿈으로만 그쳐서는 안 된다. 꿈이 아니라 내일 반드시 이루어야 할 목표가 되어야 한다.&lt;/li&gt;
  &lt;li&gt;높은 목표를 달성하려면 간절한 바람이 잠재의식에까지 미칠 정도로 곧고 강해야 한다. 주위의 시선에 우왕자왕하지 말아야 한다. 하고 싶다면, 하고자 한다면 무슨 일이 있어도 그 길을 가겠다고 굳게 다짐하라. 그리고 반드시 이룰 수 있다고 굳게 믿어라. 그런 간절함이 없다면 처음부터 꿈도 꾸지 마라.&lt;/li&gt;
  &lt;li&gt;왜 그들은 단 몇 걸음에 에베레스트 산을 오를 수 있다고 자신만만한 걸까? 왜 몇 걸음 가지도 않고 정상이 너무 멀다고 지레 포기하는 걸까? 그리고 왜 자신이 오르지 못한 것을 책망하기는커녕 다른 사람이 밤낮없이 노력해 그 자리에 오른 것을 비난하는 걸까?&lt;/li&gt;
  &lt;li&gt;“부품이 만들어지는 순간순간에 ‘잘 구워지게 해주세요’라고 절실하게 기도드렸나?”&lt;/li&gt;
  &lt;li&gt;할 수 있는 노력을 최대한 다하고 나서 그 다음은 하늘의 응답을 기다릴 뿐이라고 말할 수 있을 정도로, 정말 온 힘을 다해 노력했는가? 몸이 부서질 정도로 제품에 마음이 스며들게 했는가? 누구에게도 뒤지지 않을 노력을 그 일에 쏟아부었는가?&lt;/li&gt;
  &lt;li&gt;무엇을 이루겠다고 다짐했다면, 그것을 이루기 위해서는 평소에 하던 노력의 몇 배를 더 쏟아부어야 한다. 나 혼자라면 모를까, 경쟁선상에 잇는 누구나 그렇게 다짐하고 노력하기 때문에 같은 노력으로는 그 목표에 이를 수 없다.&lt;/li&gt;
  &lt;li&gt;사람들은 위대한 업적을 남긴 사람은 평범한 사람들과는 태어날 때부터 다르고, 그들의 노력도 우리가 따라 하기 힘들다고 말한다. 누구에게도 뒤지지 않게 노력한다는 것은 특별한 능력을 지닌 사람들의 이야기로만 넘겨버리기 쉽다. 그러나 절대로 그렇지 않다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;4-무엇을-꿈꾸는가&quot;&gt;4. 무엇을 꿈꾸는가&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;놀랄만한 큰 성과, 특별한 천재가 이루었으리라 짐작하는 위대한 업적도 알고 보면 평범한 사람이 한 발 한 발 내디딘 결과일 뿐이다.&lt;/li&gt;
  &lt;li&gt;다른 사람들보다 몇 배나 많은 고생을 하면서도 결코 놓지 않았던 일에 대한 열정, 끈기, 성실성이 그들을 비범하게 변화시킨 것이다.&lt;/li&gt;
  &lt;li&gt;그들의 그런 끈기와 성실성이 그들의 내면에 숨어 있는 위대한 능력뿐 아니라 인격까지 고양시켜 나중에는 그 누구도 범접할 수 없는 사람으로 키운 것이다.&lt;/li&gt;
  &lt;li&gt;아무것도 보지 말자. 막연한 미래는 막연한 미래에게 맡겨두자. 내가 해야 할 일은 오늘 내게 주어진 일이다. 오늘 목표는 오늘 무조건 끝내자. 이렇게 스스로를 담금질한 후 일의 성과와 진척을 하루 단위로 잘라 그날 해야 할 일은 반드시 그날 확실하게 완수해갔다.&lt;/li&gt;
  &lt;li&gt;‘나는 공부하지 않았기 때문에 지식도 기술도 가지고 있지 않다. 그래서 할 수 없다’라고 스스로를 내몰지 마라. 오히려 ‘나는 공부를 하지 않았기 때문에 지식도 기술도 없다. 하지만 하고자 하는 의욕이 있기 때문에 반드시 내년에는 할 수 있을 것이다’ 라고 생각하라.&lt;/li&gt;
  &lt;li&gt;일단 착수한 연구 개발은 100퍼센트 성공시킨다&lt;/li&gt;
  &lt;li&gt;교세라에서는 개발이 성공할 때까지 연구를 계속하기 때문에 실패로 끝나는 일이 없습니다.&lt;/li&gt;
  &lt;li&gt;실패와 고난에 맞부딪혔을 때, 불평불만을 앞세우고 잘되는 사람을 시기하면서 세상을 비난하는 것처럼 초라한 일도 없다.&lt;/li&gt;
  &lt;li&gt;현실에 안주하고 쉬운 길로 돌아가면, 그 당시는 편할지 몰라도 꿈과 목표에는 절대 이를 수 없다. 나중에 그 한순간의 타협 때문에 반드시 후회할 일ㄹ이 생긴다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;5-일에-만족하는가&quot;&gt;5. 일에 만족하는가&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;살다 보면 그처럼 ‘다시 고쳐 쓰면 되지’라고 생각하는 사람들이 적지 않다. 그러나 특히 일에서는 지우개로 지울 수 없을 때가 많다.&lt;/li&gt;
  &lt;li&gt;‘신은 세심한 부분에 머문다’는 독일 격언처럼 일의 본질은 세심한 부분에 있다. 좋은 결과는 세삼함을 잊지 않는 자세로부터 시작하는 것이다.&lt;/li&gt;
  &lt;li&gt;베스트는 상대적이지만, 퍼펙트는 절대적입니다. 다른 기업들이 최선을 다했다고 자랑할 때 교세라는 완벽함을 자부합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;6-창조적인가&quot;&gt;6. 창조적인가&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;다른 사람과 같은 길을 걷는다면 늘 같은 것만 볼 것이다. 그것은 내가 가야할 길이 아니다. 모든 사람들이 걸었던, 아무것도 남아 있지 않은 길은 편할지는 모르지만, 아무 생각없이 걸어도 그만인 길이다. 그런 길은 내가 가야할 길이 아니다. 내가 가야할 길은 낯설고 두렵지만 새로운 세상과 만나는 꿈으로 가슴이 벅차오르는 길이다.&lt;/li&gt;
  &lt;li&gt;문외한은 기존의 관습, 습성에 따르지 않고, 초보이기 때문에, 아무것도 모르기 때문에 자유롭게 생각한다. 현실에 얽매이기보다는 자유롭게 생각하고, 그래서 더욱 새로운 아이디어를 내는 것, 그것이 교토의 전문기업들이 성공한 비결이기도 하다.&lt;/li&gt;
  &lt;li&gt;정말 세상과 사람을 위해서라는, 사심없는 마음에서인가?&lt;/li&gt;
  &lt;li&gt;제임스 알렌
    &lt;ul&gt;
      &lt;li&gt;상처받은 인간이 패배를 겁내고 앞으로 나아가려 하지 않는 상황에서도, 순수한 사람은 편하게 발걸음을 내딛어 너무나 쉽게 승리를 손에 쥐는 일이 적지 않다. 왜 그럴까? 순수한 사람은 항상 자신의 에너지를 더 온화한 마음으로, 더 명확하게, 더 강력한 목적의식에 의해 사용하며 자신의 길을 밟아나가기 때문이다. … 순수하고 강렬한 생각이 있다면, 반드시 성공할 수 있다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;사고방식&quot;&gt;사고방식&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;무슨 일이든 이룰 수 있다고 다짐하라&lt;/li&gt;
  &lt;li&gt;모두와 함께 일하고 기쁨을 나누어리&lt;/li&gt;
  &lt;li&gt;항상 긍정적으로 생각하고 행동하라&lt;/li&gt;
  &lt;li&gt;다른 사람들에게 선의를 베풀어라&lt;/li&gt;
  &lt;li&gt;자신보다는 남을 먼저 배려하라&lt;/li&gt;
  &lt;li&gt;정직하고, 겸손하며, 노력을 아끼지 마라&lt;/li&gt;
  &lt;li&gt;남의 것을 탐하지 말고, 욕심을 멀리하라&lt;/li&gt;
  &lt;li&gt;모은 일이 뜻대로 된다고 믿어라&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 19 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/reading/2018/10/19/why_work/</link>
        <guid isPermaLink="true">http://localhost:4003/reading/2018/10/19/why_work/</guid>
        
        <category>book</category>
        
        
        <category>reading</category>
        
      </item>
    
      <item>
        <title>Arxiv New 2018.10.14</title>
        <description>&lt;h2 id=&quot;arxiv-new-20181014&quot;&gt;Arxiv New 2018.10.14&lt;/h2&gt;

&lt;h3 id=&quot;1-generating-shared-latent-variables-for-robots&quot;&gt;1. Generating Shared Latent Variables for Robots&lt;/h3&gt;
&lt;p&gt;to Imitate Human Movements and Understand
their Physical Limitations&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;rehabilitation : 재활&lt;/li&gt;
  &lt;li&gt;재활 분야에서는 assistive robotics가 중요. 특히 robot coach&lt;/li&gt;
  &lt;li&gt;이 논문에서는 Gaussian Process Latent Variable Model(GP-LVM)이라는 모델을 제안함&lt;/li&gt;
  &lt;li&gt;이 모델은 물리 요법가(?), 로봇, 환자 사이에서 지식을 transfer 하는 것이 목표.&lt;/li&gt;
  &lt;li&gt;우리의 모델은 사람의 시각적인 신체 특징을 로봇이 다룰 수 있는 데이터로 변환함&lt;/li&gt;
  &lt;li&gt;따라서 로봇이 모방학습을 하기에 용이해짐&lt;/li&gt;
  &lt;li&gt;중요한 것은 우리의 모델을 확장하면 환자가 물리적으로 할 수 없는 행동을 로봇이 이해할 수 있다는 것이다.&lt;/li&gt;
  &lt;li&gt;robot imitation과 환자의 한계에 따라 로봇이 적응하는 실험을 통해 검증함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/xy967ah0tk6fm8s/Screenshot%202018-10-14%2021.52.49.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;2-singlegan-image-to-image-translation-by-a&quot;&gt;2. SingleGAN: Image-to-Image Translation by a&lt;/h3&gt;
&lt;p&gt;Single-Generator Network using Multiple
Generative Adversarial Learning&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Image translation 논문임&lt;/li&gt;
  &lt;li&gt;최근 연구들에서 여러 다른 domain을 mapping하기 위해서는 여러 generator가 필요함. 이건 비효율적이고 멀티 도메인 image translation에서는 효과적이지 않음.&lt;/li&gt;
  &lt;li&gt;이 논문에서는 멀티 도메인 image translation 문제를 single generator로 해결함. 그래서 이름이 SingleGAN&lt;/li&gt;
  &lt;li&gt;그게 가능하게 하기 위해 domain code라는 것을 도입. 여러 다른 생성하는 작업을 명시적으로 컨트롤하기 위해서 사용. 그리고 optimization할 때 여러 task를 하나로 묶는데 사용함&lt;/li&gt;
  &lt;li&gt;unpaired dataset에서 실험결과로 superior performance를 냄&lt;/li&gt;
  &lt;li&gt;one-to-many, one-to-one, many-to-many에 다 테스트해봄.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/7ro759iv4ifayok/Screenshot%202018-10-14%2022.03.12.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/7dawgjktxuhjbk4/Screenshot%202018-10-14%2022.03.56.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;3-generative-replay-with-feedback-connections-as-a&quot;&gt;3. Generative replay with feedback connections as a&lt;/h3&gt;
&lt;p&gt;general strategy for continual learning&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;catastrophic forgetting 관련 논문 : continual learning 분야&lt;/li&gt;
  &lt;li&gt;최근 논문들의 성능을 비교하기 어려운 문제가 있음&lt;/li&gt;
  &lt;li&gt;이 논문들을 더 잘 비교하기 위해서 3개의 continual learning 시나리오를 생각해봄&lt;/li&gt;
  &lt;li&gt;task identity is known / 모른다면 필요한지 / 추론해야하는지 세 가지로&lt;/li&gt;
  &lt;li&gt;이 세가지 시나리오에서 generative replay combined with distillation이 좋은 성능을 냄.&lt;/li&gt;
  &lt;li&gt;우리는 generative replay의 computational cost를 줄임.&lt;/li&gt;
  &lt;li&gt;generative feedback connection을 이용해서 generative model을 main model에 통합함&lt;/li&gt;
  &lt;li&gt;성능은 변화없이 학습 속도를 많이 줄여줌.
&lt;img src=&quot;https://www.dropbox.com/s/w4yzccetemnysea/Screenshot%202018-10-14%2022.15.01.png?dl=1&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/0rm99qaxqrhp6cq/Screenshot%202018-10-14%2022.16.16.png?dl=1&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Sun, 14 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/10/14/arxiv_new/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/10/14/arxiv_new/</guid>
        
        <category>dl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>파워풀</title>
        <description>&lt;ul&gt;
  &lt;li&gt;저자 : 패티 맥코드&lt;/li&gt;
  &lt;li&gt;경력 : 퓨어아트리아소프트웨어, 선마이크로시스템스, 볼랜드, 시게이트테크놀로지, 넷플릭스 최고인재책임자, 패티맥코드컨설팅&lt;/li&gt;
  &lt;li&gt;내용 : 넷플릭스의 조직문화와 인재를 대하는 방식&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/86zxjnfrgsckmng/Screenshot%202018-10-13%2012.19.40.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;개인적인-생각&quot;&gt;개인적인 생각&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;“구글의 아침은 자유가 시작된다”와 “파워풀”을 읽고 나서 느낀 것이다. 정보가 제한적이므로 편향된 생각일 수 있다.&lt;/li&gt;
  &lt;li&gt;구글은 장난을 좋아하며 거대하게 수많은 실험을 하는 것 같다.&lt;/li&gt;
  &lt;li&gt;넷플릭스의 경우 좀 더 어른스러운 면이 느껴지며 상대적으로 적은 실험으로 차근 차근 개선시킨 것 같은 느낌이 든다.&lt;/li&gt;
  &lt;li&gt;하지만 두 개의 기업 모두 다 엄청나게 큰 기업이라는 것을 생각해보면 넷플릭스에서 말하는 “자유와 책임의 문화”가 정착되어 있는게 너무 신기하다.&lt;/li&gt;
  &lt;li&gt;문화는 아래에서 위로 향하는 것이 아니라 위에서 아래로 향하는 것이다. 창업자의 확고한 신념이 중요하며 그에 맞는 사람들이 함께 경영하는 것이 중요해보인다.&lt;/li&gt;
  &lt;li&gt;인재를 대하는 방식이 상당히 어른스럽다. 만남, 관계, 이별의 방식 모두 다 성숙하다.&lt;/li&gt;
  &lt;li&gt;책에 나오는 여러가지 질문이 스스로를 많이 돌아보게 한다. 나는 어느 부분에서 성과를 낼만한 인재인지, 어떤 기업에서 어떻게 성장할 수 있을지, 그 성장하는 부분이 해당 기업과 맞지 않을 때 어떻게 떠날지 등등 많은 생각을 한다.&lt;/li&gt;
  &lt;li&gt;직원을 어른으로 대하는 것. 우리나라에서는 흔히 “어른답다”라는 것은 “부지런하고 많은 양을 일하되 군말없이 따르는 것”이라는 뜻인 것 같다. 하지만 어른으로 대한 다는 것은 권한을 넘겨준다는 것, 스스로 결정하게 한다는 것이다. 그러기 위해서는 애초에 그렇게 할 수 있는 사람을 뽑아야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;프롤로그&quot;&gt;프롤로그&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;어떻게 하면 넷플릭스와 같은 민첩하고 높은 성과를 내는 조직문화를 만들 수 있을까?&lt;/li&gt;
  &lt;li&gt;경영에서의 전통적인 지혜를 뒤집어라
    &lt;ul&gt;
      &lt;li&gt;극도로 솔직해져라. 아이디어와 문제를 소리 내어 말하고 서로 간에 또는 경영진 앞에서 자유롭게 저항할 수 있는 문화.&lt;/li&gt;
      &lt;li&gt;넷플릭스의 문화는 계속해서 정책을 줄이고 절차를 제거해나가며 만들어졌다.&lt;/li&gt;
      &lt;li&gt;직원이 일에 몰두하게 하려면 인센티브를 주어야 하며, 각자가 해야 할 일을 명확하게 알려줘야 한다는 잘못된 생각을 했다.&lt;/li&gt;
      &lt;li&gt;직원들 자신이 힘을 가지고 출근한다는 사실을 상기시키고 그들이 실제 힘을 행사할 수 있는 상황 조건을 만들어주어라&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;비즈니스 리더의 임무: 제시간에 놀라운 일을 하는 훌륭한 팀을 만드는 것&lt;/li&gt;
  &lt;li&gt;우리가 관리자에게 요구한 점
    &lt;ul&gt;
      &lt;li&gt;해야할 일과 직면한 도전에 대해 개방적이고 명확하고 지속적으로 소통&lt;/li&gt;
      &lt;li&gt;모든 직원은 극도의 솔직함을 실천&lt;/li&gt;
      &lt;li&gt;모든 직원은 사실에 근거한 의견을 바탕으로 대담하게 토론. 결과를 엄격하게 실험&lt;/li&gt;
      &lt;li&gt;고객과 회사를 위한 최선이 무엇일까가 기준&lt;/li&gt;
      &lt;li&gt;모든 지위에 적합한 기술을 가진 고성과자를 채용해서 미래를 대비&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;어른으로-대접해라&quot;&gt;어른으로 대접해라&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;훌륭한 팀은 모든 팀원이 자신이 어디로 가고 있는지를 알고, 그곳에 가기 위해서 뭐든지 할 때 만들어진다.&lt;/li&gt;
  &lt;li&gt;나는 직원을 채용할 때 문제 해결에 매우 흥미를 느끼는 사람들을 찾는다. 돌파해야 할 굉장한 문제가 주어져 있고, 그것을 함께 풀어나갈 적합한 동료가 있다는 것이 무엇보다 강력한 인센티브다.&lt;/li&gt;
  &lt;li&gt;대부분의 사람이 일에서 원하는 것: 출근을 해서, 자신이 믿고 존경하는 동료들로 이뤄진 제대로 된 팀과 함께, 미친 듯이 집중해 멋진 일을 해내는 것&lt;/li&gt;
  &lt;li&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 13 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/reading/2018/10/13/read_netflix/</link>
        <guid isPermaLink="true">http://localhost:4003/reading/2018/10/13/read_netflix/</guid>
        
        <category>paper</category>
        
        
        <category>reading</category>
        
      </item>
    
      <item>
        <title>CIFAR-10 정복 시리즈 1: ResNet</title>
        <description>&lt;h2 id=&quot;cifar-10-정복하기-시리즈-소개&quot;&gt;CIFAR-10 정복하기 시리즈 소개&lt;/h2&gt;
&lt;p&gt;CIFAR-10 정복하기 시리즈에서는 딥러닝이 CIFAR-10 데이터셋에서 어떻게 성능을 높여왔는지 그 흐름을 알아본다. 또한 코드를 통해서 동작원리를 자세하게 깨닫고 실습해볼 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/07/start_cifar10/&quot;&gt;CIFAR-10 정복 시리즈 0: 시작하기&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/09/resnet/&quot;&gt;CIFAR-10 정복 시리즈 1: ResNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/&quot;&gt;CIFAR-10 정복 시리즈 2: PyramidNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/25/shake_shake/&quot;&gt;CIFAR-10 정복 시리즈 3: Shake-Shake&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;관련 코드 링크
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/dnddnjs/pytorch-cifar10&quot;&gt;pytorch cifar10 github code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cifar-10-정복-시리즈-1-resnet&quot;&gt;CIFAR-10 정복 시리즈 1: ResNet&lt;/h2&gt;
&lt;p&gt;ResNet은 진정한 Deep Learning의 시대를 가져왔다고 말할 정도로 영향력이 큰 네트워크이다. 이번 포스트에서는 ResNet 이전에 Deep Neural Network의 흐름과 유사한 선행 연구에 대해서 알아볼 것이다. 그 이후에 ResNet이 해결하려는 문제와 어떻게 문제를 해결했는지를 살펴본다. ResNet 저자의 후속논문을 살펴보면서 ResNet을 어떻게 더 깊게 쌓을 수 있는지 알아본다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;#toward-deeper-network&quot;&gt;Toward Deeper Network&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#from-10-to-100-layers&quot;&gt;From 10 to 100 layers&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#from-100-to-1000-layers&quot;&gt;From 100 to 1000 layers&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;toward-deeper-network&quot;&gt;Toward Deeper Network&lt;/h2&gt;
&lt;p&gt;Deep Learning은 Deep Neural Network를 사용한 Machine Learning이라고 풀어서 말할 수 있다. MNIST와 같은 데이터셋에서는 3개의 층을 가지는 CNN으로도 높은 classification 성능을 얻을 수 있다. 하지만 CIFAR이나 ImageNet과 같이 좀 더 도전적인 데이터셋에서는 &lt;strong&gt;얕은&lt;/strong&gt; 네트워크로는 한계가 있다. 따라서 연구자들은 과거부터 Layer를 더 깊게 쌓으려고 했다. 2012년 AlexNet은 8층이었으며 2014년의 VGG는 19층 GoogleNet은 22층이었다. 2014년까지는 가장 깊은 Neural Network가 몇십층 정도의 깊이를 가진 것이다. 하지만 2015년에 나온 ResNet은 152 층을 쌓았으며 2015년 ILSVRC 대회에서 우승하였다. 한마디로 진정한 &lt;strong&gt;Deep Learning&lt;/strong&gt;의 시대를 연것이다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img data-action=&quot;zoom&quot; src=&quot;https://www.dropbox.com/s/qqswcef7uu5u9pv/Screenshot%202018-11-14%2016.19.31.png?dl=1&quot; width=&quot;500px&quot; /&gt; 
  &lt;figcaption&gt; 
  https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ResNet 논문&lt;sup id=&quot;fnref:0&quot;&gt;&lt;a href=&quot;#fn:0&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;에서는 152보다 더 깊은 1000 층 이상의 ResNet도 실험했다. 하지만 논문의 실험 결과에 의하면 110층의 ResNet보다 1202층의 ResNet이 CIFAR-10에서 성능이 낮다. 이런 문제를 지적하며 ResNet 저자인 Kaiming He는 2016년에 ResNet의 후속 논문을 발표했다. “Identity Mappings in Deep Residual Networks” 에서는 ResNet 내부 구조의 변경을 통해 110층, 164층의 ResNet보다 1001층의 ResNet의 성능을 높게 만들 수 있었다. 다음 표를 보면 CIFAR-10 데이터셋에서 ResNet-1001이 (1001층의 깊이를 가지는 ResNet을 의미한다) 기존의 다른 네트워크보다 성능이 좋은 것을 볼 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/ad7l497i9vapl6e/Screenshot%202018-11-14%2017.25.54.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;위 결과는 ResNet이 Image Classification task에서 거둔 성과를 보여준다. 하지만 ResNet 또는 &lt;strong&gt;더 깊은 Network는 단지 image classification에서만 사용되는 것이 아니다&lt;/strong&gt;. Detection, Segmentation, Pose Estimation, Depth Estimation 등에서 일명 &lt;strong&gt;Backbone&lt;/strong&gt;으로 사용된다. Backbone은 등뼈라는 뜻이다. 등뼈는 뇌와 몸의 각 부위의 신경을 이어주는 역할을 한다. 뇌를 통해 입력이 들어온다고 생각하고 팔, 다리 등이 출력이라고 생각한다면 backbone은 입력이 처음 들어와서 출력에 관련된 모듈에 처리된 입력을 보내주는 역할이라고 생각할 수 있다. 여러가지 task가 몸의 각 부분이라고 생각하면 ResNet과 같은 classification model은 입력을 받아서 각 task에 맞는 모듈로 전달해주는 역할이다. 결국 객체를 검출하든 영역들을 나누든 Neural Network는 입력 이미지로부터 다양한 feature를 추출해야한다. 그 역할을 backbone 네트워크가 하는 것이다. 따라서 기본적으로 image classification 모델에 대한 이해가 필요하다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/dfz5m2va31zdklv/Screenshot%202018-11-14%2017.37.06.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
&lt;strong&gt;Deep Learning이 학습을 잘하게 되는 것은 단순히 층을 쌓는다고 되는 것이 아니다&lt;/strong&gt;. 10층을 쌓을 때까지 그리고 10층에서 100층, 100층에서 1000층은 각 단계마다의 문제가 존재한다. ResNet은 10층에서 100층 그리고 100층에서 1000층 사이에 해당한다. 10층 정도까지는 AlexNet, VGG 정도로 볼 수 있다. 이 네트워크에서는 10층을 쌓기 위해 어떤 노력을 했을까? 꽤나 최근까지 Deep Learning이 나올 수 없었던 이유는 Neural Network를 학습시키는 것이 어렵다는 사실이 큰 비중을 차지한다. Neural Network는 여러 weight와 bias라는 parameter를 가진다. Neural network를 학습시킨다는 것인 이 parameter를 stochastic gradient descent로 업데이트 한다는 것을 의미한다.&lt;/p&gt;

&lt;p&gt;Gradient를 통해 weight를 업데이트 할 때 &lt;strong&gt;gradient가 explode하거나 vanishing&lt;/strong&gt; 하는 경우가 많았다. Gradient가 안정적이지 않은 이유는 neural network에서 사용하는 activation function과 연관성이 있다. 다음 그림은 Neural network에서 많이 사용하는 tanh로 10개의 층을 가지는 간단한 뉴럴넷을 만들고 테스트한 과정이다. 작은 random number로 network의 weight를 initialize하면 모든 activation이 0이 되는 현상이 발생한다. 자세한 내용은 CS231n 강의를 참고하길 바란다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/wiqnya6j40iwq27/Screenshot%202018-11-15%2012.47.31.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture6.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;모든 activation이 0이 되거나 -1 아니면 1로 saturate되는 문제를 해결하기 위해 &lt;strong&gt;Xavier initialization&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;과 &lt;strong&gt;He initialization&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;이 나왔다. He initilization은 ReLU를 activation function으로 사용하는 경우에 더 깊은 neural network가 학습가능하게 만든 방법이다. 즉, 깊은 neural network를 학습시키려면 신경써서 initialization을 해야했다. 하지만 &lt;strong&gt;Batch-Normalization&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt;이 나오면서 이런 흐름을 바꾸었다. Batch normalization의 저자인 Sergey Ioffe와 Christian Szegedy는 Neural network가 학습하기 어려운 이유를 internal covariate shift라고 주장한다. Internal covariate shift는 neural network가 학습하면서 각 층의 입력 분포가 계속 변하는 현상이다. 따라서 근본적으로 neural network를 학습하기 어렵다고 판단했다.&lt;/p&gt;

&lt;p&gt;Batch normalization 이름처럼 이 문제를 mini-batch마다 각 층의 input을 normalization하는 방법으로 어느정도 해결했다. Batch normalization을 사용하면 initialization을 크게 신경쓰지 않아도 된다. 또한 optimizer의 learning rate를 이전보다 더 높일 수 있다. 결과적으로 더 빠른 학습을 가능하게 한 것이다. Batch normalization 논문에서 저자는 GoogleNet&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;에 batch normalization을 적용해서 성능을 평가했다. 다음 그림을 보면 BN이라고 써져있는 네트워크(BN + GoogleNet)이 Inception(GoogleNet) 보다 훨씬 더 빠르게 학습하는 것을 볼 수 있다. 심지어 batch normalization은 regularization 역할도 하기 때문에 Dropout을 사용하지 않아도 학습이 잘 되는 특성이 있다. 이 때부터 많은 neural network에서 dropout을 사용하지 않기 시작했다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/0vlv5prg0g1b95l/Screenshot%202018-11-15%2013.15.07.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    http://proceedings.mlr.press/v37/ioffe15.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;from-10-to-100-layers&quot;&gt;From 10 to 100 Layers&lt;/h2&gt;

&lt;h3 id=&quot;degradation&quot;&gt;Degradation&lt;/h3&gt;
&lt;p&gt;Initialization과 normalization은 graident가 vanishing하거나 exploding하는 문제를 잡아줘서 더 깊은 network의 학습이 가능하게 했다. 그보다 더 깊은 network를 학습할 때는 어떤 문제가 발생할까? VGG나 GoogleNet과 같이 잘 짜여진 neural network에 층을 더 쌓을 경우 &lt;strong&gt;Degradation&lt;/strong&gt; 문제가 발생한다. Degradation은 neural network의 깊이는 증가하는데 training error가 증가하는 경우를 말한다. Degradation은 overfitting으로 인해 생기는 현상이 아니다. 다음 그림에서 보면 56 층의 network가 20층의 network보다 training error와 test error가 둘 다 높은 것을 볼 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/06xom9u3umodycz/Screenshot%202018-10-09%2021.16.01.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;https://arxiv.org/pdf/1512.03385.pdf&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Degradation 문제는 &lt;strong&gt;Convolutional Neural Networks at Constrained Time Cost&lt;sup id=&quot;fnref:5&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt; 논문과 &lt;strong&gt;Highway Networks&lt;sup id=&quot;fnref:6&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;&lt;/strong&gt; 논문에서 소개되었다. “Convolutional Neural Networks at Constrained Time Cost”는 ResNet의 저자인 Kaiming He의 논문이다. 이 논문에서 학습이나 테스트 시간이라는 제약 조건 아래 여러 neural network 구조를 비교한다. Inference time을 유지하면서 네트워크의 깊이를 늘리려면 filter의 개수나 filter의 사이즈를 줄여야한다. 제약조건 아래에서 실험을 하면 성능에 어떤 요인이 영향을 주는지 확인하기 좋다. 이 논문에서 발견한 사실은 이러한 제약조건 아래에서 &lt;strong&gt;네트워크의 성능에 영향을 주는 것은 filter의 개수나 filter의 사이즈보다 네트워크의 깊이라는 것이다&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;이렇게 네트워크 깊이가 중요한 만큼 네트워크의 층을 더 늘리려는 노력이 필요하다. 하지만 일정이상 네트워크의 깊이를 늘리면 네트워크의 정확도가 변화가 없거나 오히려 떨어지는 &lt;strong&gt;degradation&lt;/strong&gt; 현상이 발생한다. 논문에서는 degradation 문제를 더 살펴보기 위해 time constraint 없이 네트워크 깊이만 늘려보면서 error rate의 변화를 살펴봤다. 논문에서 실험한 여러가지 모델 중에 D라는 모델을 사용해 실험하였다. ImageNet 데이터에 실험한 다음 표를 보면 D+4 까지는 error rate가 준다. 하지만 D+6, D+8의 경우 오히려 error rate가 늘어난다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/g7kmy4ih8hg8bn8/Screenshot%202018-11-16%2016.42.00.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;highway-network&quot;&gt;Highway Network&lt;/h3&gt;
&lt;p&gt;ResNet이 처음 Degradation 문제를 해결하기 위해 새로운 네트워크 구조를 제안한 것이 아니다. ResNet이 나온 2015년에 Highway Network 논문이 나왔다. Highway network 또한 “training deeper networks is not as straightforward as simply adding layers.” 이라고 언급하며 단순히 깊이를 늘리는 것 이외의 방법이 필요하다고 말한다. Highway network는 &lt;strong&gt;LSTM(Long Short-Term Memory Models)&lt;/strong&gt;의 구조에서 영감을 받아서 만든 네트워크이다.&lt;/p&gt;

&lt;p&gt;LSTM은 기존 RNN의 vanishing gradient 문제를 해결하기 위해 나타났다. RNN과 LSTM의 중요한 차이는 hidden state 이외의 cell state가 존재하는 것이다. Cell state는 일종의 &lt;strong&gt;information highway&lt;/strong&gt;로 작동한다. 기존 RNN에서는 이전 step의 정보가 다음 step으로 넘어갈 때 많은 반드시 non-linear 연산을 거쳐야한다. 하지만 LSTM에서는 cell state에 저장된 정보가 다음 step으로 넘어갈 때 곱하기와 더하기 연산만 거친다. 따라서 처음의 cell state 정보가 오랫동안 남아있을 수 있다. 아래 그림에서 위가 RNN이고 아래가 LSTM이다. LSTM에서는 위를 관통하는 하나의 선이 있는 것을 볼 수 있는데 이게 cell state이다. 마치 고속도로와 같기 때문에 information highway라고 하기도 한다. 혹시 RNN과 LSTM의 작동 방식을 잘 모른다면 &lt;a href=&quot;https://youtu.be/8HyCNIVRbSU&quot;&gt;Illustrated Guide to LSTM’s and GRU’s: A step by step explanation&lt;/a&gt; 영상을 추천한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/g4rhf45rag1tfqv/Screenshot%202018-11-16%2017.23.54.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://imgur.com/jKodJ1u
  &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;LSTM에서는 시간에 따라 정보가 유지되도록 했다면 Highway network에서는 앞단의 layer에서의 정보가 뒷 단의 layer로 유지되도록 한 것이다. Neural network가 L개의 layer로 이루어져있다고 하자. 각 layer에서는 &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;라는 non linear transformation을 input &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;에 적용한다. 이 때 출력을 &lt;code class=&quot;MathJax_Preview&quot;&gt;y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;라고 하면 각 layer에서의 연산은 다음과 같이 쓸 수 있다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;y = H(x, W_H)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = H(x, W_H)&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Highway network에서는 &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;C&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;라는 새로운 nonlinear transform을 사용한다. &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;는 transform gate를 의미하고 &lt;code class=&quot;MathJax_Preview&quot;&gt;C&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C&lt;/script&gt;는 carry gate를 의미한다. Transform gate는 &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; 연산을 거친 정보를 얼마나 반영할지에 대한 gate이다. Carry gate는 입력으로 들어왔던 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;의 정보를 얼마나 유지할까에 대한 gate이다. 입력 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;가 마치 LSTM의 cell state와 같다라고 생각하면 이해가 쉬울 것이다. 이 두 개의 gate는 0에서 1사이의 값을 가진다. 
새로운 정보를 &lt;strong&gt;transform&lt;/strong&gt;을 더 많이 하려면 input &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;의 정보를 더 줄여야하고 input &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;의 정보를 더 &lt;strong&gt;carry&lt;/strong&gt;하려면 carry gate의 값이 커져야한다. 따라서 &lt;code class=&quot;MathJax_Preview&quot;&gt;C = 1 - T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;C = 1 - T&lt;/script&gt;로 정의할 수 있다. 이것을 반영해서 Highway Network의 한 layer 연산은 다음과 같이 쓸 수 있다. &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;는 0에서 1사이의 값을 가지기 때문에 sigmoid function을 사용한다. 즉, &lt;code class=&quot;MathJax_Preview&quot;&gt;T(x) = \sigma(W_Tx + b_T)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T(x) = \sigma(W_Tx + b_T)&lt;/script&gt;이다. 만약 &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;가 0이라면 입력인 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;가 그대로 출력으로 나간다. &lt;code class=&quot;MathJax_Preview&quot;&gt;T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;T&lt;/script&gt;가 1이라면 &lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;의 출력이 해당 layer의 출력이 된다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;y = H(x, W_H)\cdot T(x, W_T) + x\cdot (1-T(x, W_T))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = H(x, W_H)\cdot T(x, W_T) + x\cdot (1-T(x, W_T))&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Highway network를 간단히 코드로 살펴보면 다음과 같다. self.gate와 self.nonlinear는 각각 &lt;code class=&quot;MathJax_Preview&quot;&gt;W_T, W_H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;W_T, W_H&lt;/script&gt;라고 볼 수 있다. 이 코드가 하나의 layer이고 이런 layer를 쭉 쌓으면 Highway network가 된다. 코드를 보면 직관적으로 Highway network를 이해할 수 있을 것이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;gate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nonlinear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nonlinear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gate&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nonlinear&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Highway network의 경우 논문에서 fully connected layer에 대해서만 실험하였다. 이 논문에서는 plain network(information highway가 없는 일반 네트워크)와 highway 네트워크를 깊이를 다르게하며 성능을 비교했다. 아래 그림을 보면 네트워크의 깊이가 10에서 100으로 달라질 때 plain network와 highway network의 학습 과정이 어떻게 다른지 볼 수 있다. 네트워크의 깊이가 얕을 때는 plain network가 더 좋은 성능을 보인다. 하지만 네트워크가 깊어질수록 highway network의 성능이 plain network의 성능보다 더 높아진다. 사실 &lt;strong&gt;highway network만 본다면 깊이가 늘어나는 것에 따라 네트워크의 error rate가 거의 변하지 않는 것&lt;/strong&gt;을 볼 수 있다. 이 실험결과를 통해 information highway와 같이 이전 layer의 정보를 다음 layer에 전해주는 방식이 layer를 더 깊게 쌓는데 효과가 있음을 알 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/gw5ldeetlvdlfdk/Screenshot%202018-11-17%2022.57.15.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1505.00387.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;resnet&quot;&gt;ResNet&lt;/h3&gt;
&lt;p&gt;ResNet 논문 또한 Highway network와 같이 degradation 문제를 해결하기 위한 방법을 제안한다. ResNet 논문에서도 degradation 현상을 실험을 통해 확인했다. 다음 그림에서 20-layer와 56-layer(뒤에서 이 네트워크의 구조를 설명할 것이다. 학습은 CIFAR-10에 대해서 한 것이다) 의 training error와 test error의 차이를 볼 수 있다. 네트워크의 층이 더 깊어졌을 때 오히려 training error와 test error 둘 다 높아지는 것을 볼 수 있다. 따라서 이 현상은 overfitting이 아닌 degradation 문제이다. ResNet 논문과 Highway network 논문이 둘 다 degradation 문제를 해결하려 한 것이라면 어떤 점이 다를까? ResNet과 Highway network의 중요한 차이점은 (1) &lt;strong&gt;gate를 학습하는 것이 아니라 residual learning을 하는 것&lt;/strong&gt; (2) &lt;strong&gt;CNN에 적용했다는 점&lt;/strong&gt; 이다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/je1s2bdftki4k7y/Screenshot%202018-11-17%2023.13.54.png?dl=1&quot; width=&quot;500px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1505.00387.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
왜 20-layer 네트워크보다 56-layer 네트워크의 성능이 저하되는 것일까? 이론상 만일 20층부터 56층까지의 layer가 identity mapping을 해준다면 성능은 동일해야한다. 하지만 여러 층의 non-linear function을 identity mapping이 되도록 학습시키는 것은 쉽지 않다. Plain 네트워크는 정보가 흐를 수 있는 길이 하나밖에 없다. 다음 그림은 일반적인 plain 네트워크의 하나의 layer이다. Weight layer는 convolution layer이며 activation function은 relu이다. 이 plain 네트워크는 간단히 &lt;code class=&quot;MathJax_Preview&quot;&gt;H(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H(x)&lt;/script&gt;라고 쓸 수 있다. 만약 이 layer가 identity mapping을 학습한다면 &lt;code class=&quot;MathJax_Preview&quot;&gt;H(x) = x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H(x) = x&lt;/script&gt;가 되도록 &lt;code class=&quot;MathJax_Preview&quot;&gt;H(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H(x)&lt;/script&gt;를 맞추면 된다. 하지만 층이 깊은 네트워크에서 여러 개의 layer를 identity mapping이 되도록 학습한다는 것은 쉽지 않다. Identity function 자체는 상당히 간단한 함수이다. ResNet은 이 문제를 간단한 방법으로 해결한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/shwfv3c5ykymaeh/Screenshot%202018-11-18%2012.20.30.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Residual 이라는 것은 일종의 오차이다. 기존의 neural network가 direct mapping을 학습했다면 &lt;strong&gt;Residual Learning&lt;/strong&gt;은 입력으로부터 얼만큼 달라져야하는지를 학습한다. &lt;code class=&quot;MathJax_Preview&quot;&gt;H(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H(x)&lt;/script&gt;가 학습해야하는 mapping이라면 &lt;code class=&quot;MathJax_Preview&quot;&gt;H(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H(x)&lt;/script&gt;를 &lt;code class=&quot;MathJax_Preview&quot;&gt;H(x) = F(x) + x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H(x) = F(x) + x&lt;/script&gt;으로 새롭게 정의한다. 네트워크 학습 목표를 &lt;code class=&quot;MathJax_Preview&quot;&gt;H(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H(x)&lt;/script&gt; 학습에서 &lt;code class=&quot;MathJax_Preview&quot;&gt;F(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F(x)&lt;/script&gt; 학습으로 바꾸는 것이다. 먄약 identity mapping을 학습하고자 한다면 간단히 &lt;code class=&quot;MathJax_Preview&quot;&gt;F(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F(x)&lt;/script&gt;를 0으로 만들면 된다. 다음 그림은 Residual mapping을 학습하는 neural network을 보여준다. 기존 plain network의 차이는 identity x라고 써져있는 &lt;strong&gt;shortcut connection&lt;/strong&gt; 부분이다. ResNet이 지금처럼 널리 사용되는 것은 성능 때문일수도 있지만 방법이 상당히 간단하다는데 있다. Shortcut connection이라는 것은 몇 개의 layer를 건너뛰는 것(skip)을 말한다. ResNet에서 Shortcut connection은 2개의 layer를 건너뛴다. Layer에 들어오는 입력이 shortcut connnection을 통해서 건너뛰면 layer를 지난 출력과 element-wise addition 한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/odzu3mma87klx3n/Screenshot%202018-11-18%2012.20.53.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;후속 논문인 “Identity Mappings in Deep Residual Networks”에서는 Residual block
ResNet의 한 layer를 수식으로 나타내면 다음과 같다. 수식에서 &lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt; 밑에 &lt;code class=&quot;MathJax_Preview&quot;&gt;i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;i&lt;/script&gt;가 들어가는 것은 &lt;code class=&quot;MathJax_Preview&quot;&gt;F&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;가 하나의 layer가 아니라 여러 개의 layer라는 뜻이다. 위에서 언급했듯이 &lt;code class=&quot;MathJax_Preview&quot;&gt;F&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;는 2개의 layer로 이루어져있다. 수식으로는 &lt;code class=&quot;MathJax_Preview&quot;&gt;F=W_2(\sigma (W_1 x))&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F=W_2(\sigma (W_1 x))&lt;/script&gt;으로 사용한다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\sigma&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\sigma&lt;/script&gt;는 ReLU activation function을 의미한다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;y = F(x, {W_i}) + x&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = F(x, {W_i}) + x&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;보통은 Convolution layer 이후에 relu activation function 연산을 수행한다. ResNet 논문에서 중요한 점 중에 하나가 &lt;strong&gt;Batch Normalization&lt;/strong&gt;을 사용한다는 것이다. Shortcut connection을 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;라 했을 때 shortcut connection과 합쳐지는 것은 &lt;code class=&quot;MathJax_Preview&quot;&gt;bn(conv(relu(bn(conv(x)))))&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;bn(conv(relu(bn(conv(x)))))&lt;/script&gt;가 된다. 이 연산과정을 다른 그림으로 보자면 다음과 같다. shortcut connection과 residual mapping을 더한 다음에 ReLU를 취하는 것을 볼 수 있다. 이렇게 하는 이유는 ReLU를 통과하면 +의 값만 남기 때문에 Residual의 의미가 제대로 유지되지 않기 때문이다. 이 연산을 하는 부분을 &lt;strong&gt;Residual Block&lt;/strong&gt;이라고 부른다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/jbh4yhssogopgk0/Screenshot%202018-11-18%2013.52.58.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Residual block을 코드로 보자면 다음과 같다. ResNet의 전체 코드는 뒤에서 살펴볼 것이다. conv1과 conv2는 2d convolution을 의미한다. 처음 입력 x는 self.bn2까지 거친 out과 element-wise addition을 한다. ResNet은 이 Residual Block이 여러 층으로 쌓은 neural network를 의미한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;shorcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;정확히 ResNet의 구조는 어떻게 되는 것일까? ResNet의 구조는 논문에서 ResNet과 비교하는 plain net과 거의 동일하다. 논문의 Plain net은 VGG의 구조에서 영감을 받아 만들었다. Plain net에서 여러 convolutional layer가 있는데 각 convolutional layer은 두 가지 규칙을 지킨다. (1) 만약 feature map size가 같다면 해당 convolutional layer의 filter 개수는 같다. (2) 만약 feature map size가 반으로 준다면 filter의 개수는 2배가 된다. 이 디자인 규칙은 ResNet도 그대로 가지고 있다. feature map size를 반으로 줄이는 것은 convolution을 할 때 stride를 2로 설정하는 것으로 한다. 마지막 convolutional layer의 output은 global average pooling을 통해서 pooling을 한다. 그 결과를 fully connected layer를 통해 class 개수만큼의 출력으로 변환한다.&lt;/p&gt;

&lt;p&gt;위에서 Residual block을 설명할 때 언급했던 것처럼 plain net에서도 batch normalization을 relu activation function 전에 사용한다. ResNet은 Plain network에서 두 개의 convolutional layer에 shortcut connection을 추가한 것이다. VGG와 plain network 그리고 ResNet의 구조를 비교한 그림은 다음과 같다. Plain network는 VGG에서 몇 개의 layer를 더 추가한 것임을 알 수 있다. Plain network는 VGG의 초반 몇 개의 3x3 convolution을 7x7 convolution으로 대체했다. 또한 global average pooling을 마지막 convolutional layer 출력에 적용하기 때문에 VGG에 비해 fully connected layer가 적다. ResNet은 plain network에서 두 개의 convolutional block을 skipping하는 shortcut connection을 추가한 것이다. 그림의 shortcut connection 중에서 점선인 것은 feature map size가 반으로 줄어든 경우를 의미한다. 다른 실선의 shortcut connection은 모두 parameter가 없지만 점선의 shortcut connection의 경우 1x1 convolution과 batch normalization을 적용하기 때문에 학습 대상인 connection이다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/eqfw171k9kn7gik/Screenshot%202018-10-09%2021.49.14.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
  https://arxiv.org/pdf/1512.03385.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다음 표는 Plain network와 ResNet의 구조를 깊이마다 표로 정리한 것이다. 이 구조는 ImageNet 데이터에 대해 학습할 때 사용하는 구조이다. CIFAR 데이터의 경우 이미지 사이즈가 더 작기 때문에 몇가지 점이 다르다. ImageNet에서 사용된 네트워크 구조와 CIFAR 데이터에서 사용된 네트워크 구조가 어떻게 다른지는 뒤에서 이야기하겠다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/5fawzywoqa4v7wm/Screenshot%202018-10-10%2000.32.49.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
  https://arxiv.org/pdf/1512.03385.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ResNet을 제안한 이유는 degradation이라는 문제를 해결하기 위해서이다. 논문에서는 ImageNet 데이터에 대해서 여러가지 깊이로 plain net과 ResNet을 학습했다. 그 결과는 다음 그래프와 같다. 왼쪽은 plain network에 대한 실험 결과이고 오른쪽은 ResNet에 대한 실험 결과이다. Plain net의 경우 18층에서 34층으로 깊이를 늘리면 training error가 늘어나는 것을 볼 수 있다. 하지만 ResNet의 경우 18층에서 34층으로 깊이를 늘렸을 때 training error가 줄어든다. 이 실험결과를 통해 ResNet이 degradation 문제를 어느 정도 해결했다는 것을 알 수 있다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/g3uljkfs2eu78w1/Screenshot%202018-10-10%2000.34.43.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1512.03385.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ResNet의 CIFAR-10에서의 성능을 살펴보자. 그 전에 하나 알아가야할 것이 있다. ResNet의 깊이가 점점 깊어지면 경우 parameter의 수가 너무 많아지기 때문에 residual block으로 다른 구조를 사용한다. Residual block의 원래 구조는 아래 그림의 왼쪽과 같다. 50층 이상인 ResNet에서는 오른쪽 그림과 같은 residual block을 사용한다. 1x1 convolution을 통해 filter size를 줄인 이후에 3x3 convolution을 하면 파라메터의 수를 아낄 수 있다. 이러한 구조의 residual block을 &lt;strong&gt;bottelnet block&lt;/strong&gt;이라고 부른다. Bottleneck 구조를 사용해 ResNet의 내부 구조를 바꾸면 152 layer까지 쌓아도 vgg보다 모델 크기가 작다.&lt;/p&gt;
&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/blbivxvl6kjllfn/Screenshot%202018-11-18%2021.31.58.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1512.03385.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
CIFAR-10에서 성능은 다음과 같다. 110 개의 층을 가지는 ResNet이 test data에 대해서 6.43 %의 error rate를 기록했다. 하나 유의해서 볼 점은 1202개의 층을 가지는 ResNet의 성능이다. ResNet은 기존 네트워크보다 훨씬 깊게 쌓을 수 있다는 점이 장점이다. 하지만 1000개 이상의 깊이를 가지게 되면 다시 degradation 문제를 만단다. ResNet-110이 6.43% error rate를 가지는 반면 ResNet-1202는 7.93% error rate를 가진다. 1000개의 층을 쌓아도 degradation 문제가 생기지 않게 할 수 있다. 그 부분은 “From 100 to 1000 Layers” 파트에서 살펴볼 것이다. 이제 코드로 ResNet의 구조와 학습 방법을 세세하게 보자.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/pihvoi3pn1xyk48/Screenshot%202018-10-12%2022.55.37.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
    https://arxiv.org/pdf/1512.03385.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;resnet-code-review&quot;&gt;ResNet code review&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/dnddnjs/pytorch-cifar10/tree/enas/resnet&quot;&gt;ResNet code link&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;코드는 ResNet 모델이 정의되어있는 model.py와 모델을 불러와 학습시키는 train.py로 구성되어있다. 우선 model.py를 통해 ResNet의 구조를 코드를 통해 살펴보겠다. 대략적인 CIFAR 데이터에서 학습하는 ResNet의 흐름은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;input image에 대해 conv3x3 + bn + relu 적용(3 channel –&amp;gt; 16 channel)&lt;/li&gt;
  &lt;li&gt;2n 개의 layer (Residual block n개, 16 channel –&amp;gt; 16 channel)&lt;/li&gt;
  &lt;li&gt;2n 개의 layer (Residual block n개, 16 channel –&amp;gt; 32 channel)&lt;/li&gt;
  &lt;li&gt;2n 개의 layer (Residual block n개, 32 channel –&amp;gt; 64 channel)&lt;/li&gt;
  &lt;li&gt;global average pooling + fully connected&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;n에 따라 전체 ResNet의 깊이가 달라진다. n은 3, 5, 7, 9, 18 중에 하나를 사용한다(논문에서 그렇다. 다른 숫자를 사용해도 무방하다). 각각 ResNet-20, ResNet-32, ResNet-44, ResNet-56, ResNet-110에 해당한다. 이 포스트에서는 n을 5로 사용해서 ResNet-32를 학습시켜봤다. ResNet 모델 내부 흐름이 위와 같기 때문에 ResNet의 forward 부분도 다음과 같다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_2n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_4n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_6n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;avg_pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;처음 입력에 적용되는 self.conv1과 self.bn1, self.relu는 모든 ResNet에서 동일하다. 이 함수들을 정의하는 코드는 다음과 같다. 입력으로 RGB 이미지를 사용하기 때문에 convolution layer에 들어오는 input의 channel 수는 3이 된다. Convolution filter의 크기로 3을 사용하고 padding을 1, stride를 1로 사용하면 feature map의 사이즈는 유지된다. 처음 입력으로 3x32x32의 tensor가 들어오면 이 부분을 지나면 16x32x32의 feature map이 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
             &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;첫 convolutional layer를 지나면 세 개의 2n layer 짜리 함수를 지난다. 이렇게 2n개씩 구분한 이유는 2n개의 layer마다 feature map의 사이즈가 반이 되고 channel 수는 2배가 되기 때문이다. 처음 2n개의 layer에서 feature map은 16x32x32의 크기를 가진다. 그 다음 2n개의 layer에서는 feature map이 32x16x16의 크기를 가진다. 그 다음 2n개의 layer에서는 feature map이 64x8x8의 크기를 가진다. 각 2n개의 layer는 self.get_layers 함수를 통해 생성한다. self.get_layers의 인자에는 block, in_channels, out_channels, stride가 있다. 만약 in_channels와 out_channels가 다르면 feature map의 사이즈는 반으로 줄고 channel은 2배가 되는 연산이 일어나야한다. 그 부분은 뒤에서 설명하겠다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# feature map size = 16x32x32
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_2n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# feature map size = 32x16x16
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_4n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;# feature map size = 64x8x8
&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_6n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;self.get_layers의 인자인 block은 residual block을 의미한다. Residual block은 다음 그림과 같다고 위에서 이야기했었다. Shortcut connection이 있고 residual 부분은 conv3x3 + bn + relu + conv3x3 + bn의 연산을 한다. Shortcut connection과 residual은 element-wise addition을 수행한다. Addition을 하고 나서 한 번 더 ReLU를 취한다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/jbh4yhssogopgk0/Screenshot%202018-11-18%2013.52.58.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
    https://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
Residual block을 정의하는 코드는 다음과 같다. Residual block은 nn.Module을 상속받는 class로 정의한다. Residual block의 인자는 in_channels, out_channels, stride, down_sample이다. In_channels와 out_channels는 두 개의 convolutional layer 중에 첫 번째 convolution의 in, out channels를 의미한다. 두 번째 convolutional layer는 out_channels로 입력이 들어와서 out_channels로 출력이 나간다. In_channels와 out_channels가 다르면 첫 번째 convolutional layer에서 stride를 2로 사용한다. 이 경우에 shortcut connection으로 전달되는 x와 self.bn2를 지난 out이 크기가 다르다. 그 크기를 맞춰주기 위한 것이 down sample이다. ImageNet에 학습되는 ResNet의 경우 이 down sample로 1x1 convolution을 사용한다. 하지만 CIFAR에 학습되는 ResNet의 경우 down sample로 zero padding을 사용한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;ResidualBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ResidualBlock&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                         &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ReLU&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Conv2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kernel_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                         &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bias&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BatchNorm2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IdentityPadding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;


  &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bn2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;shortcut&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;IdentityPadding을 정의하는 부분은 다음과 같다. Feature map의 사이즈를 줄이는 것은 max pooling을 통해서 하고 feature map의 channel을 늘리는 것은 zero padding을 통해서 해준다. F.pad 함수에서 두 번째 인자가 padding 어떻게 줄 것인지에 대한 것이다. 코드에 (0, 0, 0, 0, 0, self.add_channels)라고 되어있는데 이것은 feature map의 마지막 축에 대해서는 (0, 0)으로 padding하고 마지막에서 두 번째 축에 대해서는 (0, 0), 그리고 마지막에서 세 번째 축은 (0, self.add_channels)로 padding하라는 뜻이다. 따라서 channels 축에 대해서 한 방향으로 self.add_channels만큼 padding이 될 것이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;IdentityPadding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IdentityPadding&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MaxPool2d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
block을 가지고 2n개의 layer를 만드는 get_layer 함수가 정의된 부분은 다음과 같다. Down sample이 일어날 경우(feature map 사이즈는 반이 되고 channel 수는 2배가 되는 첫 번째 residual block에서 down sample을 적용한다. 나머지 residual block에서는 일반적인 residual block의 구조를 가진다. nn.Sequential(*layers_list)는 layers_list에 들어있는 block들을 차례대로 연산하는 모듈을 생성한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;get_layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
  
  &lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ModuleList&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;in_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;stride&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;down_sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)])&lt;/span&gt;
    
  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_layers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_channels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sequential&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers_list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이 ResNet은 다음과 같은 함수를 통해 정의된다. n을 5로 사용하면 6x5 + 2 = 32개의 층을 가지는 ResNet을 만드는 것이다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;resnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResidualBlock&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;# total number of layers if 6n + 2. if n is 5 then the depth of network is 32.
&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ResNet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;block&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이제 train.py의 내용을 살펴보자. CIFAR-10 데이터를 가지고 학습하려면 데이터를 불러오고 학습이나 테스트를 위해 mini-batch 형식으로 읽어와야 한다. PyTorch는 torchvision 안에 데이터셋으로 CIFAR10을 가지고 있다. 따라서 밑의 코드처럼 CIFAR10으로 함수를 정의하면 데이터를 사용할 수 있다. Download를 True로 설정하면 자동으로 ../data 폴더에 다운로드를 한다. Train이 True이면 training dataset을 불러오는 것이며 50000개의 이미지 데이터를 가져온다. Train이 False이면 testing dataset을 불러오는 것이고 10000개의 이미지 데이터를 가져온다. DataLoader는 mini batch 사이즈만큼 호출될 때마다 이미지와 라벨을 가져오는 함수이다. 어떤 데이터셋에서 mini batch를 가져올지 인자로 넣어주면 된다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torchvision.datasets&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CIFAR10&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.utils.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;dataset_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CIFAR10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'../data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
            &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dataset_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CIFAR10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'../data'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
             &lt;span class=&quot;n&quot;&gt;download&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transform&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;transforms_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                        &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_worker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;DataLoader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                       &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_workers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_worker&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;CIFAR10 데이터셋을 정의할 때 각 데이터셋에 적용될 augmentation을 정해준다. Training dataset은 4로 padding한 이후에 32의 크기로 random cropping을 하고 horizontal flip을 랜덤하게 수행한다. 그 이후에 이미지의 평균과 표준편차로 standardization 해준다. Testing dataset은 standardization만 수행한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;transforms_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomCrop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandomHorizontalFlip&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4914&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4822&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4465&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2023&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1994&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2010&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;transforms_test&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Compose&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ToTensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;transforms&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Normalize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;((&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.4914&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4822&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.4465&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2023&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.1994&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.2010&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;PyTorch에서는 다음과 같은 코드로 모델에 포함되어있는 parameter의 수를 셀 수 있다. 논문에 따르면 ResNet-32의 경우 0.46M개의 parameter가 있다. 코드에서 출력한 parameter의 개수는 464154개이다. 따라서 모델의 구조를 제대로 설정했음을 알 수 있다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;resnet&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;num_params&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;requires_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;criterion은 loss function을 의미한다. Classification 문제이기 때문에 cross entropy를 사용한다. Optimizer는 SGD with momentum을 사용한다. SGD의 learning rate는 처음에 0.1로 설정하고 32000 번 업데이트를 하고나면 0.01로 48000번 업데이트를 하고나면 0.001로 설정한다. Pytorch에는 lr_scheduler가 있어서 이런 learning rate decay를 손쉽게 할 수 있도록 지원한다.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;optim&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SGD&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameters&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                    &lt;span class=&quot;n&quot;&gt;momentum&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.9&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;weight_decay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;1e-4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;decay_epoch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;48000&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;step_lr_scheduler&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lr_scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MultiStepLR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
                 &lt;span class=&quot;n&quot;&gt;milestones&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decay_epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gamma&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Training 부분은 상당히 간단하다. 모델 net을 train()을 통해 학습 모드로 전환해준다. train_loader에서 mini batch씩 데이터를 꺼내와서 loss를 계산한다. 계산한 loss에 대해 batch propagation을 하고 optimizer가 net을 업데이트한다. learning rate scheduler가 update step 단위로 체크하기 때문에 매 mini batch마다 한 번씩 수행해준다. 테스트 코드도 이와 동일하기 때문에 설명은 생략한다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_steps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;

  &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;global_steps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;step_lr_scheduler&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;criterion&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;predicted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;targets&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    
  &lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;correct&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train epoch : {} [{}/{}]| loss: {:.3f} | acc: {:.3f}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
       &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;ResNet-32를 64000 step 동안 업데이트한 학습 과정은 다음과 같다. 왼쪽은 test dataset에 대한 error rate(%)이고 오른쪽은 train dataset에 대한 error rate(%)이다. 가장 낮은 test error rate는 7.46%인데 논문의 결과는 7.51%이니 재현이 되었다고 본다. 밑의 그림은 논문의 학습 곡선이다. 실선이 test error rate이고 점선이 train error rate이다. ResNet-32로 표시된 결과와 비교해도 결과가 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/mt460odvmrcnfh3/Screenshot%202018-11-19%2022.11.28.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/h35rtyw70yenpju/Screenshot%202018-11-19%2023.49.21.png?dl=1&quot; width=&quot;400px&quot; /&gt;
  &lt;figcaption&gt;
     https://arxiv.org/pdf/1512.03385.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;코드는 https://github.com/dnddnjs/pytorch-cifar10/tree/enas/resnet 에서 다운로드 받을 수 있다. Pytorch, torchvision, numpy, tensorboardx, tensorboard, tensorflow를 설치해야한다. 다음과 같은 명령어로 실행할 수 있으니 직접 학습시켜보기 바란다.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python train.py
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;from-100-to-1000-layers&quot;&gt;From 100 to 1000 Layers&lt;/h2&gt;
&lt;p&gt;ResNet이 이전의 네트워크에 비해 상당히 깊은 네트워크를 학습시키며 degradation 문제를 해결했다. 하지만 1000 층 정도의 ResNet은 여전히 degradation 문제를 가지고 있다. ResNet의 저자인 Kaiming He는 다음 해에 &lt;strong&gt;Identity Mappings in Deep Residual Networks&lt;/strong&gt;&lt;sup id=&quot;fnref:7&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot;&gt;8&lt;/a&gt;&lt;/sup&gt;이라는 논문을 발표했다. 이 논문에서는 새로운 Residual block의 구조를 제안하며 1001 층의 ResNet의 성능을 올렸다. ResNet-1001층의 CIFAR-10에서의 test error rate는 4.62%이다. 여기서는 간단히 어떻게 개선을 했는지만 소개할 것이다.&lt;/p&gt;

&lt;p&gt;아래 그림에서 왼쪽이 어떻게 residual block의 구조를 바꿨는지 보여준다. 원래는 conv3x3 + bn + relu + conv3x3 + bn의 연산을 한 이후에 shortcut connection과 더하고 그 이후에 relu를 취했다. 하지만 새로운 residual block에서는 bn + relu + conv3x3 + bn + relu + conv3x3의 연산을 취한 뒤에 shortcut connection과 더한다. 기존 residual block을 &lt;strong&gt;post-activation&lt;/strong&gt;이라고 부르고 새로운 residual block을 &lt;strong&gt;pre-activation&lt;/strong&gt;이라고 부른다.&lt;/p&gt;

&lt;figure&gt;
  &lt;img src=&quot;https://www.dropbox.com/s/twmko76rcsk4jkl/Screenshot%202018-11-20%2000.03.55.png?dl=1&quot; /&gt;
  &lt;figcaption&gt;
     https://arxiv.org/pdf/1603.05027.pdf
  &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;br /&gt;
논문에서는 새로운 residual block의 구조가 더 높은 성능을 달성한 것은 두 가지 이유가 있다고 설명한다. (1) optimization을 더 쉽게 해주었다. 위 학습 그래프를 보면 기존 ResNet보다 Pre-activation ResNet이 training loss가 더 빠르게 주는 것을 볼 수 있다. 이것은 기존 residual block에서는 ReLU 이후에 다음 residual block으로 넘어가기 때문에 back-propagation 할 때 ReLU에 의해 truncated 될 수 있기 때문이다. 하지만 새로운 residual unit에서는 back-propagation 할 때 identity에 대한 부분은 출력부분부터 입력부분까지 유지될 수가 있다. 따라서 좀 더 optimization이 쉬워진다. (기존 ResNet 또한 100 층 정도까지는 optimization에 어려움이 생기지는 않는다)&lt;/p&gt;

&lt;p&gt;(2) Batch-normalization으로 인한 regularization 효과 때문이다. 기존 Residual Block의 경우 BN을 한 이후에 identity mapping과 addition을 해주고 그 이후에 ReLU가 나온다. 따라서 Batch normalization의 regularization 효과를 충분히 보지 못한 것이다. 새로운 residual block에서는 Batch normalization 이후에 바로 ReLU가 나오므로 성능 개선의 여지가 있던 것이다.&lt;/p&gt;

&lt;p&gt;이 논문 이후에 나오는 논문에서는 Pre-activation 구조를 사용하는 경우가 많다. 따라서 이 논문의 내용을 알고 있는 것이 좋다. 자세한 내용은 논문을 읽어보는 것을 추천한다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;참고문헌&quot;&gt;참고문헌&lt;/h3&gt;
&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:0&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1512.03385.pdf &lt;a href=&quot;#fnref:0&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1502.01852.pdf &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;http://proceedings.mlr.press/v37/ioffe15.pdf &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1409.4842.pdf &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot;&gt;
      &lt;p&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/He_Convolutional_Neural_Networks_2015_CVPR_paper.pdf &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1505.00387.pdf &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1603.05027.pdf &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Tue, 09 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/cifar10/2018/10/09/resnet/</link>
        <guid isPermaLink="true">http://localhost:4003/cifar10/2018/10/09/resnet/</guid>
        
        <category>dl</category>
        
        
        <category>cifar10</category>
        
      </item>
    
      <item>
        <title>CIFAR-10 정복 시리즈 0: 시작하기</title>
        <description>&lt;h2 id=&quot;cifar-10-정복하기-시리즈-소개&quot;&gt;CIFAR-10 정복하기 시리즈 소개&lt;/h2&gt;
&lt;p&gt;CIFAR-10 정복하기 시리즈에서는 딥러닝이 CIFAR-10 데이터셋에서 어떻게 성능을 높여왔는지 그 흐름을 알아본다. 또한 코드를 통해서 동작원리를 자세하게 깨닫고 실습해볼 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;CIFAR-10 정복하기 시리즈 목차(클릭해서 바로 이동하기)
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/07/start_cifar10/&quot;&gt;CIFAR-10 정복 시리즈 0: 시작하기&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/09/resnet/&quot;&gt;CIFAR-10 정복 시리즈 1: ResNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/24/pyramidnet/&quot;&gt;CIFAR-10 정복 시리즈 2: PyramidNet&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/cifar10/2018/10/25/shake_shake/&quot;&gt;CIFAR-10 정복 시리즈 3: Shake-Shake&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;관련 코드 링크
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://github.com/dnddnjs/pytorch-cifar10&quot;&gt;pytorch cifar10 github code&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;h3 id=&quot;딥러닝과-비전&quot;&gt;딥러닝과 비전&lt;/h3&gt;
&lt;p&gt;2012년 AlexNet&lt;sup id=&quot;fnref:1&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;의 등장 이후로 딥러닝은 무섭게 발전했다. 딥러닝의 Image Classification에서 뛰어난 능력을 보여줬지만 그 자리에만 머무르지 않았다. 6년이 지난 지금 딥러닝을 빼놓고 기술을 말하기가 어려워졌다. 개발자가 아닌 일반인들도 딥러닝이라는 단어를 알 정도로 기술의 영향력이 커졌다. 다음은 2018년 가장 성공한 AI 스타트업 100개를 모아놓은 곳이다. 이 100개의 기업이 받은 투자는 100억 달러가 넘는다. 이 중에서 11개의 기업은 소위 “유니콘” 기업이 되었다. 교육, 헬스케어, IOT, 여행, 로보틱스 등 AI 스타트업이 걸치지 않은 분야가 없을 정도이다. AI라고 불리우는 기술은 대부분 딥러닝을 사용한다. AI라는 단어를 사용하지 않더라도 이미 제품이나 서비스를 가지고 있는 기업에서도 딥러닝을 광범위하게 사용한다. 이 점을 고려하면 딥러닝만큼 다양한 application에 적용되는 기술을 찾기 어렵다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/xhfhavvkppujqan/Screenshot%202018-11-04%2012.03.08.png?dl=1&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;

&lt;center&gt;이미지 출처: https://www.cbinsights.com/research/artificial-intelligence-top-startups/&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;이렇다보니 많은 사람들이 연구를 위해 혹은 사업을 위해 딥러닝을 공부한다. 딥러닝이 여러 분야에서 성과를 내고 있지만 딥러닝의 기본은 vision 분야에서 나오는 경우가 많다. 강화학습 또한 딥러닝을 통해 이전에는 하기 힘들었던 task를 에이전트가 학습한다. 그 중에 대표적인 적인 것이 OpenAI의 “Learning Dexterity&lt;sup id=&quot;fnref:2&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;“라는 연구이다. 이 연구가 하고자 하는 것은 시뮬레이션 상에서 로봇손이 어떤 물체를 손안에서 돌리는 것이다. 시뮬레이션 상에서 학습이 끝나면 실제 로봇손에서 같은 일을 수행한다. 로봇손이 어떻게 물체가 어떤 상태에 있는지 알 수 있을까? 바로 카메라로 그 물체를 찍고 그 물체의 이미지를 CNN으로 분석하기 때문이다. 이때 카메라 영상에서 물체의 위치와 회전을 예측하는 것을 ResNet 구조를 사용한 뉴럴넷 모델이 수행한다. 다음은 OpenAI 연구에 대한 동영상이다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/jwSbzNHGflM?t=0s&quot;&gt;&lt;img src=&quot;http://img.youtube.com/vi/jwSbzNHGflM/0.jpg&quot; alt=&quot;Video Label&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;강화학습 뿐만 아니라 많은 곳에서 비전 기반의 딥러닝이 기본이 된다. 따라서 많은 사람들이 딥러닝을 시작할 때 비전 분야의 딥러닝을 공부한다. 그것이 CS231n 강의&lt;sup id=&quot;fnref:3&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;가 그렇게 인기가 많은 이유이다. CS231n은 Stanford 대학교에서 학부생을 대상으로하는 강의이다. Fei Fei Li나 Andrey Karphathy와 같은 훌륭한 연구자가 강의를 하며 온라인에 무료로 공개하였다. 이와 같은 강좌들이 많은 사람이 딥러닝을 더 쉽게 시작할 수 있도록 도와준다. CS231n 강의에서 소개한 딥러닝을 이용한 application은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Deep Learning을 이용한 Vision 분야 application
    &lt;ul&gt;
      &lt;li&gt;Image Classification&lt;/li&gt;
      &lt;li&gt;Object Localization&lt;/li&gt;
      &lt;li&gt;Object Detection&lt;/li&gt;
      &lt;li&gt;Semantic Segmentation&lt;/li&gt;
      &lt;li&gt;Instance Segmentation&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;CS231n에서 말하지 않은 application은 다음과 같다.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;그 이외의 application
    &lt;ul&gt;
      &lt;li&gt;face recognition&lt;/li&gt;
      &lt;li&gt;object tracking&lt;/li&gt;
      &lt;li&gt;optical character recognition&lt;/li&gt;
      &lt;li&gt;pose estimation&lt;/li&gt;
      &lt;li&gt;image generation&lt;/li&gt;
      &lt;li&gt;style transfer&lt;/li&gt;
      &lt;li&gt;inpainting&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Image classification은 vision 분야에서 가장 간단한 application이다. 이미지가 입력으로 들어오면 Neural Network는 이미지가 어떤 class에 해당하는지를 예측하는 것이다. 다음 그림은 이미지를 분류하는 과정을 보여준다. Image classification은 상당히 간단한 task이지만 AlexNet, GoogleNet, VGG, ResNet, DenseNet, MobileNet 등 모두 이 task를 풀기 위해 개발된 모델이다. 이 모델들은 image classification 이외의 다른 application 혹은 아예 다른 분야에서도 모델 구조로 사용되는 경우가 많다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/s9acumwosy14ah9/Screenshot%202018-11-04%2012.26.14.png?dl=1&quot; width=&quot;500px&quot; /&gt;&lt;/p&gt;
&lt;center&gt;이미지 출처: http://cs231n.stanford.edu/&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;상대적으로 연구를 하기에 간단한 task이기 때문에 image classification은 정말 다양한 연구가 되어있다. 단순히 모델 구조 뿐만 아니라 optimizer, weight initialization, regularization, data augmentation 등에 대해 풍부한 연구 결과가 쌓여있다. 따라서 image classification은 딥러닝을 처음 공부할 때 시작하기 좋은 분야이다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;더-좋은-모델-만들기&quot;&gt;더 좋은 모델 만들기&lt;/h3&gt;
&lt;p&gt;Image classification은 단순히 분류 문제이기 때문에 모델을 평가하기 쉽다. 테스트 데이터에서 대해서 얼마나 class를 잘 예측했는지를 통해 모델의 성능을 평가할 수 있다. 물론 계산량이나 모델 크기와 같은 수치로도 모델의 성능을 평가하기도 한다. 하지만 이 글에서는 accuracy만을 모델의 성능이라고 가정하고 이야기를 진행할 것이다. 딥러닝 학계에서는 모델의 성능을 올리기 위해 “딥”이라는 단어가 말해주듯 ‘어떻게 하면 모델을 더 깊게 만들 수 있을까’라는 주제에 노력을 기울였다. 하지만 2014년 VGG 논문에서 16 층의 딥러닝 모델을 만들고 2015년 GoogLeNet에서 22층의 딥러닝 모델을 만들면서 단순히 층을 쌓는 것의 한계를 체감한다. 하지만 GoogLeNet으로부터 VGG처럼 단순히 층을 쌓지 않고 창의적으로 구조를 변형하는 것이 효과가 있다는 것을 알게 된다. 2015년에 등장한 Batch Normalization과 함께 ResNet은 Paradigm Shift를 가져왔다.&lt;/p&gt;

&lt;p&gt;ResNet을 포함해서 그 이후의 논문이 CIFAR-10에서 딥러닝 모델의 성능을 키우는 방법은 크게 다음 세가지로 구분할 수 있다. ResNet이 나온 이후에는 주로 ResNet의 구조를 다양하게 변형시켜가며 학습을 해보는 것에 집중했다. 하지만 딥러닝 모델이 training data에서 100%에 가까운 정확도를 달성하면서 초점이 조금 더 generalization 성능 향상에 맞춰졌다. 따라서 regularization에 관한 연구가 활발히 진행되었다. Generalization 성능을 높이기 위해서 regularization 말고도 data 자체를 다양하게 만드는 방법도 있다. 특히 CIFAR-10는 데이터수가 별로 없기 때문에 data augmentation 방법도 효과가 꽤나 많다. 그리고 사람이 경험적으로 네트워트 구조를 디자인하는 것을 neural network가 대신하는 Neural Architecture Search에 대한 연구가 최근 활발하다. 이런 흐름으로 CIFAR-10 정복하기 시리즈를 진행할 것이다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1 Network Architecture
    &lt;ul&gt;
      &lt;li&gt;ResNet&lt;/li&gt;
      &lt;li&gt;PyramidNet&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;2 regularization
    &lt;ul&gt;
      &lt;li&gt;Shake-shake regularization&lt;/li&gt;
      &lt;li&gt;Cutout&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3 Neural Architecture Search
    &lt;ul&gt;
      &lt;li&gt;ENAS&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;왜-cifar-10인가&quot;&gt;왜 CIFAR-10인가&lt;/h3&gt;
&lt;p&gt;Image classification은 간단하면서도 앞으로 딥러닝을 공부하고 연구하는데 있어서 기본을 만들어주는 분야이다. Image classification에도 여러가지 dataset 있다. 그 중에서 CIFAR-10 데이터를 선택하였다. CIFAR-10 데이터는 학습데이터 50000개 테스트 데이터 10000개로 구성되어있다. 상당히 작은 데이터셋임에도 불구하고 가장 최근까지 정복이 안된 Challenge한 데이터셋이다. Auto-Augment 논문&lt;sup id=&quot;fnref:4&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;에서 1.48%의 error rate를 달성했다. 따라서 CIFAR-10 데이터셋은 최근에 정복되었다고 볼 수 있다. 지금까지 CIFAR-10에 적용되었던 논문을 살펴보면 어떻게 정복을 해나갔는지 그 흐름을 볼 수 있는 장점이 있다.&lt;/p&gt;

&lt;p&gt;또한 많은 경우 딥러닝을 공부할 때는 개인 장비나 클라우드의 GPU를 활용하는 경우가 많다. CIFAR-10 데이터셋은 하나의 GPU만 가지고도 학습할 수 있기 때문에 실습하기가 쉽다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;이-post를-통해-기대하는-것&quot;&gt;이 post를 통해 기대하는 것&lt;/h3&gt;
&lt;p&gt;CIFAR-10 정복하기 시리즈를 끝내고 나면 다음이 가능하기를 기대한다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;비전 분야 딥러닝 논문을 더 쉽고 빠르게 읽을 수 있다.&lt;/li&gt;
  &lt;li&gt;딥러닝이 어떻게 발전해왔는지 그 흐름을 파악할 수 있다.&lt;/li&gt;
  &lt;li&gt;여러가지 application에서 딥러닝을 쓸 때 이해를 하고 쓸 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;footnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot;&gt;
      &lt;p&gt;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot;&gt;
      &lt;p&gt;https://blog.openai.com/learning-dexterity/ &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot;&gt;
      &lt;p&gt;http://cs231n.stanford.edu/ &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot;&gt;
      &lt;p&gt;https://arxiv.org/pdf/1805.09501.pdf &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;
</description>
        <pubDate>Sun, 07 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/cifar10/2018/10/07/start_cifar10/</link>
        <guid isPermaLink="true">http://localhost:4003/cifar10/2018/10/07/start_cifar10/</guid>
        
        
        <category>cifar10</category>
        
      </item>
    
      <item>
        <title>위대한 기업은 다 어디로 갔을까</title>
        <description>&lt;ul&gt;
  &lt;li&gt;저자 : 짐 콜린스&lt;/li&gt;
  &lt;li&gt;경력 : 스탠퍼드 경영학 석사, HP, 맥킨지, 매니지먼트랩 경영연구소&lt;/li&gt;
  &lt;li&gt;목적 : 절대 망하지 않을 것 같지 않던 기업들이 어떻게 몰락하게 되었는지에 대한 통찰을 제시해 리더들이 비극적인 운명을 피할 수 있도록 돕는 것&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;몰락의-5단계&quot;&gt;몰락의 5단계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;1단계: 성공으로부터 자만심이 생겨나는 단계&lt;/li&gt;
  &lt;li&gt;2단계: 원칙없이 더 많은 욕심을 내는 단계&lt;/li&gt;
  &lt;li&gt;3단계: 위험과 위기 가능성을 부정하는 단계&lt;/li&gt;
  &lt;li&gt;4단계: 구원을 찾아 헤매는 단계&lt;/li&gt;
  &lt;li&gt;5단걔: 유명무실해지거나 생명이 끝나는 단계&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;조용히-다가오는-파멸의-전조&quot;&gt;조용히 다가오는 파멸의 전조&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;가장 강한 것이 끝까지 정상의 자리를 지키는 법은 없다. 누구든 몰락할 수 있으며 대개는 결국 그렇게 된다.&lt;/li&gt;
  &lt;li&gt;몰락에 대처하는 방법은 ‘변하지 않으면 죽는다’는 단순한 구호에서 찾을 수 있는 것이 아니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;몰락의-5단계-1&quot;&gt;몰락의 5단계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;1단계: 성공으로부터 자만심이 생겨나는 단계
    &lt;ul&gt;
      &lt;li&gt;위대한 기업들이 그동안 이룬 성공에 도취해 스스로를 격리시킨다.&lt;/li&gt;
      &lt;li&gt;그동안의 탄력 때문에 앞으로 나아가는 단계.&lt;/li&gt;
      &lt;li&gt;성공을 당연한 것으로 간주해 거만해지고 진정한 성공의 근본 원인을 잊을 때 시작됌&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;2단계: 원칙없이 더 많은 욕심을 내는 단계
    &lt;ul&gt;
      &lt;li&gt;1단계에서 생겨난 자만심&lt;/li&gt;
      &lt;li&gt;원칙 없이 더 많은 욕심을 냄. 더 큰 규모, 더 높은 성장, 더 많은 찬사&lt;/li&gt;
      &lt;li&gt;자기통제와 규율 없이 더욱 위대해지기 힘들거나 빠르게 성장하기 어려운 영역으로 진입&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3단계: 위험과 위기 가능성을 부정하는 단계
    &lt;ul&gt;
      &lt;li&gt;리더들이 부정적인 데이터는 축소하고 긍정적인 데이터는 부풀리며 모호한 데이터는 긍정적으로 채색&lt;/li&gt;
      &lt;li&gt;사실에 근거한 활발한 대화는 사라짐&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;4단계: 구원을 찾아 헤매는 단계
    &lt;ul&gt;
      &lt;li&gt;누구에게나 기업의 가파른 하락세가 뚜렷이 보임&lt;/li&gt;
      &lt;li&gt;구원투수들은 비전과 카리스마가 있고 과감하지만 입증되지 않은 전략, 급격한 전환, 드라마틱한 문화적 변혁을 추구&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;5단걔: 유명무실해지거나 생명이 끝나는 단계
    &lt;ul&gt;
      &lt;li&gt;거듭된 차질과 실책으로 재무적 강점이 침식되기 시작해 리더들은 위대한 미래를 건설하려는 모든 희망을 버림&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;왜 이 어두운 여행을 해야하는가?
    &lt;ul&gt;
      &lt;li&gt;단지 성공만 연구하는 것은 결코 이롭지 않다. 기업이 지속적으로 좋은 성과를 올리려면 위대한 기업들이 어떻게 쓰러질 수 있는지 반드시 이해해야한다.&lt;/li&gt;
      &lt;li&gt;궁극적으로 희망을 창조해내는 작업. 내리막길을 향하고 있을 때 미리 브레이크 밟을 수 있음.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;1단계-성공으로부터-자만심이-생겨나는-단계&quot;&gt;1단계: 성공으로부터 자만심이 생겨나는 단계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;자만의 종류
    &lt;ul&gt;
      &lt;li&gt;최고가 될 수 없는 분야에 역량을 갖추지 못한 채 뛰어듬&lt;/li&gt;
      &lt;li&gt;자신이 탁월하게 일할 수 있는 수준 이상으로 성장을 추구함&lt;/li&gt;
      &lt;li&gt;모순적인 혹은 부정적인 증거를 뻔히 알면서도 과감하게 위험한 결정을 내림&lt;/li&gt;
      &lt;li&gt;위부 위협이나 내부 침식으로 위기에 몰릴 수 있는 가능성을 부정함&lt;/li&gt;
      &lt;li&gt;오만하게 기존 사업을 방치하는 것&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;기존 사업을 등한시하는 태도
    &lt;ul&gt;
      &lt;li&gt;성공적으로 플라이휠을 만든다&lt;/li&gt;
      &lt;li&gt;곧 닥칠 위협 때문에, 혹은 좀 더 흥미로운 기회를 발견했기 때문에 새로운 기회가 처음 만든 플라이휠보다 성공을 더욱 공고하게 우지해줄 것이라는 생각에 사로잡힌다&lt;/li&gt;
      &lt;li&gt;첫 번째 플라이휠에는 이전처럼 세심하게 신경 쓰지 않으면서 새로운 모험에는 마치 삶의 전부가 걸린 것처럼 창의성을 쏟아 붓는다&lt;/li&gt;
      &lt;li&gt;야심차게 시작한 새로운 사업이 명백히 실패로 판단됌. 성공한다해도 오래 걸림&lt;/li&gt;
      &lt;li&gt;첫 플라이휠로 다시 돌아와도 가속도를 이미 잃어버림.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;변함없는 탁월성을 유지하면서 놀라운 창의력을 발휘 –&amp;gt; 예술가&lt;/li&gt;
  &lt;li&gt;우리가 만난 최고의 경영자들은 호기심많은 과학자처럼 ‘왜’라는 질문을 끊임없이 던졌다&lt;/li&gt;
  &lt;li&gt;목표의식과 겸손한 탐구 정신 –&amp;gt; 월마트의 미션 “서민도 부자가 구입하는 것과 같은 물건을 구입하게 한다” 에 집중&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;2단계-원칙없이-더-많은-욕심을-내는-단계&quot;&gt;2단계: 원칙없이 더 많은 욕심을 내는 단계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;혁신은 성장을 가속화할 수도 있지만 혁신을 급하고 과하게 할 경우, 성장으로 인해 기업의 탁월한 기반이 침식됌.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;문제는 자만심에 젖어 야단스럽게 점점 더 많은 것을 약속했다는 데 있다. 너무 기대를 높여 놓았다는 것을 발견하는 날, 그날이 바로 몰락의 날이다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;위대한 기업의 창업자는 모두 단순히 돈을 버는 것 이상의 고상한 사업 목적 아래 기업을 일구었음
    &lt;ul&gt;
      &lt;li&gt;머크: 조지 머크 2세는 인간의 생명을 보전하고 개선하는 길을 모색&lt;/li&gt;
      &lt;li&gt;HP: 빌 휴렉과 데이비드 팩커드는 HP의 존재 이유가 기술적 공헌에 있다고 생각, 수익은 그러한 목적을 실현하다보면 자연스레 얻는 것&lt;/li&gt;
      &lt;li&gt;다시 창업자의 정신으로 돌아가야한다&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;규모가 큰 동시에 위대할 수는 있지만 위대하다고 모두 규모가 큰 것은 아니다&lt;/li&gt;
  &lt;li&gt;위대한 기업은 기회가 없어서가 아니라 기회가 너무 많아 소화불량으로 망할 수 있음
    &lt;ul&gt;
      &lt;li&gt;팩커드 법칙: 그 어떤 기업도 성장을 실현하고 나아가 위대한 회사를 만들어갈 적임자를 충분히 확보하는 능력 이상으로 계속해서 수익을 빠르게 늘려갈 수 없다.&lt;/li&gt;
      &lt;li&gt;우수한 기업의 기반은 무엇보다 자신을 통제할 수 있고 동기를 부여하는 인재&lt;/li&gt;
      &lt;li&gt;기업 문화의 핵심을 이러한 인재의 확충과 육성에 두어야 함&lt;/li&gt;
      &lt;li&gt;스스로 책임지는 적임자가 있을 경우 굳이 많은 규칙과 생각 없는 관료 제도를 내세울 필요가 없음&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;한 기업이 비범한 성과를 지속하는 것은 적임자를 핵심 요직에 얼마나 잘 배치하고 유지하느냐에 좌우됌&lt;/li&gt;
  &lt;li&gt;적임자와 적임이 아닌 자는 의식에서부터 확실히 구분됌&lt;/li&gt;
  &lt;li&gt;적임자는 ‘책임’을 맡았다고 여김&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;적임이 아닌 자는 ‘일’을 맡았다고 생각&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;우리가 연구한 훌륭한 리더들은 신기하게도 자신의 역할을 그리 중요하게 여기지 않았다. 그들은 기업의 핵심 가치를 바탕으로 경영진을 구성하고 영웅적인 리더 한 사람에게 의지하지 않는 문화를 만들어야 한다고 강조했다.&lt;/li&gt;
  &lt;li&gt;하지만 권력을 쥔 잘못된 리더 한 사람이 기업을 몰락으로 이끌 수 있다. 그만큼 적임자를 잘 뽑는 일이 중요하다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;3단계-위험과-위기-가능성을-부정하는-단계&quot;&gt;3단계: 위험과 위기 가능성을 부정하는 단계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;모호하고 상충되는 데이터 앞에서 위험한 도박을 하거나 결졍을 내려야 한다면 다음의 세 가지 질문 (챌린지호 예시)
    &lt;ul&gt;
      &lt;li&gt;만약 좋은 결과를 얻는다면 좋은 점은 무엇인가?&lt;/li&gt;
      &lt;li&gt;만약 나쁜 결과를 얻는다면 나쁜 점은 무엇인가?&lt;/li&gt;
      &lt;li&gt;부정적인 결과를 얻더라도 견뎌낼 수 있는가?&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;3단계에서 나타나는 특징: 기업이 곤란한 처지에 빠질 수 있는 현실에 직면했을 때, 실권자들이 정면으로 맞서기보다 다른 사람 혹은 외부 요인을 탓함&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;구조 조정과 리스트럭처링은 뭔가 생산적인 일을 하고 있다는 착각을 하게 만든다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;3단계의 징조
    &lt;ul&gt;
      &lt;li&gt;긍정적인 징조는 확대하고 부정적인 징조는 축소한다.&lt;/li&gt;
      &lt;li&gt;실증적인 증거 없이 과감한 목표를 세우고 크게 투자한다.&lt;/li&gt;
      &lt;li&gt;모호한 데이터를 기반으로 큰 위험을 초래할 수 있는 일을 단행한다.&lt;/li&gt;
      &lt;li&gt;건강한 팀 역동성이 침식된다.&lt;/li&gt;
      &lt;li&gt;비난을 다른 곳으로 돌린다.&lt;/li&gt;
      &lt;li&gt;구조조정에 몰두한다.&lt;/li&gt;
      &lt;li&gt;경영자들이 현실에서 격리된다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;4단계-구원을-찾아-헤매는-단계&quot;&gt;4단계: 구원을 찾아 헤매는 단계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;IBM을 부활시킨 거스너가 취임한 후 100일 지났을 무렵 홍보기사를 거절하며 “고맙지만 사양하겠소. 당면 과제를 분석하느라 힘겨운 날을 보냈을 뿐이오”&lt;/li&gt;
  &lt;li&gt;“지금 IBM에 필요한 것은 결코 비전이 아닙니다” : 인재의 적재적소 배치, 수익성 회복, 현금흐름 개선 그리고 모든 활동의 중심에 고객을 두는 등 기본적인 행동에 우선순위를 두어야한다&lt;/li&gt;
  &lt;li&gt;4단계는 하락세에 대처하기 위해 기업이 단번에 사태를 역전시킬 수 있는 묘안을 찾아 요동칠 때부터 시작된다.&lt;/li&gt;
  &lt;li&gt;TI의 경영진은 위대한 기업의 지위를 회복하려면 현명한 행동을 하나하나 수행해나가야 한다는 사실을 이해하고 있었다. 하룻밤 사잉에 크게 성공한 듯한 스토리도 알고 보면 20년 정도 가꾸어낸 결과다.&lt;/li&gt;
  &lt;li&gt;몰락으로 향하는 길을 되돌리고 싶다면 무슨 일을 하지 말아야 할지 곰곰이 생각해보아야 한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;5단계-유명무실해지거나-생명이-끝나는-단계&quot;&gt;5단계: 유명무실해지거나 생명이 끝나는 단계&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;흑자를 내더라도 파산할 수 있다.&lt;/li&gt;
  &lt;li&gt;기업은 수익 부족으로 죽지 않는다. 오히려 그 반대로 현금 부족으로 죽는다.&lt;/li&gt;
  &lt;li&gt;투쟁의 포인트는 단순히 생존하는 것이 아니라 세상에 공헌할 수 있는 기업을 세우고 훌륭하게 운영해나가는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;희망을-잃지-말아야하는-이유&quot;&gt;희망을 잃지 말아야하는 이유&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;제록스 CEO 멀케이 “직원들이 은퇴할 때까지 다니고 싶어 하는 직장, 자기 자녀들이 와서 일하도록 권할 수 있는 직장, 회사가 이룬 업적을 자랑스러워 할 수 있는 직장을 만드는 것이 내 바람입니다.”&lt;/li&gt;
  &lt;li&gt;올바른 리더는 속임수에 의존하지 않고 불타는 승강장에 서 있든 그렇지 않든 꾸준히 개선을 주도해 나간다.&lt;/li&gt;
  &lt;li&gt;어둠에서 벗어나는 길은 포기할 줄 모르는 끈질김과 함께 시작된다.&lt;/li&gt;
  &lt;li&gt;패배로 비틀거리는 것과 그래도 끈질기게 싸워나갈 수 있는 열망과 가치를 포기하지 않는 것은 완전히 별개의 문제다.&lt;/li&gt;
  &lt;li&gt;실패는 물리적 상태보다 정식적 상태와 더 관련 깊은 단어다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sun, 07 Oct 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/reading/2018/10/07/read_mighty/</link>
        <guid isPermaLink="true">http://localhost:4003/reading/2018/10/07/read_mighty/</guid>
        
        <category>paper</category>
        
        
        <category>reading</category>
        
      </item>
    
      <item>
        <title>구글의 아침은 자유가 시작된다</title>
        <description>&lt;ul&gt;
  &lt;li&gt;저자 : 라즐로 복&lt;/li&gt;
  &lt;li&gt;경력 : 예일대 경영학 석사, GE 인사 담당 부사장, 맥킨자 컨설턴트, 구글&lt;/li&gt;
  &lt;li&gt;내용 : 직원에게 자유를 줄 때 얼마나 큰 힘이 발휘되는지, 본능에 의존하지 않고 과학적인 방법론으로 의사결정&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;서문&quot;&gt;서문&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;어디를 가든 사람들은 자기가 하는 일과 관련해 정당한 처우를 받지 못했다&lt;/li&gt;
  &lt;li&gt;세상의 모든 기업이 직원을 대하는 방식에 영향을 미칠 수 있는 방안을 찾는 것&lt;/li&gt;
  &lt;li&gt;웨스먼스 CEO “가슴을 열고 이끌면 사업은 성공한다”&lt;/li&gt;
  &lt;li&gt;브랜딕스 최고인적자원책임자 “직원들이 경영자의 리더십을 신뢰할 때 회사 브랜드의 적극적인 홍보자가 되어 가족과 사회 그리고 주변 환경에서 적극적인 변화를 이끌어냅니다”&lt;/li&gt;
  &lt;li&gt;세상에서 가장 재능이 있는 사람들은 물리적으로 점점 더 쉽고 빠르게 직장을 옮길 수 있으며, 기술을 통해 점점 더 많이 연결되고 또 점점 더 찾아내기 어려워지고 있다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;구글에서 의사결정자가 일방적으로 결정할 수 없는 것들&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;누구를 고용할 것인가&lt;/li&gt;
  &lt;li&gt;누구를 해고할 것인가&lt;/li&gt;
  &lt;li&gt;직원의 성과를 어떻게 평가할 것인가&lt;/li&gt;
  &lt;li&gt;어떤 직원의 연봉인상과 상여금 혹은 스톡그랜트의 수준을 어떻게 결정할 것인가&lt;/li&gt;
  &lt;li&gt;누구를 팀 우수 관리자로 선정할 것인가&lt;/li&gt;
  &lt;li&gt;누구를 승진시킬 것인가&lt;/li&gt;
  &lt;li&gt;코드의 품질이 어느정도 완성이 되었을 때 이 코드를 소프트웨어에 탑재할 것인가&lt;/li&gt;
  &lt;li&gt;어떤 제품을 출시할 때 이 제품의 최종 디자인을 무엇으로 결정할 것인가&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;문제는 사람마다 최고의 직원을 다르게 규정할 수 있다는 점이다&lt;/li&gt;
  &lt;li&gt;관리자는 장애물을 제거하고 팀을 격려하는데 초점을 맞춘다&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;창업자가-된다는-것&quot;&gt;창업자가 된다는 것&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;창업자가 되고 또 자기가 속한 팀의 문화 창조자가 된다는 것은 모든 사람에게 영향력을 행사할 수 있는 위치에 선다는 뜻이다.&lt;/li&gt;
  &lt;li&gt;가장 먼저 할 일은 자기가 창업자가 되기를 원하는지 아니면 누군가의 직원이 되기를 원하는지를 선택하는 일이라는 것이다.&lt;/li&gt;
  &lt;li&gt;직원이 각자 가치있는 영향을 미치고 있으며 사회를 좀 더 낫게 만드는 데 기여하고 있다고 느끼도록 하는 것이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;문화는-아침으로-전략을-먹는다&quot;&gt;문화는 아침으로 전략을 먹는다&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;우리가 진실이라고 믿고 있는 열 가지&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;사용자에 집중하라. 그러면 나머지는 자연히 따라온다&lt;/li&gt;
  &lt;li&gt;한 가지를 아주 아주 잘하는게 최고다&lt;/li&gt;
  &lt;li&gt;빠른 것이 느린 것보다 낫다&lt;/li&gt;
  &lt;li&gt;민주주의는 웹상에서도 먹힌다&lt;/li&gt;
  &lt;li&gt;정답을 얻기 위해 꼭 컴퓨터를 켜야 하는 것은 아니다&lt;/li&gt;
  &lt;li&gt;악을 행하지 않고서도 돈을 벌 수 있다&lt;/li&gt;
  &lt;li&gt;더 많은 정보는 언제나 널려 있다&lt;/li&gt;
  &lt;li&gt;정보의 필요성은 국경을 초월한다&lt;/li&gt;
  &lt;li&gt;정장을 입지 않고서도 얼마든지 진중할 수 있다&lt;/li&gt;
  &lt;li&gt;아무리 좋다 해도 충분히 좋지는 않다&lt;/li&gt;
&lt;/ol&gt;

&lt;ul&gt;
  &lt;li&gt;재미는 구글의 주요한 요소이며, 무작정 시도해보는 탐험의 출발점이 되는 한편 중요한 발견의 기회를 열어준다.&lt;/li&gt;
  &lt;li&gt;사명과 투명성 그리고 목소리가 구글의 기업문화를 규정하는 세 가지 측면이다.&lt;/li&gt;
  &lt;li&gt;
    &lt;사명&gt;
&lt;/사명&gt;
    &lt;ul&gt;
      &lt;li&gt;사명에 대한 깊은 공감이 대부분의 구글 직원을 하나로 묶어준다.&lt;/li&gt;
      &lt;li&gt;세상에서 가장 재능이 넘치는 사람들은 야망을 자극하는 어떤 영감을 원한다.&lt;/li&gt;
      &lt;li&gt;관리자 혹은 리더가 해야할 일은 조직에 이런 목표를 만드는 것이다.&lt;/li&gt;
      &lt;li&gt;직원들로 하여금 자기가 돕는 사람을 직접 만나게 하는 것이야말로 가장 큰 동기부여 요소라는 사실이다.&lt;/li&gt;
      &lt;li&gt;자기가 하는 일이 이 세상에서 오직 자기만이 할 수 있는 가치 있는 일임을 깨닫는 것보다 더 강력한 동기부여는 없다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;사람들이 각자 자기가 하는 일을 바라보는 방식
    &lt;ol&gt;
      &lt;li&gt;일 : 자기 인생에서 그다지 중요하지 않지만 해야할 필요가 있는 어떤 것&lt;/li&gt;
      &lt;li&gt;경력 : 성취를 하거나 성장을 하기 위한 어떤 것&lt;/li&gt;
      &lt;li&gt;천직 : 사회에 도움이 되는 일을 함으로써 얻을 수 있는 기쁨과 충족감의 원천&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;투명&gt;
&lt;/투명&gt;
    &lt;ul&gt;
      &lt;li&gt;투명하게 공개함으로써 얻을 수 있는 이득은 회사의 모든 직원이 현재 모슨 일이 어떻게 진행되고 있는지 알게 된다는 것이다.&lt;/li&gt;
      &lt;li&gt;단지 자료를 공유하는 것만으로도 생산성이 향상된다는 점이다.&lt;/li&gt;
      &lt;li&gt;개방과 공개는 당신이 직원이 올바른 판단을 내린다고 믿으며 그들을 신뢰한다고 공표하는 일이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;구글의 문화가 구글의 전략을 형성하지, 그 반대가 아니다.&lt;/li&gt;
  &lt;li&gt;당신이 사람들에 대해 갖고 있는 생각은 무엇인가 그리고 당신에게는 자신의 생각이 가리키는 방향으로 사람들을 대하고 지휘할 용기가 있는가? 이 질문들에 대해 당신은 분명한 답을 갖고 있어야한다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;신입-직원은-모두-평균-이상이다&quot;&gt;신입 직원은 모두 평균 이상이다&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;인수 채용이 과연 기업을 올바르게 키워나가는 방법인지는 아직 명확하게 입증된 바 없다.&lt;/li&gt;
  &lt;li&gt;어떤 사람의 성과에 대해 이 사람을 고용하고자 하는 사람이 갖고 있는 유일한 증거는 그가 제출한 이력서와 그가 하는 말뿐이다.&lt;/li&gt;
  &lt;li&gt;각 기업에서 지원자를 면접하는 사람은 대부분 면접에 특별히 능하지 않다. 회사는 자기들이 당연히 최고의 인재를 뽑았다거 생각하지만 이런 생각은 옳지 않다.&lt;/li&gt;
  &lt;li&gt;면접 과정에서 이뤄지는 평가의 대부분은 처음 3-5분 사이에 혹은 그보다 더 일찍 이뤄진다는 자료도 많다&lt;/li&gt;
  &lt;li&gt;면접관은 무의식적으로 자기와 닮은 지원자에게 이끌린다&lt;/li&gt;
  &lt;li&gt;최고 수준의 기술자가 갖는 가치는 평균적인 기술자의 300배에 가깝다. 공대 졸업반의 기술자 전체를 포기하는 한이 있어도 단 한 명의 비범한 기술자를 선택하겠다.&lt;/li&gt;
  &lt;li&gt;자기보다 더 나은 사람을 채용하라&lt;/li&gt;
  &lt;li&gt;최고의 인재가 사람들이 일반적으로 상상하는 모습과 항상 같지는 않다는 점이다&lt;/li&gt;
  &lt;li&gt;고난을 극복할 능력과 끈기를 이미 보여준 후보자들을 물색하기 시작했다&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;어떤 사람이 대학에서 받은 교육보다는 이 사람이 그동안 이룩한 성취가 훨씬 중요하다&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;“똑똑하기만 한 사람을 채용하지 말라”&lt;/li&gt;
  &lt;li&gt;어떤 회사의 구체적인 맥락 속에서 성공을 거둘 사람, 주변의 모든 사람들까지 성공하더록 만들어줄 사람을 채용하라&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;최상의-결과를-찾기-위한-검색&quot;&gt;최상의 결과를 찾기 위한 검색&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;지금 당장 닥친 문제를 풀 수 있을 뿐 아니라 미래에 닥칠 미지의 문제들까지도 풀 수 있는 인재를 어디에서 찾을 것인가 하는 과제는 지금도 중요한 숙제로 남아 있다.&lt;/li&gt;
  &lt;li&gt;최고의 인재는 일자리를 찾지 않는다는 사실이었다. 유능한 사람은 지금 있는 것에서 충분히 많은 보상을 받고 있으며 현재 상태에 만족한다.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Fri, 28 Sep 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/reading/2018/09/28/read_google/</link>
        <guid isPermaLink="true">http://localhost:4003/reading/2018/09/28/read_google/</guid>
        
        <category>paper</category>
        
        
        <category>reading</category>
        
      </item>
    
      <item>
        <title>글쓰기 체력 기르기 12</title>
        <description>&lt;h2 id=&quot;글쓰기-체력-기르기-12&quot;&gt;글쓰기 체력 기르기 12&lt;/h2&gt;

&lt;p&gt;나는 내가 1년전, 2년전의 나보다 성장했다고 믿는다. 자신이 성장했는지 어떻게 판단할 수 있을까? ‘나는 과연 성장하고 있는가?’라고 수없이 자신에게 물어봤다. 그 때마다 성장에 대한 판단 기준이 계속 변했다. 지금 내가 생각하는 성장에 대한 근거는 다음과 같다.&lt;/p&gt;

&lt;p&gt;우선, ‘나 자신에 대해 얼마나 알고 있는지’를 생각해본다. 나 자신에 대해 잘 모를때는 A도 잘 할 수 있을 것 같고 B도 잘 할 수 있을 것이라고 생각한다. 물론 둘 다 잘할 가능성도 있지만 많은 경우에 A를 잘하면 B를 잘 못한다. 자신이 무엇을 잘하고 무엇을 못하는지는 생각만으로 알 수 없다. 직접 부딪혀보면서 자신의 한계를 깨닫게 되는데 그 깨닫는 과정이 성장의 지표인 것이다. 결국 자신이 무엇을 잘 하는지에 대해 아는 것은 경험을 통한 성장을 알려준다.&lt;/p&gt;

&lt;p&gt;두 번째는 ‘상대방에 대해 얼마나 있는 그대로 평가하는지’이다. 상대방에 대한 평가는 두 가지로 잘못될 수 있다. 상대방을 있는 그대로보다 더 높게 평가하는 것과 있는 그대로보다 더 낮게 평가하는 것이다. 둘 다 본인의 교만으로부터 오는 경우가 많다. 상대방을 더 높게 평가하는 경우 자신이 누가 뛰어난지를 잘 알고 있다는 교만일 수 있다. 상대방을 더 낮게 평가하는 경우는 자신이 그 사람보다 잘 났다는 생각하는 교만일 수 있다. 따라서 나 자신도 교만에 빠지지 않기 위해 예전보다는 상대방을 쉽게 판단하지 않는다.&lt;/p&gt;

&lt;p&gt;마지막으로 순간 순간에 너무 좌지우지되지 않고 항상 다음을 생각하고 행동하는지이다. 세상의 많은 일들은 당장에 그 가치를 알 수 없는 경우가 많다. 지금 잘 되고 있는 것 같이 보이는 일들도 나중에보면 가치가 생각보다 떨어질 수 있다. 따라서 정말로 가치있는 일을 하려면 꾸준히 개선해나가야 하는데 그게 정말로 쉽지 않다. 그 쉽지 않은 것을 계속해서 견뎌내는 것이 성장했기 때문에 가능하다고 생각한다. 그래서 개인적으로는 어떠한 일이 마무리 됐을 때 ‘그래서 다음에 나는 무엇을 해야할까?’를 많이 생각한다.&lt;/p&gt;

&lt;p&gt;각자 자신의 전문 분야에서 일을 계속 하다보면 ‘스킬’은 자연스럽게 늘 수 있다. 하지만 위에서 이야기한 ‘나 자신에 대해 알고’ ‘상대방을 있는 그대로 보며’ ‘다음에 해야할 행동을 생각하는 것’은 의식적으로 노력하지 않으면 절대 개선되지 않는다. 따라서 이러한 점을 토대로 나는 자신이 성장했는지를 판단해본다.&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Sep 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/writing/2018/09/27/writing-practice-12/</link>
        <guid isPermaLink="true">http://localhost:4003/writing/2018/09/27/writing-practice-12/</guid>
        
        <category>paper</category>
        
        
        <category>writing</category>
        
      </item>
    
      <item>
        <title>글쓰기 체력 기르기 11</title>
        <description>&lt;h2 id=&quot;글쓰기-체력-기르기-11&quot;&gt;글쓰기 체력 기르기 11&lt;/h2&gt;

&lt;p&gt;우리는 자신이 처한 상황이나 환경에 대해 “이해하고 있다”라고 생각하는 경우가 많다. 하지만 많은 경우에 우리가 지각하는 이유가 아니라 다른 이유로 현재 상황이 벌어진다.&lt;/p&gt;

&lt;p&gt;“마스터 알고리즘”에서 소개하는 일화가 가장 인상적이다. 사진을 보고 탱크인지 아닌지를 알아맞추는 컴퓨터 모델을 만드는 일이었다. 탱크가 있는 사진과 없는 사진을 통해 모델을 학습했고 학습이 너무나 잘 이뤄졌다. 하지만 새로운 사진에 대해서는 전혀 잘 예측하지 못했다. 실제로 벌어진 상황은 다음과 같다. 탱크가 있는 사진이 대부분 흐릿흐릿했던 것이다. 컴퓨터 모델은 탱크를 인식하도록 학습한 것이 아니라 그저 흐릿한 사진을 인식하도록 학습이 된 것이다.&lt;/p&gt;

&lt;p&gt;머신러닝과 딥러닝, 강화학습을 하는 사람들은 매일 같이 실험을 하고 결과를 본다. 결과가 잘 나온 경우와 잘 나오지 않은 경우 그 이면에 내가 알지 못하는 일들이 있다고 생각하는 습관을 기르는 것이 중요하다. 쉽게 말하자면 결과 분석을 꼭 해야한다라고 할 수도 있다. 예를 들면 이런 식으로 항상 생각한다는 것이다. “학습 결과가 내 예상보다 잘 나왔어. 하지만 나는 이 결과가 왜 이렇게 나왔는지 제대로 이해하지 못했어. 내가 놓치고 있는 점이 무엇일까? 이 결과는 나에게 어떤 이야기를 해주는 걸까?”&lt;/p&gt;

&lt;p&gt;실험을 하고 결과를 볼 때까지 이미 에너지를 많이 소비했기 때문에 결과가 나오면 생각의 끈을 놓고 그냥 쉬고 싶어진다. 하지만 결국 차이는 결과가 나온 이후에 생각을 더 하느냐 마느냐에 달려있다. 실험 결과에 대해 생각을 해보는 것이 더 남아서 야근을 한다는 말은 아니다. 말 그대로 생각의 끈을 놓지 않는다는 것이다. 하지만 일상을 살아가다보면 일 이외에도 수많은 것들을 생각해야한다. 따라서 단순히 생각의 끈을 놓치 않는 것은 상당히 어렵다. 더군다가 여러가지 생각이 머리속에서 뒤섞이기 시작하면 오히려 생각해왔던 것들을 놓칠 것이다.&lt;/p&gt;

&lt;p&gt;따라서 기록이 중요한 것이다. 더 나은 연구자가 되기 위해 결과를 분석해야한다. 쉽게 끝나는 일이 아니며 계속 고민을 해야하는 일이기 때문에 시간이 오래걸릴 수 있다. 하지만 개인은 일 이외에도 가족과 시간을 보낸다던가하는 중요한 일들이 있기 때문에 생각을 분리할 수 있어야한다. 따라서 기록을 통해 잊어버리지 않으면서도 생각을 적절히 끊어주는 것이다. 다음 날 출근해사 끊어진 생각을 기록으로 보완해서 계속하는 것이다. 기록은 자신의 한계를 인지하면서부터 그 빛을 발한다.&lt;/p&gt;

&lt;p&gt;이 때 중요한 것은 기록을 하는 원칙이다. 어떤 상황에 어떤 식으로 기록을 해야 하는지를 학습해야한다. 어떻게 기록해야 나의 생각을 효과적으로 복원할지에 대해 알아내야한다. 앞으로 내가 발전시켜야할 점이다.&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Sep 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/writing/2018/09/27/writing-practice-11/</link>
        <guid isPermaLink="true">http://localhost:4003/writing/2018/09/27/writing-practice-11/</guid>
        
        <category>paper</category>
        
        
        <category>writing</category>
        
      </item>
    
      <item>
        <title>글쓰기 체력 기르기 10</title>
        <description>&lt;h2 id=&quot;글쓰기-체력-기르기-10&quot;&gt;글쓰기 체력 기르기 10&lt;/h2&gt;

&lt;p&gt;글쓰기는 생각의 근력을 기르는 것이라는 말을 듣고 잊고 있던 짧은 글쓰기를 다시 시작해야겠다고 생각했다.&lt;/p&gt;

&lt;p&gt;글쓰기를 하지 않으면 점점 생각이 맥락을 잃는다. 많은 현대인은 정말 바쁘게 살아간다. 우리에게는 항상 “시간”이 없다. 책을 읽을 시간이 없고 자기 자신을 되돌아볼 시간이 없으며 무엇이 옳은지에 대해 생각할 시간이 없다. “바쁘니까”라고 변명을 하기에는 잃는 것이 많다. 자신의 생각을 가다듬는 방법이 여러가지가 있다. 존경하는 멘토와 이야기를 할 수도 있으며 사랑하는 사람과 생각을 공유할 수도 있다. 하지만 개인적으로 그 중에 글쓰기가 가장 좋은 방법이라고 생각한다.&lt;/p&gt;

&lt;p&gt;왜 글쓰기가 생각을 다듬는데 가장 좋은 방법일까? 지금 바로 당신이 최근에 생각하던 것들을 글로 쓰려고 해보면 그 답을 알 것이다. 보통 자신의 생각을 글로 쓰는 것은 쉽지 않다. 명확하다고 생각했던 의견이나 주장들도 글로 적다보면 허점 투성이다. 생각을 글로서 다듬지 않는 것이 당장은 명확한 차이를 보이지 않을 수도 있다. 하지만 시간이 지나고 보면 말과 행동에서 분명한 차이를 보인다.&lt;/p&gt;

&lt;p&gt;글쓰기는 생각을 다듬는 것 말고도 생각의 범위를 넓혀준다는 장점이 있다. 사람은 편한 것을 추구한다. 몸이 편한 것만 추구하는 것이 아니라 생각 또한 편하게 하려한다. 따라서 항상 생각하던 주제를 잘 벗어나려 하지 않는다. 새로운 것을 생각하는 것은 스트레스를 주기 때문이다. 하지만 글을 쓰려고 하면 항상 같은 이야기를 쓸 수 없기 때문에 새로운 생각에 대한 동기가 생긴다. 글을 쓰기 위해 새로운 책을 읽기도 하고 강연을 듣기도 한다. 다른 사람의 글을 보면서 새로운 관점을 배우기도 한다. 글쓰기는 우리에게 “배움”의 길을 열어준다.&lt;/p&gt;

&lt;p&gt;주변 사람 중에 발표를 하거나 말을 할 때 조리있고 잘 짜여진 구조롤 말을 하는 사람이 있는가? 한 번 그 사람이 쓰는 글을 본다면 그 이유를 알 것이다.&lt;/p&gt;
</description>
        <pubDate>Thu, 27 Sep 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/writing/2018/09/27/writing-practice-10/</link>
        <guid isPermaLink="true">http://localhost:4003/writing/2018/09/27/writing-practice-10/</guid>
        
        <category>paper</category>
        
        
        <category>writing</category>
        
      </item>
    
      <item>
        <title>Neural Kinematic Networks for Unsupervised Motion Retargetting</title>
        <description>&lt;h2 id=&quot;논문-제목-neural-kinematic-networks-for-unsupervised-motion-retargetting-2018-march&quot;&gt;논문 제목: Neural Kinematic Networks for Unsupervised Motion Retargetting [2018 March]&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/qnjkud2nqgjn0qp/Screenshot%202018-07-29%2013.02.13.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Ruben Villegas, Jimei Yang, Duygu Ceylan, Honglak Lee&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;http://openaccess.thecvf.com/content_cvpr_2018/papers/Villegas_Neural_Kinematic_Networks_CVPR_2018_paper.pdf&quot;&gt;http://openaccess.thecvf.com/content_cvpr_2018/papers/Villegas_Neural_Kinematic_Networks_CVPR_2018_paper.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;프로젝트 링크: &lt;a href=&quot;https://sites.google.com/umich.edu/nik&quot;&gt;https://sites.google.com/umich.edu/nik&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;유투브 링크: &lt;a href=&quot;https://youtu.be/BGMyCFmGJWQ&quot;&gt;https://youtu.be/BGMyCFmGJWQ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;코드 링크: &lt;a href=&quot;https://github.com/rubenvillegas/cvpr2018nkn&quot;&gt;https://github.com/rubenvillegas/cvpr2018nkn&lt;/a&gt;
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-overview&quot;&gt;1. Overview&lt;/h2&gt;

&lt;h2 id=&quot;2-forward-kinematics--inverse-kinematics&quot;&gt;2. Forward Kinematics &amp;amp; Inverse Kinematics&lt;/h2&gt;

&lt;h2 id=&quot;3-neural-kinematics-networks&quot;&gt;3. Neural Kinematics Networks&lt;/h2&gt;

&lt;h2 id=&quot;4-code-review&quot;&gt;4. Code review&lt;/h2&gt;
</description>
        <pubDate>Sun, 29 Jul 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/07/29/nkn/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/07/29/nkn/</guid>
        
        <category>dl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>Playing hard exploration games by watching YouTube</title>
        <description>&lt;h2 id=&quot;논문-제목-playing-hard-exploration-games-by-watching-youtube-2018-march&quot;&gt;논문 제목: Playing hard exploration games by watching YouTube [2018 March]&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/p8r1y3xxeurh8ko/Screenshot%202018-07-19%2022.33.07.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Yusuf Aytar, Tobias Pfaff, David Budden, Tom Le Paine, Ziyu Wang, Nando de Freitas (DeepMind)&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://arxiv.org/pdf/1805.11592.pdf&quot;&gt;https://arxiv.org/pdf/1805.11592.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;유투브 링크: &lt;a href=&quot;https://youtu.be/Msy82sIfprI&quot;&gt;https://youtu.be/Msy82sIfprI&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;1-abstract&quot;&gt;1. Abstract&lt;/h2&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;이 논문은 sparse reward인 문제 상황을 지적. 이런 문제 상황에서 exploration을 잘하는 것이 중요함&lt;/li&gt;
  &lt;li&gt;이전에는 사람이 reference가 될 수 있는 demonstation을 만들고 에이전트가 모방하도록 하는 방법이 있었음&lt;/li&gt;
  &lt;li&gt;하지만 demonstration을 얻는 것은 여러가지 이슈로 쉽지 않음&lt;/li&gt;
  &lt;li&gt;이 논문에서는 noisy하고 unaligned되어있는 video로부터 학습하는 방법을 제시함. 이 방법은 2 stage로 나뉨.&lt;/li&gt;
  &lt;li&gt;우선 self-supervised learning을 통해 여러개의 유투브 영상에서 common representation을 학습&lt;/li&gt;
  &lt;li&gt;그 다음 하나의 유투브 영상을 embed해서 reward를 만들고 그걸로 agent를 학습시킴&lt;/li&gt;
  &lt;li&gt;이렇게 학습하니 human-level performance 이상을 낼 수 있었음&lt;/li&gt;
  &lt;li&gt;테스트는 Montezuma’s revenge외 2개의 아타리 게임에 대해서 함.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 논문은 유투브 영상으로부터 그 유명한 몬테주마 리벤지 게임을 학습하는 논문이다. 준비물은 몬테주마 리벤지를 플레이하는 유투브 영상 몇 개이다. 이 논문은 크게 다음 세 가지를 알면 된다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;어떻게 유투브 영상에서 representation을 학습해내는지&lt;/li&gt;
  &lt;li&gt;학습한 representation을 가지고 어떻게 reward를 만들어내는지&lt;/li&gt;
  &lt;li&gt;어떻게 학습한 embedding을 평가하는지&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-experiment-result&quot;&gt;2. Experiment Result&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;이 논문에서 제시하는 방법을 썼을 경우 사람보다 더 잘 플레이할 수 있다. 아래 표가 그 결과를 보여준다. 기존 Rainbow 등과 차이가 많이 난다.
&lt;img src=&quot;https://www.dropbox.com/s/ngelr1n3ep8hpqh/Screenshot%202018-07-26%2022.57.22.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음 그림은 학습 곡선이다. 기존 강화학습만을 사용한 알고리즘은 거의 학습이 안되는 반면 논문에서 제시하는 방법은 학습이 잘 되는 것을 볼 수 있다. 
&lt;img src=&quot;https://www.dropbox.com/s/8m70cbbi6m03xyl/Screenshot%202018-07-26%2022.58.58.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이러한 결과를 봤을 때 기존 강화학습과는 확실한 차이가 있음을 알 수 있다. 사실 몬테주마 리벤지를 푸는 방법은 여러가지가 있어서 이 논문이 이 문제를 ‘처음’ 푼 것은 아니다. 하지만 유투브 영상 몇 개만으로 사람의 실력을 넘어서는건 상당히 의미가 있다고 본다. 유투브로부터 에이전트가 학습하는 법을 배울 수 있을 경우 앞으로 무엇이 가능할지 상상해보라.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-introduction&quot;&gt;3. Introduction&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;사람은 비디오를 시청함으로서 많은 것들은 배운다. 온라인 demonstration으로부터 knowledge를 transfer 하는데 뛰어나다.&lt;/li&gt;
  &lt;li&gt;사람이 이렇게 학습할 수 있다는 것은 AI 연구의 대상이 되어왔다.&lt;/li&gt;
  &lt;li&gt;이 논문에서는 아직 풀리지 않은 Atari 2600 게임에서 사람보다 플레이를 잘하는 에이전트를 소개한다. 이 에이전트는 noisy demonstration seqeunce로부터 self-supervised alignment를 학습한다.&lt;/li&gt;
  &lt;li&gt;deep rl이 최근 많은 발전을 이루었지만 sparse reward 문제에서 exploration 하는 것은 여전히 challenge하다.&lt;/li&gt;
  &lt;li&gt;몬테주마 리벤지 게임은 point를 주는 아이템을 방에서 방을 오가며 수집해야하는 게임이다. 따라서 이런 게임에서는 epsilon greedy 방식의 exploration은 안먹힌다.
    &lt;ul&gt;
      &lt;li&gt;예를 들어 몬테주마 리벤지 게임에서 첫번째 보상을 받으려면 환경에서 100 스텝을 진행해야한다. 행동의 개수가 18개라면 이 때 가능한 action sequence의 개수는 100^18이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;sparse reward 문제를 풀기 위한 접근은 크게 두 개로 나뉜다.
    &lt;ul&gt;
      &lt;li&gt;첫번째는 intrinsic motivation 이다. 환경에서 주는 보상이 아닌 별도의 보상을 통해 에이전트가 탐험을 하도록 유도하는 것이다. 특정 상태나 action trajectory가 어떠한 기준에 따라 novel 혹은 informative한지 측정하는 것이다. 이건 방문했던 state space 중에서 흥미로운 부분을 re-explore 하는 것이라서 처음 만나는 환경에서 exploration을 guide하기는 힘들다. 따라서 초반에는 random하게 exploration한다.&lt;/li&gt;
      &lt;li&gt;두번째는 imitation learning이다. human demonstrator의 플레이 데이터를 활용하는 것이다. 이 데이터를 활용해서 학습하는 것은 상당히 유용하다. 사람은 게임을 플레이할 때 skull은 위험하다는 것을 알고 key가 문을 연다는 것을 아는데 이런 사실이 이미 사람이 inductive bias를 가지로 있다는 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DQfD가 현재까지 hard exploration 아타리 게임에서 가장 높은 점수를 기록했다. DQfD에는 두 가지 한계가 존재한다.
    &lt;ul&gt;
      &lt;li&gt;첫번째. agent와 demonstrator 사이에 domain gap이 없다고 가정한다. 하지만 다음 그림과 같이 게임 플레이 영상과 유투브 영상들은 서로 다 색깔 등에서 다르다. 즉 domain gap은 존재하는 경우가 많다. 
&lt;img src=&quot;https://www.dropbox.com/s/msiy6k1ggaceuoj/Screenshot%202018-11-02%2021.25.49.png?dl=1&quot; /&gt;&lt;/li&gt;
      &lt;li&gt;두번째. agent는 demonstrator이 플레이하면서 취했던 정확한 행동과 받았던 보상들을 다 알아야한다. 이런 한계들은 강화학습을 위해 구성된 인공적인 환경에서는 문제가 되지 않는다. 하지만 우리는 이 한계를 뛰어넘는 방법을 소개할 것이다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;여러개의 demonstration 사이에 존재하는 domain gap을 극복하는 방법을 소개할 것이다. 여기에 self-supervised classification task가 사용될 것이다.&lt;/li&gt;
  &lt;li&gt;이 task를 통해 common representation을 학습할 수 있다.&lt;/li&gt;
  &lt;li&gt;우리가 제시하는 방법은 frame 단위로 align이 안맞아도 된다.&lt;/li&gt;
  &lt;li&gt;class label 같은 annotation도 필요없다.&lt;/li&gt;
  &lt;li&gt;cycle consistency를 통해 학습한 embedding을 평가하는 방법도 제시한다.&lt;/li&gt;
  &lt;li&gt;embedding을 통해 auxiliary imitation loss를 에이전트에게 준다. 이 loss를 통해 에이전트는 demonstrator에 대한 사전지식없이 hard exploration 문제를 해결할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-related-work&quot;&gt;4. Related Work&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;domain gap 문제를 해결하기 위해 지금까지 여러가지 시도가 있었다.
    &lt;ul&gt;
      &lt;li&gt;frame-by-frame alignment가 맞다면 CCA나 DCTW나 TCN 같은 방법을 통해 common representation을 학습할 수 있다.&lt;/li&gt;
      &lt;li&gt;유투브 영상같은 경우는 alignment가 안맞는다.&lt;/li&gt;
      &lt;li&gt;이런 경우에는 여러 도메인에서의 shared auxiliary objective 를 푸는 방법이있다. 어떤 논문에서는 여러 도메인에서 같은 scene classification task를 같은 모델로 푼다면 alignment 문제를 해결할 수 있다는 것을 소개했다. 하지만 우리는 여기서와 달리 category guided supervision을 사용하지 않았다. 우리는 labeled data없이 이 문제를 풀었다. 즉 self-supervision인것이다.&lt;/li&gt;
      &lt;li&gt;single-view TCN이라는 논문도 있다. 이것 또한 paired training data가 필요없는 self-supervised task이다. 우리는 이 논문과 triplet-based ranking 대신 temporal classification을 사용한다는 점에서 다르다. 이 점 때문에 하이퍼 파라메터에 민감하지 않다.&lt;/li&gt;
      &lt;li&gt;L3-Net이라는 것도 있다. 여기서는 vision과 sound modality의 alignment를 학습한다. 하지만 우리는 multiple audio-visual sequence를 align하는 것을 배운다. multimodal alignment를 self-supervised objective로 삼는다. (뭐가 다른건지..)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;우리는 TCN과 L3-net을 둘 다 활용했다.&lt;/li&gt;
  &lt;li&gt;우리는 demonstrator의 행동을 설명해주는 보상함수를 학습한다고 할 수도 있다. 이렇게 보면 inverse-rl과도 비슷하다. 하지만 우리는 demonstrator의 action과 reward를 몰라도 학습하므로 좀 더 복잡한 설정이라고 볼 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-closing-the-domain-gap&quot;&gt;5. Closing the domain gap&lt;/h2&gt;
&lt;hr /&gt;

&lt;p&gt;이 논문이 하고 싶은 것은 다음 그림과 같다. 왼쪽이 실제 에이전트가 있는 게임화면이고 나머지가 유투브 영상이다. 이 유투브 영상들을 통해 에이전트가 게임 플레이하는 법을 학습할 것이다. 하지만 문제는 유투브 영상끼리 frame이 맞지 않고 align도 안 맞다는 점이다. 또한 밑에 그림에서 볼 수 있듯이 유투브 영상끼리도 색상이 다르다. 
&lt;img src=&quot;https://www.dropbox.com/s/rr3wrmhuuijnfmc/Screenshot%202018-07-26%2023.08.06.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서 이 논문에서는 이 영상들로부터 바로 에이전트를 학습시키는게 아니라 단계를 나눈다. 일단 영상들로부터 어떠한 공통적인 representation을 학습한다. 이 때 사용하는 방법을 TDC, CMC라고 부른다. TDC는 Temporal distance classification으로 영상에서 frame 끼리 얼마나 시간적 차이가 있는지 예측하는 것을 통해 representation을 학습하는 방법이다. CMC는 Cross-modal temporal distance classification으로 유투브 영상에 포함되어있는 소리 정보를 통해 TDC와 비슷하게 소리끼리의 시간 차이를 예측하는 것을 통해 representation을 학습하는 방법이다. representation을 학습하면 그 정보를 가지고 imitation을 위한 reward를 생성한다. 이제 어떻게 representation을 학습하는지 살펴보자.&lt;/p&gt;

&lt;h3 id=&quot;51-tdc&quot;&gt;5.1 TDC&lt;/h3&gt;
&lt;p&gt;다음 그림으로 살펴보는 것이 이해하기 가장 좋다. 
&lt;img src=&quot;https://www.dropbox.com/s/117n8s3ruva0y40/Screenshot%202018-07-26%2023.23.09.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위 그림에서 초록색 부분이 TDC이다. video가 있으면 그 video는 여러 frame으로 구성되어있을 것이다. 그 frame들 사이의 시간적 차이를 prediction 하는 unsupervised task로 학습하는 것이다. 이 과정에서 모델은 의미있는 representation을 학습하게 된다.&lt;/p&gt;

&lt;p&gt;시간은 discrete하게 몇 개의 구간으로 나누는데 논문에서는 6개의 구간으로 나누었다. 
&lt;img src=&quot;https://www.dropbox.com/s/w5oezcrlelh5u64/Screenshot%202018-07-26%2023.25.51.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;즉, 두 개의 image가 주어지면 모델은 그 두 개를 입력받아서 6개의 category 중 하나로 classify하는 것이다. v와 w는 서로 다른 두 frame을 의미한다. v와 w가 각각 phi라는 일반적인 CNN 모델을 통해 feature(embedding)으로 변환이 되면 이 feature 들 간에 element-wise multiplication을 한다. 그리고 MLP를 통과해서 6개의 output 노드를 통해 각 category일 확률을 output으로 내보낸다. 결과를 가지고 다음과 같이 cross entropy loss를 만든다. y는 label이고 y^은 prediction이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/cxtisa50sav4scu/Screenshot%202018-07-26%2023.30.54.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;visual embedding 모델과 classifier 모델의 세부사항은 다음과 같다. 이미지 인풋은 random cropping을 하며 4장의 이미지를 stack해서 하나의 input을 만든다. embedding vector는 l2-normalize한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/ld7cgjvysk2lc53/Screenshot%202018-07-26%2023.32.37.png?dl=1&quot; /&gt;
&lt;img src=&quot;https://www.dropbox.com/s/68wk22d2ur9odbj/Screenshot%202018-07-26%2023.33.21.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;52-cmc&quot;&gt;5.2 CMC&lt;/h3&gt;
&lt;p&gt;유투브 영상에서 소리는 의미있는 정보를 포함하고 있다. 특히 아타리 게임과 같은 게임의 경우 소리는 어떤 event의 발생을 의미하는 경우가 많다. 따라서 video frame 뿐만 아니라 소리에 대해서도 temporal distance classification을 수행한다. 다시 아래 그림을 살펴보자. 소리에 대해서 classification 할 때는 소리끼리 하는 것은 아니고 frame 하나 소리 하나가 주어지고 두 개 사이의 관계를 알아내는 것이다. 이 경우 category가 2개라서 모델은 소리가 해당 frame과 같이 일어나는 것인지 아닌지만 classify하면 된다. 이 때, 소리를 embedding하기 위해 additional embedding function이 필요하다.
&lt;img src=&quot;https://www.dropbox.com/s/117n8s3ruva0y40/Screenshot%202018-07-26%2023.23.09.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CMC의 모델에 대한 상세한 정보는 다음과 같다. stft + conv1d 라고 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/6pf66nz9d0b3hhb/Screenshot%202018-07-26%2023.57.12.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;학습할 때는 TDC loss와 CMC loss를 weight를 곱해서 더해서 하나의 loss로 만들어서 학습한다. 
&lt;img src=&quot;https://www.dropbox.com/s/x2z7pv3frvgjps0/Screenshot%202018-11-02%2022.13.34.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;53-cycle-consistency&quot;&gt;5.3 Cycle-consistency&lt;/h3&gt;
&lt;p&gt;이렇게 학습한 embedding이 얼마나 좋은 것인지를 판단하는 것이 어렵다. 이 논문에서는 CycleGAN에서 사용되었던 ‘cycle relation’ 개념을 활용한다. 만약 두 개의 sequence V = {v1, v2, v3, …}와 W = {w1, w2, w3, …}가 있다고 하면 이 두 개의 sequence에 대해 다 embedding을 구할 수 있다. 이 때, 각 embedding에 대해 Euclidean distance를 구할 수 있다. 
&lt;img src=&quot;https://www.dropbox.com/s/tqt3pqerfm2rl3n/Screenshot%202018-07-26%2023.42.36.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 때 특정 vi에 대해서 W 중에서 embedding 상에서 가장 거리가 짧은 것을 찾았더니 wj였다면 그 반대도 성립하는 것을 cycle consistency라고 한다. cycle consistency를 만족할 확률을 one-to-one alignment capacity라고 하며 이것을 통해 embedding이 잘 학습되었는지를 파악할 수 있다. 여러가지 embedding 모델들의 cycle consistency를 비교한 것은 다음과 같다. 역시 논문에서 제시한 방법이 one-to-one alignment capacity가 가장 높다. 3-cycle-consistency 또한 비교를 했는데 TDC+CMC가 제일 높다.
&lt;img src=&quot;https://www.dropbox.com/s/ksu4ekly1qxtt8c/Screenshot%202018-07-26%2023.45.42.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;또한 embedding space를 시각화해볼 수 있는데 논문에서는 t-SNE를 가지고 시각화를 하였다. 색깔이 다른 것이 서로 다른 유투브 영상이라는 뜻이다. pixel만 가지고 embedding을 할 경우 완전 실패하고 TDC만 사용할 경우에 여러가지 영상끼리 align이 잘 안 맞는다. TDC에다 CMC까지 사용할 경우 아래 그림의 맨 오른쪽 처럼 서로 다른 색깔이 비슷하게 가는 것을 볼 수 있다. 맨 위에 첨부한 유투브 영상에 이와 관련된 부분이 있으니 찾아보길 바란다. 
&lt;img src=&quot;https://www.dropbox.com/s/g1l52e890mlbkw4/Screenshot%202018-07-26%2023.47.43.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-one-shot-imitation-from-youtube-footage&quot;&gt;6. One-shot imitation from Youtube footage&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;이렇게 representation을 학습했다면 이제 이 representation을 통해 agent를 학습시키는 단계이다. 에이전트를 학습시킬 때는 유투브 영상 하나만 사용한다. 유투브 영상을 frame 마다 embedding을 하면 trajectory가 생긴다. 이 trajectory에서 16 frame 마다를 checkpoint로 정한다. 그 checkpoint 마다 다음 수식을 통해 reward를 정의한다.
&lt;img src=&quot;https://www.dropbox.com/s/cb140iezg3hw765/Screenshot%202018-07-26%2023.53.59.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;학습에 사용되는 agent는 IMPALA를 사용했다. 여기서 약간 허탈하지만 (actor가 100개나 된다…) 충분히 의미는 있다고 생각한다. 학습하고 나서 embedding model의 마지막 convolution layer를 한 번 visualize 해봤다. 각 뉴런이 상당히 의미있는 부분들을 포착하는 것을 볼 수 있고 CMC가 좀 더 key item에 집중하는 것을 볼 수 있다. 이것을 통해 embedding이 잘 학습된 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/jufq5uj53snqebh/Screenshot%202018-07-26%2023.59.22.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;7-implementation-details&quot;&gt;7. Implementation Details&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;temporal classification과 cross-modal classification에 둘 다 쓰이는 shallow network는 2-layer relu-activated MLP이다. 
&lt;img src=&quot;https://www.dropbox.com/s/z27c5fq7oflyqbw/Screenshot%202018-11-02%2022.26.24.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;학습 데이터는 세 개의 유투브 동영상으로부터 추출했다. 우선 동영상으로부터 demonstration sequence를 추출하고 interval과 distance(시간차)를 함께 구한다. 구성한 데이터셋으로부터 데이터를 랜덤하게 추출한다. 모델은 Adam으로 lr=10-4d로 학습했다. batch size는 32이고 200,000 번 업데이트했다.&lt;/p&gt;

&lt;p&gt;imitation loss를 만들기 위해 N=16마다 checkpoint를 만들었다. checkpoint는 하나의 youtube 영상의 embedding의 sequence 상에서 정의된다. 우리는 agent를 imitation reward와 환경의 reward를 합해서 학습시켰다. IMPALA를 사용해서 학습했는데 이 때, 논문과 다른 점이 하나 있다. 우리는 모델의 마지막 conv output에 현재 agent와 남은 두 개의 checkpoint 까지의 거리를 concat했다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;8-analysis-and-experiments&quot;&gt;8. Analysis and Experiments&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;학습용으로 유투브 영상 3개를 사용했고 테스트용으로 1개의 유투브 영상을 사용했다. 각 유투브 영상은 3에서 10분 정도의 길이다. 중요한 것은 유투브 영상을 모을때 ALE 사용한 영상을 모으지 않았다는 것이다. preprocessing도 최소한으로 했다.&lt;/p&gt;

&lt;p&gt;유투브 영상사이의 또는 ALE 환경과의 domain 차이를 극복하기 위해 두가지가 필요하다.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;one-to-one alignment&lt;/li&gt;
  &lt;li&gt;meaningful abstraction&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;첫번째로 one-to-one alignment는 cycle-consistency를 통해서 확인할 수 있다. cycle consistency를 계산하기 전에 모델의 앞 layer에서 centered되고 l2-normalize된다. 2-way cycle consistency를 계산할 때는 training video 하나와 test video 하나를 비교했다.&lt;/p&gt;

&lt;p&gt;meaningful abstraction을 확인하기 위해서 game의 high-level information을 확인했다. 다음 그림을 통해 확인할 수 있다. 재밌는 결과를 확인할 수 있다. 왼쪽은 마지막 convolution layer에서 각 뉴런의 activation을 가시화한 것 이다. 어떤 neuron은 agent를, 어떤 놈은 적을, 어떤 놈은 inventory를 중요하게 보고 있는 것을 알 수 있다. 오른쪽은 마지막 layer의 모든 channel은 더한 결과이다. CMC가 있으면 key를 더 중요하게 보는 것을 알 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/hmhr7ef4u0jvfpf/Screenshot%202018-11-02%2022.42.01.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;다음은 학습곡선이다. 
&lt;img src=&quot;https://www.dropbox.com/s/9cl14u0x77cfsj6/Screenshot%202018-11-02%2022.46.43.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;8-개인생각&quot;&gt;8. 개인생각&lt;/h2&gt;
&lt;p&gt;결국 몬테주마 리벤지 게임을 학습하기 위한 새로운 reward를 설계하는데 사람이 하지 않고 유투브 영상으로부터 배운 representation으로 하겠다는게 논문의 핵심이다. 하지만 이 논문에서와 다르게 영상에서 소리가 의미가 없는 경우도 많다. 또한 이 경우는 시간과 색상의 alignment가 영상과 실제 환경이 다른 것이지 화면 자체는 동일하다고 볼 수 있다. 하지만 많은 경우 에이전트가 유투브 영상으로부터 배울 때 이러한 조건들이 만족하지 않을 수 있다. 그럴 경우 어떻게 유용한 embedding을 학습하고 그로부터 reward를 잘 만들어낼지는 좀 더 살펴봐야할 것 같다.&lt;/p&gt;
</description>
        <pubDate>Thu, 19 Jul 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/07/19/youtube/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/07/19/youtube/</guid>
        
        <category>rl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>VAE Tutorial 4</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;VAE Tutorial 목차&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/19/vae/&quot;&gt;Tutorial 1: CS231n 강의 내용&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/20/vae2/&quot;&gt;Tutorial 2: VAE 논문 &amp;amp; 코드 리뷰&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae3/&quot;&gt;Tutorial 3: SentenceVAE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae4/&quot;&gt;Tutorial 4: MusicVAE&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;vae-tutorial-4-musicvae&quot;&gt;VAE Tutorial 4: MusicVAE&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/b4pqdgaluovew27/Screenshot%202018-06-21%2014.09.55.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Adam Roberts (Google Brain)&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://arxiv.org/pdf/1803.05428.pdf&quot;&gt;https://arxiv.org/pdf/1803.05428.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;참고한 자료:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://magenta.tensorflow.org/music-vae&quot;&gt;https://magenta.tensorflow.org/music-vae&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 논문을 Tutorial에 넣은 이유는 다음과 같은 이유에서입니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;VAE의 발전과 흐름에 대해서 잘 설명한다.&lt;/li&gt;
  &lt;li&gt;딥러닝에 대한 좋은 방향을 제시한다고 생각한다.&lt;/li&gt;
  &lt;li&gt;이미 웹에 application이 프로토타입으로 구현되어 올라와있다.&lt;/li&gt;
  &lt;li&gt;논문이 예쁘고 읽기 좋게 되어있다.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;다음 유투브 링크는 MusicVAE에 대해서 설명해주는 동영상입니다. 안봤다면 한 번 보는 것을 추천합니다. 한 문장으로 요약하자면 “음악데이터에 대해 VAE를 학습한 다음에 학습한 latent code를 가지고 이러저러한 음악 창작 도구로 사용할 수 있다”라고 볼 수 있습니다. 밑의 화면을 &lt;strong&gt;클릭&lt;/strong&gt; 해주세요.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://youtu.be/G5JT16flZwM&quot;&gt;&lt;img src=&quot;https://img.youtube.com/vi/G5JT16flZwM/0.jpg&quot; alt=&quot;musicvae youtube&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;MusicVAE가 하고 싶은 것은 Music generation입니다. 문장을 만들어내는 일과는 다르게 음악을 만들어내는 일은 훨씬 긴 sequence를 generation 하는 것입니다. 1초의 음원당 보통 16000개의 sample을 가지고 있으니까요. 논문에서는 “very long sequence”라고 표현합니다. 이 문제를 해결하기 위해 MusicVAE는 hierarchical recurrent decoder를 제안합니다. 이 모델 구조만 보자면 다음 그림과 같습니다. Likelihood 쪽이 conductor와 decoder로 hierarchical 하게 구성되어있는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/pkii424867hd5np/Screenshot%202018-06-21%2018.24.29.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 모델 구조를 가지고 음악 데이터에 대해서 학습하고 나면 이미지와 문장에서 했듯이 latent vector interpolation을 할 수 있습니다. 제일 왼쪽에 있는 음원에 대한 latent vector를 구하고 가장 오른쪽의 있는 음원에 대한 latent vector를 구한 다음에 linear interpolation한 것입니다. 이제 MusicVAE에 대해 자세히 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/b01abza1g80n00r/Screenshot%202018-06-21%2018.25.33.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;2-model-of-musicvae&quot;&gt;2. Model of MusicVAE&lt;/h2&gt;
&lt;h3 id=&quot;21-bidirectional-encoder&quot;&gt;2.1 Bidirectional Encoder&lt;/h3&gt;

&lt;h3 id=&quot;22-hierarchical-decoder&quot;&gt;2.2 Hierarchical Decoder&lt;/h3&gt;

&lt;h3 id=&quot;23-multi-stream-modeling&quot;&gt;2.3 Multi-stream Modeling&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;3-experiment--conclusion&quot;&gt;3. Experiment &amp;amp; Conclusion&lt;/h2&gt;
&lt;h3 id=&quot;31-experiment&quot;&gt;3.1 Experiment&lt;/h3&gt;

&lt;h3 id=&quot;32-conclusion&quot;&gt;3.2 Conclusion&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;4-code-review&quot;&gt;4. Code Review&lt;/h2&gt;
</description>
        <pubDate>Thu, 21 Jun 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/06/21/vae4/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/06/21/vae4/</guid>
        
        <category>dl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>VAE Tutorial 3</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;VAE Tutorial 목차&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/19/vae/&quot;&gt;Tutorial 1: CS231n 강의 내용&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/20/vae2/&quot;&gt;Tutorial 2: VAE 논문 &amp;amp; 코드 리뷰&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae3/&quot;&gt;Tutorial 3: SentenceVAE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae4/&quot;&gt;Tutorial 4: MusicVAE&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;vae-tutorial-3-sequencevae&quot;&gt;VAE Tutorial 3: SequenceVAE&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/hz7whkd5iv9n203/Screenshot%202018-06-21%2023.41.18.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Samuel R. Bowman (Stanford NLP group)&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://arxiv.org/pdf/1511.06349.pdf&quot;&gt;https://arxiv.org/pdf/1511.06349.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;참고한 자료:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1803.05428.pdf&quot;&gt;https://arxiv.org/pdf/1803.05428.pdf&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1803.05428.pdf&quot;&gt;MusicVAE 논문&lt;/a&gt;의 introduction 내용을 참고했습니다.&lt;/p&gt;

&lt;p&gt;Tutorial 1과 2에서 다룬 VAE는 generative model입니다. GAN과는 달리 모델의 explicit하게 학습할 수 있는 explicit density model 입니다. 또한 같은 explicit density model 중의 하나인 PixelCNN &amp;amp; PixelRNN 과는 다르게 데이터의 의미있는 latent representation을 학습할 수 있습니다. 이 때 model은 Deep Neural Network를 사용해서 복잡한 transformation을 표현하도록 합니다. 따라서 VAE를 다른 이름으로는 deep latent variable model이라고도 부릅니다.&lt;/p&gt;

&lt;p&gt;VAE는 이전 논문과 코드에서 살펴봤듯이 image를 생성하는데 초첨이 맞춰서 발전했습니다. VAE를 사용해서 여러 흥미로운 연구들이 이뤄졌습니다. 대부분은 VAE가 latent code를 학습하기 때문에 할 수 있는 것들입니다. 예를 들어 갈색 머리 사람의 사진을 모아서 encoder를 통과해 나온 latent vector를 평균 취하면 갈색 머리에 대한 “attribute vector”를 구할 수 있습니다. 만약 검은색 머리에 대한 attribute vector도 구했다면 재밌는 것을 할 수 있습니다. 다음 그림은 “Autoencoding beyond pixels using a learned similarity metric”이라는 논문에서 가져왔는데 attribute vector를 통해 input data를 변형시켜 새로운 data를 생성하는 것을 보여줍니다. 머리색이 변하기도 하고 성별이 변하기도 하며 수염이 생기기도 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/5l4x0xu1kykpavw/Screenshot%202018-06-21%2014.43.53.png?dl=1&quot; /&gt;&lt;/p&gt;
&lt;center&gt;그림 출처 https://arxiv.org/pdf/1512.09300v1.pdf &lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;또한 latent space 상에서 서로 다른 두 latent vector의 interporlation을 통해 재밌는 실험을 할 수도 있습니다. 다음 그림은 “Deep Feature Consistent Variational Autoencoder” 논문에서 가져왔습니다. 그림의 왼쪽 사진에 대한 latent vector를 얻어내고 오른쪽 사진에 대한 latent vector를 얻어내면 &lt;code class=&quot;MathJax_Preview&quot;&gt;z_{new} = \alpha z_{left} + (1-\alpha) z_{right}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z_{new} = \alpha z_{left} + (1-\alpha) z_{right}&lt;/script&gt;와 같은 식으로 linear interpolation으로 새로운 latent vector를 구할 수 있습니다. 다음 그림은 이 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; 값을 0에서 1사이에서 변형시켜가며 그 latent vector를 decoder 통과시켰을 때 나온 이미지를 보여준 것입니다. continuous 하게 왼쪽 이미지에서 오른쪽 이미지로 변해가는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/gp7xncdtri29ox7/Screenshot%202018-06-21%2015.21.02.png?dl=1&quot; /&gt;&lt;/p&gt;
&lt;center&gt;그림 출처 https://arxiv.org/pdf/1610.00291.pdf&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;
이러한 연구들은 대부분 이미지 데이터에 대해서 진행되었습니다. 하지만 최근 이와는 다르게 sequential data를 generation 하는 VAE에 대한 연구들도 이뤄지고 있습니다. Sequential한 data를 생성하기 위해서는 latent vector를 통해 data를 generate하는 decoder가 그냥 CNN이나 MLP가 아닌 RNN이나 PixelRNN 같은 autoregressive model을 사용해야 합니다(latent vector가 RNN에 대한 attention과 같이 작동한다고 생각하시면 이해가 쉬울지도 모르겠습니다 혹은 condition). 문제는 VAE의 decoder에 autoregressive model를 사용할 경우 latent code를 무시해버린다는 것입니다. 이러한 현상은 더 long sequence data를 생성할 때 더 명확히 드러납니다. decoder가 latent code를 무시하는 현상을 “posterior collapse”라고 합니다. 이 논문에서는 “posterior collapse” 문제를 해결하기 위해 ELBO의 KL term에 weight를 곱해주는 방법과 RNN(decoder)의 condition을 조정하는 방법을 사용했습니다.&lt;/p&gt;

&lt;p&gt;sentence에 대해서도 image에서와 마찬가지로 latent vector interpolation을 테스트해볼 수 있습니다. 아래 사진은 “Generating Sentences from a Continuous Space” 논문에서 가져온 사진입니다. &lt;strong&gt;“ i want to talk to you . ”&lt;/strong&gt;라는 문장의 latent vector와 &lt;strong&gt;she did n’t want to be with him .&lt;/strong&gt; 라는 문장의 latent vector 사이를 linear 하게 interpolation 한 것입니다. 문장이 조금씩 변해가는 것을 볼 수 있습니다. 중요한 것은 모두 문법적으로 맞다는 것입니다. 이렇게 문장을 생성하면서 문장의 의미있는 representation을 학습하는 VAE 모델을 SentenceVAE라고 합니다. 이제 살펴보도록 하겠습니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/r2bh3hkvdkw92do/Screenshot%202018-06-22%2011.13.03.png?dl=1&quot; width=&quot;400px&quot; /&gt;&lt;/center&gt;
&lt;center&gt;그림 출처 https://arxiv.org/pdf/1511.06349.pdf&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;2-a-vae-for-sentences&quot;&gt;2. A VAE for sentences&lt;/h2&gt;
&lt;h3 id=&quot;21-recurrent-vae&quot;&gt;2.1 Recurrent VAE&lt;/h3&gt;
&lt;p&gt;(논문이 쓰여질 당시에)기존에 문장을 생성해내는 generative model 중에 state-of-art는 RNNLM(Recurrent Neural Network Language Model)이었습니다. Machine Translation, Image captioning과 같은 task에서 두각을 드러냈지만 문장의 global한 feature를 학습하지 못한다는 단점이 있었습니다. 따라서 이 논문은 VAE라는 새로운 네트워크를 통해 latent vector가 문장의 global feature를 나타내게 하겠다는 것이 목표입니다. RNNLM과 VAE를 적절히 합친다는 뜻입니다. VAE가 hidden latent variable로부터 data를 생성해내는데 여러 task에서 이미 보였듯이 의미있는 representation을 학습합니다. 따라서 RNNLM의 문제를 해결하는데 적절한 방법으로 보이는 것입니다. 다음은 이 논문에서 VAE를 설명하는 부분입니다. 이와같이 만들어낸 사람이 아닌 다른 사람의 해석을 보는 것이 직관력을 기르는데 도움이 됩니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The variational autoencoder (vae, Kingma and
Welling, 2015; Rezende et al., 2014) is a generative
model that is based on a regularized version
of the standard autoencoder. This model imposes
a prior distribution on the hidden codes z which
enforces a regular geometry over codes and makes
it possible to draw proper samples from the model
using ancestral sampling.
The vae modifies the autoencoder architecture
by replacing the deterministic function ϕenc with
a learned posterior recognition model, q(z|x). This
model parametrizes an approximate posterior distribution
over z (usually a diagonal Gaussian) with
a neural network conditioned on x. Intuitively, the
vae learns codes not as single points, but as soft
ellipsoidal regions in latent space, forcing the codes
to fill the space rather than memorizing the training
data as isolated codes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;VAE의 학습 목표인 ELBO는 다음과 같습니다. 이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;g_{\phi}(\epsilon, x) = \mu + \sigma\epsilon&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;g_{\phi}(\epsilon, x) = \mu + \sigma\epsilon&lt;/script&gt; 입니다(reparametization trick).&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/script&gt;

&lt;p&gt;VAE를 sequence generation에 사용하기 위해 encoder와 decoder에 MLP가 아닌 single layer LSTM을 사용합니다. prior는 똑같이 gaussian 분포를 사용합니다. 이 gaussian prior는 ELBO의 KL term에 의해서 일종의 regularizer로 작용합니다. 그리고 latent code는 decoder에 condition으로 작용하며 generation 과정을 조절합니다. 이 모델의 구조는 다음과 같습니다. 문장은 단어 단위로 LSTM에 input으로 들어갑니다. 이 input은 word embedding의 output vector 입니다. 문장이 모두 끝까지 입력으로 들어가고 나면 hidden state의 값이 latent space 상에서 multivariate gaussian의 mean과 variance가 됩니다. 그러면 reparameterization trick을 사용해서 latent code z를 sampling 할 수 있습니다. 이 latent code가 decoder LSTM 모델의 initial hidden state가 되어 word space 상에서의 확률을 output으로 내보냅니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/6islhw0mneo3to5/Screenshot%202018-06-22%2000.32.45.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
이 모델을 학습시키는데 있어서 challenge한 부분은 ELBO의 KL term 입니다. 만약 모델이 의미있는 latent code를 얻어내도록 학습이 되었다면 KL term은 0이 아닐 것이고(문장마다 다른 latent code를 생성하려면 prior와 posterior가 동일한 분포를 가지면 안됩니다) cross entropy term은 상대적으로 작을 것(reconstruction quality가 좋다)입니다. 하지만 위 모델을 그대로 구현해서 학습을 하면 KL term이 0이 되어버리는 문제가 생깁니다. 이렇게 되면 decoder가 문장을 생성할 때 latent code가 영향을 줄 수 없습니다(모든 문장이 동일한 latent code를 가진다고 생각해보세요).&lt;/p&gt;

&lt;p&gt;KL term이 0이 되는 이유는 LSTM의 hidden state 입력이 확률적이기 때문이며 이러한 입력에 LSTM이 민감하기 때문입니다. 이런 경우 decoder는 latent를 무시해버리고 gradient는 encoder로 흘러가지 않습니다. 그러면 encoder는 오직 KL term에 의해서만 학습이 되기 때문에 KL term은 0이 되어버립니다. 이런 현상 즉, “posterior collapse”가 발생하면 VAE를 사용하는 의미가 없습니다. 이 문제를 해결하는 것이 중요합니다. 이 논문에서는 두 가지 방법을 제시합니다. (1) KL cost annealing (2) word dropout and historyless decoding 입니다.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;22-kl-cost-annealing&quot;&gt;2.2 KL cost annealing&lt;/h3&gt;
&lt;p&gt;Posterior collapse 문제를 해결하기 위해 KL term에 곱해지는 weight variable을 도입합니다. 이 weight가 도입된 Objective 식은 다음과 같습니다. 단순히 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;라는 weight를 KL에 곱해준 겁니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - \beta D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - \beta D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/script&gt;

&lt;p&gt;학습이 시작될 때 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;는 0으로 setting 됩니다. 이렇게 되면 cross entropy 항만 남게 되므로 그냥 auto-encoder와 같은 형태가 됩니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;를 0으로 놓는 것은 가능한 많은 정보를 latent에 담아내라는 것입니다. 학습이 진행됨에 따라 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; 값을 점점 증가시킴으로서 latent code를 smooth하게 prior 분포로 묶어줍니다. 그리고 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;가 1이 되면 원래 ELBO의 값이 되어서 일반적인 VAE가 됩니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;beta&lt;/script&gt;를 0에서 1로 키우는 과정(파란선)은 아래 그림과 같이 sigmoid 함수의 형태로 scheduling 합니다. 이에 따라 KL term은 초반에 엄청 상승합니다. KL term의 영향이 적기 때문에 encoder는 제약없이 latent space 상에 representation을 형성하기 때문입니다. 하지만 점점 &lt;code class=&quot;MathJax_Preview&quot;&gt;\beta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;가 1에 가까워지면 이 KL term은 줄어들어서 수렴합니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/shv5n81ymvg8isv/Screenshot%202018-06-22%2001.21.11.png?dl=1&quot; width=&quot;450px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;23-word-dropout-and-historyless-decoding&quot;&gt;2.3 Word dropout and historyless decoding&lt;/h3&gt;
&lt;p&gt;word drop은 decoder를 약하게 만드는 방법입니다. 학습 과정 동안 decoder의 lstm은 이전 ground truth를 input으로 받는데 이러면 데이터 자체에 의존해서 다음 word를 생성합니다. 이 때, 이 ground truth의 일부를 (예를 들어 3 단어 중 1 단어) 무작위로 다른 단어로 바꾸는 것입니다. 그러면 decoder는 좀 더 latent vector에 의존해서 문장을 생성합니다. 전체 문장 중에 단어 몇 개를 바꿀 것인지는 새로운 parameter &lt;code class=&quot;MathJax_Preview&quot;&gt;k \in [0, 1]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;k \in [0, 1]&lt;/script&gt; 를 적용해서 조절합니다. SequenceVAE에 대해서 더 자세히 알기 위해 Code를 살펴보겠습니다.
&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;4-code-review&quot;&gt;4. Code Review&lt;/h2&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;3-experiment--conclusion&quot;&gt;3. Experiment &amp;amp; Conclusion&lt;/h2&gt;
&lt;h3 id=&quot;31-experiment&quot;&gt;3.1 Experiment&lt;/h3&gt;

&lt;h3 id=&quot;32-conclusion&quot;&gt;3.2 Conclusion&lt;/h3&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

</description>
        <pubDate>Thu, 21 Jun 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/06/21/vae3/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/06/21/vae3/</guid>
        
        <category>dl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>VAE Tutorial 2</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;VAE Tutorial 목차&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/19/vae/&quot;&gt;Tutorial 1: CS231n 강의 내용&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/20/vae2/&quot;&gt;Tutorial 2: VAE 논문 &amp;amp; 코드 리뷰&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae3/&quot;&gt;Tutorial 3: SentenceVAE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae4/&quot;&gt;Tutorial 4: MusicVAE&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;vae-tutorial-2-vae-논문--코드-리뷰&quot;&gt;VAE Tutorial 2: VAE 논문 &amp;amp; 코드 리뷰&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/1niug5qggbfatg7/Screenshot%202018-06-19%2021.36.15.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Diederik P. Kingma (Universiteit van Amsterdam)&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://arxiv.org/pdf/1312.6114.pdf&quot;&gt;https://arxiv.org/pdf/1312.6114.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;참고한 자료:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.05908.pdf&quot;&gt;https://arxiv.org/pdf/1606.05908.pdf&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://blog.evjang.com/2016/08/variational-bayes.html&quot;&gt;https://blog.evjang.com/2016/08/variational-bayes.html&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf&quot;&gt;https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이번 포스트에서는 VAE의 원래 논문인 “Auto-Encoding Variational Bayes”의 내용 중 일부를 다루고 Pytorch VAE example code를 리뷰해봅니다.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;1-variational-inference--reparameterization-trick&quot;&gt;1. Variational Inference &amp;amp; Reparameterization Trick&lt;/h2&gt;
&lt;p&gt;논문의 Abstract에서는 다음과 같은 말을 던지면서 시작합니다. 이 포스트에서도 기본적으로 MNIST 데이터셋에 대한 generative model의 전제하에 이야기합니다.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable posterior
distributions, and large datasets?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;VAE가 하고 싶은 것은 명확합니다. 또한 그것을 가로막는 문제도 명확히 제시합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;목표&lt;/strong&gt;: efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;문제&lt;/strong&gt;: intractable posterior, large dataset&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이것을 이해하기 위해 이전 포스트에서 언급했던 식을 다시 살펴보겠습니다. directed probabilistic model 이라는 말은 explicit density estimation이라고도 볼 수 있습니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z)&lt;/script&gt;에서 latent variable을 sampling 한다면 대부분의 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;에 대해 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x \vert z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x \vert z)&lt;/script&gt;는 거의 0의 값을 가질 것입니다. 그렇다면 다음 식처럼 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x)&lt;/script&gt;에 대한 Monte-Carlo estimation을 하는데 sample이 너무 많이 필요하게됩니다. 데이터포인트 하나당 많은 sample을 필요로 하므로 large dataset에 대해서 이렇게 estimation을 하면 학습과정이 너무 느려집니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz \approx \frac{1}{N}\sum_{i=1}^N p_{\theta}(x|z^{(i)})&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz \approx \frac{1}{N}\sum_{i=1}^N p_{\theta}(x|z^{(i)})&lt;/script&gt;

&lt;p&gt;따라서 데이터에 dependent하게 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 sampling하기 위해 posterior &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z \vert x)&lt;/script&gt;를 정의했었는데 이 posterior는 intractable 합니다. 따라서 이 posterior를 approximate하는 새로운 posterior &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x)&lt;/script&gt;를 정의했었습니다. 이렇게 우리가 다루기 쉬운 paramterized된 posterior를 대신 사용하고 이 posterior가 원래의 posterior와 최대한 가깝게 만드는 것이 &lt;strong&gt;variational inference&lt;/strong&gt; 입니다. 또는 &lt;strong&gt;variational bayes&lt;/strong&gt;라고도 합니다. 이것은 다음 그림과 같이 파란색 분포에 초록색 분포를 최대한 맞추는 것과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/ocojvekrigo247z/Screenshot%202018-06-20%2023.45.41.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;center&gt;그림출처 https://blog.evjang.com/2016/08/variational-bayes.html&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;
최적화과정을 거쳐 approximate된 posterior와 posterior가 최대한 비슷해지면 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x)&lt;/script&gt;를 통해 inference 할 수 있습니다. Inference 하는 것을 variational parameter &lt;code class=&quot;MathJax_Preview&quot;&gt;\phi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;를 통해 하는 것입니다. 이 최적화과정은 이전 포스트에서 구한 ELBO를 최대화하는 것입니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z)) - (1)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z)) - (1)&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\theta^*, \phi^* = argmax_{\theta, \phi}\sum_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^*, \phi^* = argmax_{\theta, \phi}\sum_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)&lt;/script&gt;

&lt;p&gt;이 ELBO의 값을 maximize하는 parameter는 (1) analytic하게 구하거나 (2) stochastic gradient ascent를 통해 구할 수 있습니다. Analytic하게 구하는 방식 중에 Mean-Field Variational Bayes가 있습니다. 논문에서는 이 방법이 likelihood function인 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x \vert z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x \vert z)&lt;/script&gt;이 뉴럴넷과 같은 복잡한 함수로 표현될 경우 intractable 하다고 말합니다. 논문에서 VAE의 방법과 Mean-Field Variational Bayes 사이의 차이에 대해서 다음과 같이 언급합니다.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;Note that in contrast with the approximate
posterior in mean-field variational inference, it is not necessarily factorial and its parameters φ are
not computed from some closed-form expectation&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;따라서 (1)식의 gradient를 구해서 stochastic하게 parameter를 업데이트하는 방식을 사용할 것입니다. 이 때 (1) 식을 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;에 대해서 미분하는 것은 문제가 없으나 &lt;code class=&quot;MathJax_Preview&quot;&gt;\phi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;에 대해서 미분하는 것은 문제가 있습니다. (1) 식 중에서도 첫번째 항이 문제가 있습니다. 첫번째 항의 expectation 안에 있는 함수를 &lt;code class=&quot;MathJax_Preview&quot;&gt;f(z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f(z)&lt;/script&gt;라고 가정해보겠습니다. 이 함수의 expectation에 대한 미분은 다음과 같이 쓸 수 있습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\nabla_{\phi}\mathbb{E}_{q_{\phi}(z)}[f(z)] = \int \nabla_{\phi} q_{\phi}(z) f(z) dz&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_{\phi}\mathbb{E}_{q_{\phi}(z)}[f(z)] = \int \nabla_{\phi} q_{\phi}(z) f(z) dz&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;= \int q_{\phi}(z)\frac{\nabla_{\phi} q_{\phi}(z)}{q_{\phi}(z)} f(z) dz&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \int q_{\phi}(z)\frac{\nabla_{\phi} q_{\phi}(z)}{q_{\phi}(z)} f(z) dz&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;=\mathbb{E}_{q_{\phi}(z)}[f(z)\nabla_{\phi}log q_{\phi}(z)]&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\mathbb{E}_{q_{\phi}(z)}[f(z)\nabla_{\phi}log q_{\phi}(z)]&lt;/script&gt;

&lt;p&gt;이 미분값은 monte-carlo estimation을 통해 estimate 할 수 있습니다. 이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;z^{(i)}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;는 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/script&gt;로부터 sampling 합니다. 따라서 다음 gradient 값은 상당히 variance가 높습니다. 이 경우 impractical 합니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\frac{1}{L}\sum_{l=1}^L f(z^{(l)})\nabla_{\phi}log q_{\phi}(z^{(l)})&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{L}\sum_{l=1}^L f(z^{(l)})\nabla_{\phi}log q_{\phi}(z^{(l)})&lt;/script&gt;

&lt;p&gt;이러한 문제를 해결하기 위해 VAE는 reparameterization trick이라는 technique을 사용합니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 posterior &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x)&lt;/script&gt;로부터 sampling 하는 것이 아니라 differentiable 한 함수 &lt;code class=&quot;MathJax_Preview&quot;&gt;g_{\phi}(\epsilon, x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;g_{\phi}(\epsilon, x)&lt;/script&gt;로부터 deterministic하게 정해진다고 보는 것입니다. 이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;\epsilon&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;은 noise variable입니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\tilde{z} = g_{\phi}(\epsilon, x) \qquad where \quad \epsilon \sim p(\epsilon)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\tilde{z} = g_{\phi}(\epsilon, x) \qquad where \quad \epsilon \sim p(\epsilon)&lt;/script&gt;

&lt;p&gt;이 경우 다음과 같이 &lt;code class=&quot;MathJax_Preview&quot;&gt;f(z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f(z)&lt;/script&gt;의  &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z)&lt;/script&gt; 대한 expectation을 &lt;code class=&quot;MathJax_Preview&quot;&gt;epsilon&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;epsilon&lt;/script&gt;에 대한 expectation으로 바꿀 수 있습니다. 이제 바뀐 expectation에 대해 monte carlo estimation을 적용할 수 있습니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;f(g_{\phi}(z^{(l)}, x^{(i)}))&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f(g_{\phi}(z^{(l)}, x^{(i)}))&lt;/script&gt;는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;\phi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;에 대해 미분가능하기 때문에 바로 미분할 수 있습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathbb{E}_{q_{\phi}(z \vert x^{(i)})}[f(z)] = \mathbb{E}_{\epsilon}[f(g_{\phi}(\epsilon, x^{(i)}))] = \frac{1}{L}\sum_{l=1}^L f(g_{\phi}(z^{(l)}, x^{(i)}))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbb{E}_{q_{\phi}(z \vert x^{(i)})}[f(z)] = \mathbb{E}_{\epsilon}[f(g_{\phi}(\epsilon, x^{(i)}))] = \frac{1}{L}\sum_{l=1}^L f(g_{\phi}(z^{(l)}, x^{(i)}))&lt;/script&gt;

&lt;p&gt;이 수식을 이용해서 ELBO를 고쳐쓸 수 있습니다. 이 식을 SGVB(Stochastic Gradient Variational Bayes) estimator라고 합니다. 이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;z^{(i,l)}=g_{\phi}(\epsilon^{(l)}, x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z^{(i,l)}=g_{\phi}(\epsilon^{(l)}, x^{(i)})&lt;/script&gt;이고 &lt;code class=&quot;MathJax_Preview&quot;&gt;\epsilon^{(l)} \sim p(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon^{(l)} \sim p(\theta)&lt;/script&gt; 입니다. 보통은 &lt;code class=&quot;MathJax_Preview&quot;&gt;g_{\phi}(\epsilon, x) = \mu + \sigma\epsilon&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;g_{\phi}(\epsilon, x) = \mu + \sigma\epsilon&lt;/script&gt;으로 많이 사용합니다(univariate gaussian case).&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/script&gt;

&lt;p&gt;이러한 reparameterization trick을 그림으로 보자면 다음과 같습니다. 원래는 encoder로부터 구한 data dependent한 mean과 variance를 가지고 posterior를 만듭니다. 그 posterior로부터 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 샘플링한 다음에 그 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 가지고 decoder는 data를 generation 했습니다. 하지만 reparametization을 하면 computation graph 내의 sampling 과정이 noise sampling이 되어 옆으로 빠져버립니다. 따라서 Back propagation을 통해 decoder output으로부터 encoder까지 gradient가 전달될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/5249ixq6r4t38l8/Screenshot%202018-06-21%2000.57.08.png?dl=1&quot; /&gt;&lt;/p&gt;
&lt;center&gt;그림출처 https://arxiv.org/pdf/1606.05908.pdf&lt;/center&gt;

&lt;p&gt;이렇게 업데이트를 하는 알고리즘이 Auto-Encoding Variational Bayes이며 다음과 같습니다. 
&lt;img src=&quot;https://www.dropbox.com/s/hxacd2bhz1hi3yl/Screenshot%202018-06-21%2001.05.46.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2&gt;&lt;br /&gt;&lt;br /&gt;&lt;/h2&gt;
&lt;h2 id=&quot;2-vae-code-example&quot;&gt;2. VAE code example&lt;/h2&gt;
&lt;h3 id=&quot;21-vae-example-of-paper&quot;&gt;2.1 VAE example of paper&lt;/h3&gt;
&lt;p&gt;prior와 posterior를 모두 gaussian으로 가정하고 likelihood를 Bernoulli라고 가정하면 ELBO 식은 다음과 같이 쓸 수 있습니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;y&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;는 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;와 decoder를 통해 나온 값입니다. 마지막 식의 첫번째 항은 잘 보면 cross-entropy 인 것을 알 수 있습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)&lt;/script&gt;

&lt;p&gt;&lt;br /&gt;
논문의 실험결과는 다음과 같습니다. 이 그림은 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 임의로 변형시켜보면서 data를 생성해낸 결과입니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;에 따라 data가 continuous하게 변화하는 것을 볼 수 있습니다. 또한 비슷한 숫자끼리는 서로 뭉쳐있음을 알 수 있습니다. 이를 통해 VAE가 의미있는 representation을 학습하는 것을 확인합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/f2dpiyr9rptcgwe/Screenshot%202018-06-21%2001.27.20.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 마지막 식을 가지고 이제 우리는 VAE 코드를 살펴볼 수 있습니다. Pytorch는 공식적으로 VAE에 대한 simple한 example을 제공합니다. 지금까지 살펴본 VAE의 이론에 충실한 코드입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;코드링크: &lt;a href=&quot;https://github.com/pytorch/examples/tree/master/vae&quot;&gt;https://github.com/pytorch/examples/tree/master/vae&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;22-vae-network-class&quot;&gt;2.2 VAE network class&lt;/h3&gt;
&lt;p&gt;VAE의 network 구조는 다음과 같습니다. 28x28의 이미지를 일렬로 펴서 784 크기의 vector로 만듭니다. 이 vector를 입력으로 받기 때문에 self.fc1이 784에서 400개의 hidden unit으로의 fully connected인 것을 알 수 있습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;VAE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VAE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc21&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc22&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;400&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reparameterize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reparameterize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
Encoder의 구조는 다음과 같습니다. 784-d vector가 input으로 들어와서 400-d hidden layer를 통과하고 이 때 activation function은 relu를 사용합니다. 그 이후에 20개의 gaussian 분포에 대한 mean과 variance를 내보낼 것입니다. Mean은 self.fc21(h1)이며 linear 연산을 통한 output입니다. Variance의 경우 항상 0보다 크거나 같아야하는데 linear 연산을 한 self.fc22(h1)의 경우 -값이 될 수 있습니다. 따라서 이 값을 variance로 보지 않고 log variance라고 보는 것입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc21&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc22&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
latent space 상에서의 mean과 log variance를 구했다면 reparameterization을 통해 latent vector &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 sampling 할 수 있습니다. Noise로부터 &lt;code class=&quot;MathJax_Preview&quot;&gt;\epsilon&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;인 eps를 구하고 이 eps와 standard deviation을 곱하고 mean을 더해줍니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;reparameterize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;std&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eps&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mul&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;std&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 구하고나면 이 latent 값으로부터 decoder를 통해 data에 대한 Bernoulli distribution을 출력할 수 있습니다. Bernoulli 분포는 0에서 1 사이이므로 sigmoid 함수를 output layer의 activation function으로 사용합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
결국 VAE의 forward path는 다음과 같습니다. x.view(-1, 784)는 이미지를 784-D vector로 만드는 부분입니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;z&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reparameterize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;z&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;23-train-vae&quot;&gt;2.3 Train VAE&lt;/h3&gt;
&lt;p&gt;학습에 관련된 가장 중요한 부분 중의 하나인 loss function 정의 부분입니다. 다음 수식을 생각해서 코드로 어떻게 구현되었는지 보면 됩니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)&lt;/script&gt;

&lt;p&gt;첫번째 항은 F.binary_cross_entropy로 구현되었으며 두번째 항은 KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())로 구현되었습니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Reconstruction + KL divergence losses summed over all elements and batch
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;BCE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;binary_cross_entropy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;784&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size_average&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c1&quot;&gt;# see Appendix B from VAE paper:
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# https://arxiv.org/abs/1312.6114
&lt;/span&gt;    &lt;span class=&quot;c1&quot;&gt;# 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;KLD&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;exp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BCE&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KLD&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
학습 코드 부분은 상당히 간단합니다. model.train()을 통해 현재 학습할 것이라는 것을 선언합니다. 그 이후에 train_loader를 통해 mini_batch를 추출합니다. 그 다음에 model에 data를 입력으로 넣어서 출력을 받습니다. 그 출력을 통해 loss function 값을 구할 수 있고 loss.backward()를 통해 back-propagation으로 각 parameter의 gradient 값을 구합니다. 그 이후에 optimizer(Adam optimizer)를 통해 각 parameter를 update 합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zero_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;recon_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;backward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;optimizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;step&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;log_interval&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Train Epoch: {} [{}/{} ({:.0f}%)]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;Loss: {:.6f}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;mf&quot;&gt;100.&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch_idx&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'====&amp;gt; Epoch: {} Average loss: {:.4f}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
          &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3 id=&quot;24-evaluate-vae&quot;&gt;2.4 Evaluate VAE&lt;/h3&gt;
&lt;p&gt;학습과정을 evaluate 하는 것은 더 간단합니다. model.eval()을 통해 평가 중이라는 것을 선언합니다. 그 이후에 test dataset에 대해 reconstruction을 출력합니다. 그 이후에 loss function 값을 출력해서 학습이 어떻게 진행되고 있는지 평가합니다. 그리고 각 평가과정마다 생성된 하나의 sample을 저장합니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;enumerate&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;recon_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;test_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loss_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recon_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;logvar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;item&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;comparison&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                                      &lt;span class=&quot;n&quot;&gt;recon_batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]])&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;save_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;comparison&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;
                         &lt;span class=&quot;s&quot;&gt;'results/reconstruction_'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;test_loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_loader&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'====&amp;gt; Test set loss: {:.4f}'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_loss&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;25-main-loop&quot;&gt;2.5 Main loop&lt;/h3&gt;

&lt;p&gt;epoch마다 train을 하고 test를 한 다음에 64개의 sample 이미지를 생성합니다. latent는 임의로 20개를 normal distribution에서 sampling 합니다. 이 sampling된 latent variable을 Decoder에 통과시키면 decoder는 이미지를 생성해냅니다.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;args&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epochs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;no_grad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;randn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;to&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;decode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cpu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;save_image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sample&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;view&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
                   &lt;span class=&quot;s&quot;&gt;'results/sample_'&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;epoch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
이 코드는 python main.py를 하면 바로 실행이 되며 실행 화면은 다음과 같다.
&lt;img src=&quot;https://www.dropbox.com/s/15kxe3fiuvgnu25/Screenshot%202018-06-21%2002.13.39.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;첫 epoch 때 test set에 대한 loss는 119.4120이며 real data reconstruction과 z sampled recontruction은 다음 사진과 같습니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/7o8i909pf6jgnt9/Screenshot%202018-06-21%2002.15.50.png?dl=1&quot; width=&quot;300px&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/gk7y171bbgefwnc/Screenshot%202018-06-21%2002.16.46.png?dl=1&quot; width=&quot;300px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;50 epoch 때 test set에 대한 loss는 93.3473이며 real data reconstruction과 z sampled recontruction은 다음 사진과 같습니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/wk93oi0ahh0t54n/Screenshot%202018-06-21%2002.19.13.png?dl=1&quot; width=&quot;300px&quot; /&gt;&lt;/center&gt;
&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/jnmrzjpyw36nh9l/Screenshot%202018-06-21%2002.19.39.png?dl=1&quot; width=&quot;300px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;학습이 잘 된 것을 확인할 수 있습니다. 이제 다음 post에서 VAE를 사용해 Music generation이라는 도메인에 적용한 MusicVAE 를 살펴보겠습니다.&lt;/p&gt;
</description>
        <pubDate>Wed, 20 Jun 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/06/20/vae2/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/06/20/vae2/</guid>
        
        <category>dl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>VAE Tutorial 1</title>
        <description>&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;VAE Tutorial 목차&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/19/vae/&quot;&gt;Tutorial 1: CS231n 강의 내용&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/20/vae2/&quot;&gt;Tutorial 2: VAE 논문 &amp;amp; 코드 리뷰&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae3/&quot;&gt;Tutorial 3: SentenceVAE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae4/&quot;&gt;Tutorial 4: MusicVAE&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;vae-tutorial-1-cs231n-lecture-13&quot;&gt;VAE Tutorial 1: CS231n Lecture 13&lt;/h1&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/ridjvvwx0ve4v3y/Screenshot%202018-06-20%2014.20.11.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;참고한 자료:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture13.pdf&quot;&gt;CS231n lecture 13&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1606.05908.pdf&quot;&gt;https://arxiv.org/pdf/1606.05908.pdf&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;cs231n 강의 내용과 Kingma의 논문을 통해 Variational Auto-Encoder를 정리해봅니다. 그리고 그 이후에 sequential data에 VAE를 적용한 사례인 SentenceVAE와 MusicVAE를 다룹니다. 다음과 같이 이 VAE 튜토리얼은 진행합니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;VAE Tutorial 목차&lt;/strong&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/19/vae/&quot;&gt;Tutorial 1: CS231n 강의 내용&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/20/vae2/&quot;&gt;Tutorial 2: VAE 논문 &amp;amp; 코드 리뷰&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae3/&quot;&gt;Tutorial 3: SentenceVAE&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://dnddnjs.github.io/paper/2018/06/21/vae4/&quot;&gt;Tutorial 4: MusicVAE&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 글에서는 1. CS231n 강의 내용을 다룰 것입니다. CS231n lecture 13에서는 여러가지 generative model을 다루는데 그 중에 VAE가 들어있습니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;1-supervised-learning-vs-unsupervised-learning&quot;&gt;1. Supervised Learning vs Unsupervised Learning&lt;/h2&gt;
&lt;p&gt;13강 이전까지는 supervised learning 문제에 대한 딥러닝을 다뤘습니다. Supervised Learning은 데이터와 정답이 있는 경우 모델의 output이 정답과 같아지도록 loss function을 정의하고 학습합니다. Supervised Learning을 통해 Classification, Regression, Detection, Segmentation, image-captioning 등 다양한 task를 학습할 수 있습니다.&lt;br /&gt;
&lt;img src=&quot;https://www.dropbox.com/s/fe8fmjxdi7c510o/Screenshot%202018-06-19%2021.47.16.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
하지만 이 경우 항상 label이 있어야합니다. 그래서 데이터에 대한 cost가 큽니다. 딥러닝의 경우 모델의 성능이 데이터의 양과 질에 크게 의존하기 때문에 이에 대한 부담이 있습니다. 그렇다면 label이 없이 학습하는 모델은 없을까요? 그게 바로 Unsupervised Learning 입니다. Unsupervised Learning은 input이 되는 x 데이터만으로 학습할 수 있습니다. 주 목적은 x 데이터의 underlying hidden structure을 학습하는 것입니다. 개인적으로는 data의 meaningful representation을 학습하는 것이 중요하다고 생각해서 Unsupervised Learning을 관심있게 보고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/xjv2wwwquno8n8j/Screenshot%202018-06-19%2021.50.49.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
Unsupervised Learning으로는 Clustering, dimensionality reduction, feature learning, density estimation 등을 할 수 있습니다. 지도학습과는 달리 아직 미개척 분야이며 풀어야할 문제가 많고 어쩌면 그 안에 보물을 담고 있을지도 모릅니다. Unsupervised Learning 중에 가장 유명한 것이 K-means와 Auto-Encoder 입니다. 하지만 Auto-Encoder를 그냥 사용하는 경우는 없고 보통 VAE(Variational Auto-Encoder)를 많이 사용합니다. VAE의 경우 Auto-Encoder와는 달리 데이터의 의미있는 representation을 학습할 수 있습니다. 하지만 Auto-Encoder를 처음 접하고 나서 VAE로 넘어갈 때 상당히 큰 벽이 있는 것 같습니다.&lt;/p&gt;

&lt;p&gt;VAE를 제대로 이해하려면 코드부터 보는 것이 아니라 네 가지를 알아야합니다.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;VAE는 Generative model이라는 것&lt;/li&gt;
  &lt;li&gt;latent variable이라는 것이 있으며 이것을 바탕으로 데이터를 생성한다는 것(Decoder)&lt;/li&gt;
  &lt;li&gt;문제를 더 쉽게 만들기 위해 latent variable 이라는 것을 Encoder를 통해 추출한다는 것&lt;/li&gt;
  &lt;li&gt;VAE의 학습과정은 MLE라는 것&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;2-generative-model&quot;&gt;2. Generative Model&lt;/h2&gt;
&lt;p&gt;VAE는 일종의 Generative model이라고 봐야합니다. Generative model이란 training data가 주어졌을 때 이 data가 sampling된 분포와 같은 분포에서 새로운 sample을 생성하는 model입니다. 즉 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{model}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{model}(x)&lt;/script&gt;가 최대한 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{data}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{data}(x)&lt;/script&gt;에 가깝게 만드는 것이 목표입니다. 이것을 어떻게 할 수 있을까요? 결국 얼마나 기존 모델과 가까운 것인가에 대한 지표를 만들어야하고 그 차이를 최소화하도록 gradient를 계산해서 업데이트할 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/ltfz827o0ltj611/Screenshot%202018-06-19%2021.58.21.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
다음 그림은 Generative Model에 대해 Ian Goodfellow가 정리한 도표입니다. Generative Model은 크게 Explicit Density와 Implicit Density 두 가지로 나눌 수 있습니다. Explicit Density 모델을 data를 샘플링한 모델의 구조를 명확히 정의를 합니다. 그 모델로부터 data를 sampling 하는 것입니다. 하지만 Implicit Density에서는 모델에 대한 explicit하게 정의하지 않습니다. 예를 들어, GAN의 경우 noise로부터 바로 data로의 transformation을 학습합니다. VAE는 data의 model의 density model을 explicit하게 정의해서 직접적으로 학습하는 경우라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/ce7x00eq6eltvho/Screenshot%202018-06-19%2022.05.22.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;다시 말하자면 다음과 같습니다. “density estimation”은 x라는 데이터만 관찰할 수 있을 때, 관찰할 수 없는 x가 샘플된 확률밀도함수(probability density function)을 estimate하는 것입니다. 예를 들어 다음 그림의 첫번째 그림처럼 데이터 포인트가 찍혀있을 경우에 대해서 생각해봅니다. Density estimation이 하는 일은 이 데이터 포인트가 샘플링 됐을 것 같은 확률밀도함수를 찾아내는 것입니다. 데이터 포인트가 많은 곳은 확률이 높고 데이터 포인트가 없는 곳은 확률이 낮을 것입니다. 이러한 확률분포(파란색 선)가 있다면 현재 데이터 포인트와 같지는 않지만 비슷한 데이터를 생성해낼 수 있을 것입니다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/t78ehuhvoor7rj1/Screenshot%202018-06-19%2022.49.03.png?dl=1&quot; width=&quot;400px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;&lt;br /&gt;
즉, 확률밀도함수 즉 최대한 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{data}(\mathcal{x})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{data}(\mathcal{x})&lt;/script&gt;에 가까운 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{model}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{model}(x)&lt;/script&gt;를 찾아냈을 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{model}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{model}(x)&lt;/script&gt;을 통해 새로운 데이터를 “generate” 할 수 있습니다. 이렇게 새로운 data를 generate를 할 수 있으므로 Generative Model이라고 부르는 것입니다. 이미지의 경우 각 pixel은 0에서 255의 값을 가질 수 있고 rgb 3차원이므로 나타낼 수 있는 이미지의 양은 어마어마하게 많습니다. 하지만 우리가 “realistic”이라고 느끼는 이미지들은 그 중에 정말 일부입니다. 또한 데이터셋에 있는 이미지들은 그보다 더 일부일 것입니다. Generative Model은 그 엄청나게 큰 space에서 realistic한 이미지를 샘플링할 수 있는 확률밀도함수입니다.&lt;/p&gt;

&lt;p&gt;Generative Model 중에 가장 선명하고 진짜같은 이미지를 생성하는 것은 GAN(Generative Adversarial Network)입니다. GAN은 여러 변형체들이 많은데 그 중에 하나의 학습된 모델을 가지고 생성한 데이터 예시입니다. 하지만 GAN의 경우 random noise로부터 데이터를 생성해내므로 데이터의 의미있는 representation을 학습할 수 없습니다. 이제 VAE에 대해서 살펴보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/rqxbf2jw3odz7hn/Screenshot%202018-06-19%2023.06.16.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;3-variational-auto-encoder&quot;&gt;3. Variational Auto-Encoder&lt;/h2&gt;
&lt;p&gt;앞으로 MNIST 데이터 셋을 생성하는 Generative Model에 대해서 이야기하도록 하겠습니다. 일단 probability density function을 정의합니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x)&lt;/script&gt;라고 정의합니다. dataset은 &lt;code class=&quot;MathJax_Preview&quot;&gt;X = [x^{(i)}]_{i=1}^N&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;X = [x^{(i)}]_{i=1}^N&lt;/script&gt;이라고 할 수 있습니다. 이 때, dataset의 datapoint(&lt;code class=&quot;MathJax_Preview&quot;&gt;x^{(i)}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;)는 i.i.d. 하다고 가정합니다. 이 density function은 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;를 parameter로 가지고 있습니다. 이 정의의 의미는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;라는 parameter가 정해졌을 때 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;라는 데이터가 나올 확률입니다. 이 확률을 최대화하는 것이 Generative Model 혹은 density estimation의 목표입니다. 이 식은 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;라는 latent variable을 사용해서 다음과 같이 쓸 수 있습니다. 앞으로 우리는 이 식을 미분해서 그 미분값에 따라 stochastic gradient ascent를 할 것입니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz - (1)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz - (1)&lt;/script&gt;

&lt;p&gt;이 식은 다음 그림으로 보면 더 이해가 잘 갑니다. VAE에서는 Auto-Encoder와는 달리 latent variable을 정의합니다. 왜 갑자기 “latent variable”이라는게 등장했을까요? 우리가 생성하고 싶은 데이터들은 상당히 차원이 높고 예를 들어 datapoint의 pixel 사이에 복잡한 관계가 있습니다. 따라서 pixel 사이의 관계를 확률모델로 모델링하는 것이 아니라 데이터를 표현하는 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; (mnist의 경우 어떤 숫자인지)가 있으면 그 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;로부터 데이터를 생성하는 graphical model을 생각해보는 것입니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/jyiu96dd3tp8rue/Screenshot%202018-06-20%2000.53.19.png?dl=1&quot; width=&quot;150px&quot; /&gt;&lt;/center&gt;
&lt;center&gt;그림출처 http://pyro.ai/examples/vae.html&lt;/center&gt;

&lt;p&gt;(1) 식의 우변에서 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z)&lt;/script&gt;는 latent variable &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 sampling 할 수 있는 확률밀도함수입니다. 그리고 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x\vert z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x\vert z)&lt;/script&gt;는 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;가 주어졌을 때 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;를 생성해내는 확률밀도함수입니다. 중요한 것은 이 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x\vert z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x\vert z)&lt;/script&gt;가 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;에 대해 미분가능해야한다는 것입니다. 보통 output을 Gaussian 분포나 Bernoulli 분포로 설정합니다. (1) 식은 intractable 한데 그 이유는 가능한 모든 z에 대해 integral을 취하기 때문입니다. 따라서 이 식에 대해서 직접적으로 미분할 수 없습니다. 하지만 이 MLE 문제를 풀기 위해서 우리는 미분을 해야합니다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/h0kcdfe7r0o2lqa/Screenshot%202018-06-20%2008.09.47.png?dl=1&quot; width=&quot;450px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;문제는 두 가지입니다. (1) 어떻게 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;라는 놈을 구성할 것인가? (2) intergral을 어떻게 해결할 것인가? 입니다. VAE는 이 두 가지 문제를 해결합니다. 일단 CS231n 강의의 그림으로 다시 보자면 다음과 같습니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta^*&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta^*&lt;/script&gt;는 dataset이 sampling된 true distribution의 parameter입니다(이 분포는 parameterized 되었다고 가정하는 것입니다). simple한 Gaussian 분포로부터 &lt;code class=&quot;MathJax_Preview&quot;&gt;z^{(i)}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;를 sampling 합니다. decode network &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta^*}(x\vert z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta^*}(x\vert z)&lt;/script&gt;는 &lt;code class=&quot;MathJax_Preview&quot;&gt;z^{(i)}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z^{(i)}&lt;/script&gt;로부터 데이터를 생성합니다. 우리가 하고 싶은 것은 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta^*&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta^*&lt;/script&gt;를 estimate 하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/9lnibxvcsmyhe8r/Screenshot%202018-06-20%2008.04.21.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;는 사실 mnist에서 어떤 숫자인지만 나타내야하는 것이 아니라 각 숫자마다도 다른 복잡한 특징들을 나타내야 합니다. 이것을 단순한 normal distribution으로 만들고 decoder network의 layer들이 알아서 data를 생성해내는데 필요한 정보를 추출하도록 하는 것입니다. 즉, 여러 층의 layer 들이 있다면 앞의 층들은 normal distribution을 latent value로 변환해주는 일을 하는 것이고 뒤의 층들은 이 latent value를 가지고 realistic한 digit를 sampling 할 수 있는 확률밀도함수를 만들어내는 것입니다.&lt;/p&gt;

&lt;p&gt;두 번째 문제인 integral의 경우 integral을 다 계산하지 않고 Monte-Carlo estimation을 통해 estimate 할 것입니다. 여기서 Bayesian이 등장합니다. 대부분의 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;에 대해서는 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x \vert z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x \vert z)&lt;/script&gt;는 거의 0의 값을 가질 것입니다. 따라서 sampling이 상당히 많이 필요합니다. 데이터셋이 클 경우에 이것은 너무 cost가 큽니다. 좀 더 efficient하게 이 sampling 과정을 진행하려면 data에 dependent하게 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 sampling 할 필요가 있습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz \approx \frac{1}{N}\sum_{i=1}^N p_{\theta}(x|z^{(i)})- (2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz \approx \frac{1}{N}\sum_{i=1}^N p_{\theta}(x|z^{(i)})- (2)&lt;/script&gt;

&lt;p&gt;따라서 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z \vert x)&lt;/script&gt;를 생각해보는 것입니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z \vert x)&lt;/script&gt;가 하는 역할은 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;가 주어졌을 때 이 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;를 생성해낼 것 같은 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;에 대한 확률분포를 만드는 것입니다. 이것은 Bayes’ rule에 의해 다음과 같이 쓸 수 있습니다. 하지만 (1) 식에서 보듯이 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x)&lt;/script&gt;를 계산하는 것은 intractable 하므로 이 posterior 또한 intractable posterior가 됩니다. 따라서 이 poterior를 approximate 하는 새로운 함수를 정의합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/fi8reyjhzjh2h46/Screenshot%202018-06-20%2008.49.00.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;code class=&quot;MathJax_Preview&quot;&gt;\phi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi&lt;/script&gt;라는 새로운 parameter로 표현되는 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x)&lt;/script&gt;는 일종의 encoder라고 볼 수 있습니다. 원래의 posterior를 approximate 했기 때문에 error가 존재합니다. 따라서 원래의 objective function에 대한 lower bound를 정의할 것입니다. 그 전에 VAE의 네트워크 구조를 살펴보겠습니다. “encoder”는 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x)&lt;/script&gt;이며 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;를 input으로 받아서 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; space 상에서 확률분포를 만듭니다. 이 확률분포는 gaussian이라고 가정해서 만듭니다. 이 data dependent한 gaussian 분포로부터 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 sampling 합니다. sampling된 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 가지고 “decoder” &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x|z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x|z)&lt;/script&gt;는 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;의 space 상의 gaussian distribution 혹은 Bernoulli distribution을 output으로 내놓습니다. 그러면 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;를 이 분포로부터 sampling 할 수 있습니다. 이러한 구조를 가지기 때문에 “Auto-Encoder”가 되는 것이며 학습이 되고 나면 latent variable &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;라는 data의 의미있는 representation을 얻을 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/dxn7qfpfztrjduh/Screenshot%202018-06-20%2008.53.53.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;4-elboevidence-lower-bound&quot;&gt;4. ELBO(Evidence Lower Bound)&lt;/h2&gt;
&lt;p&gt;이제 VAE를 어떻게 학습시키는지를 살펴보기 위해 objective function을 변형시켜보겠습니다. log likelihood는 다음과 같습니다. 이 값을 최대화시키는 것이 목표입니다. 이 식 자체는 intractable 하기 때문에 변형이 필요합니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;log p_{\theta}(x^{(i)})&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p_{\theta}(x^{(i)})&lt;/script&gt;

&lt;p&gt;이 log likelihood를 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x)&lt;/script&gt;로부터 sampling한 latent variable &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;에 대한 expectation 식으로 바꿀 수 있습니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x^{(i)}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x^{(i)}&lt;/script&gt;가 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;에 dependent하지 않기 때문입니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z\sim q_{\phi}(z \vert x^{(i)})}[log p_{\theta}(x^{(i)})] - (3)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z\sim q_{\phi}(z \vert x^{(i)})}[log p_{\theta}(x^{(i)})] - (3)&lt;/script&gt;

&lt;p&gt;여기서 Bayes’ Rule을 적용해보겠습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;p_{\theta}(z \vert x^{(i)}) = \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(x^{(i)})}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\theta}(z \vert x^{(i)}) = \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(x^{(i)})}&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;p_{\theta}(x^{(i)}) = \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(z \vert x^{(i)})} -(4)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;p_{\theta}(x^{(i)}) = \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(z \vert x^{(i)})} -(4)&lt;/script&gt;

&lt;p&gt;(3)식에 (4)를 넣어서 다시 쓰면 다음과 같습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z\sim q_{\phi}(z \vert x^{(i)})}[log \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(z \vert x^{(i)})}] - (5)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z\sim q_{\phi}(z \vert x^{(i)})}[log \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(z \vert x^{(i)})}] - (5)&lt;/script&gt;

&lt;p&gt;그 다음에 expectation 안의 항에 같은 값을 곱하고 나눕니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z\sim q_{\phi}(z \vert x^{(i)})}[log \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(z \vert x^{(i)})}\frac{q_{\phi}(z \vert x^{(i)})}{q_{\phi}(z \vert x^{(i)})}] - (6)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z\sim q_{\phi}(z \vert x^{(i)})}[log \frac{p_{\theta}(x^{(i)} \vert z)p_{\theta}(z)}{p_{\theta}(z \vert x^{(i)})}\frac{q_{\phi}(z \vert x^{(i)})}{q_{\phi}(z \vert x^{(i)})}] - (6)&lt;/script&gt;

&lt;p&gt;이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z)&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/script&gt;를 하나로 묶고 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z \vert x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z \vert x^{(i)})&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/script&gt;를 하나로 묶어서 별도의 expectation으로 내보냅니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - \mathbb{E}_{z}[log\frac{q_{\phi}(z \vert x^{(i)})}{p_{\theta}(z)}] + \mathbb{E}_{z}[log\frac{q_{\phi}(z \vert x^{(i)})}{p_{\theta}(z \vert x^{(i)})}] - (7)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - \mathbb{E}_{z}[log\frac{q_{\phi}(z \vert x^{(i)})}{p_{\theta}(z)}] + \mathbb{E}_{z}[log\frac{q_{\phi}(z \vert x^{(i)})}{p_{\theta}(z \vert x^{(i)})}] - (7)&lt;/script&gt;

&lt;p&gt;(7)식에서 우변을 살펴보겠습니다. 우변의 두번째 항과 세번째 항은 잘 보면 KL-Divergence의 형태인 것을 알 수 있습니다. 따라서 KL의 형태로 바꿔쓰면 다음과 같습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z)) + D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z \vert x^{(i)}))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;log p_{\theta}(x^{(i)}) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z)) + D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z \vert x^{(i)}))&lt;/script&gt;

&lt;p&gt;이 식의 의미를 살펴보면 다음과 같습니다. 우변의 첫번째 항은 reconstruction을 의미합니다. &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/script&gt;로부터 sampling한 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;가 있으며 그 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;를 가지고  &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x^{(i)} \vert z)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x^{(i)} \vert z)&lt;/script&gt;가 &lt;code class=&quot;MathJax_Preview&quot;&gt;x^{(i)}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x^{(i)}&lt;/script&gt;를 생성할 log likelihood입니다. 우변의 두번째 항은 prior인 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;와 근사된 posterior인 &lt;code class=&quot;MathJax_Preview&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;q_{\phi}(z \vert x^{(i)})&lt;/script&gt; 사이의 KL-divergence 입니다. 즉, 근사된 posterior의 분포가 얼마나 normal distribution과 가깝운지에 대한 척도입니다(prior를 normal distribution으로 가정했을 때). 마지막 항은 원래의 posterior와 근사된 posterior의 차이로서 approximation error라고 볼 수 있습니다. 하지만 앞에서 살펴봤듯이 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(z \vert x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(z \vert x^{(i)})&lt;/script&gt;는 intractable 해서 세번째 항을 계산하기 어렵습니다. 하지만 KL의 성질대로 이 세번째 항은 무조건 0보다 크거나 같습니다. 
 &lt;img src=&quot;https://www.dropbox.com/s/ph4mzl3un2ai0dx/Screenshot%202018-06-20%2013.47.01.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt; 
따라서 첫번째 항과 두번째 항을 하나로 묶어주면 원래의 objective function에 대한 tractable한 lower bound를 정의할 수 있습니다. MLE 문제를 풀기 위해 objective function을 미분해서 gradient ascent 할 것입니다. Lower bound가 정의된다면 이 lower bound를 최대화하는 문제로 바꿀 수 있고 결국 lower bound의 gradient를 구하게 될 것입니다. lower bound의 두 항은 모두 미분가능하기 때문에(어떻게 미분가능한건지는 뒤에서 살펴보겠습니다) 이제 우리는 최적화를 할 수 있습니다. 
&lt;img src=&quot;https://www.dropbox.com/s/6wm7uf3nejsp21t/Screenshot%202018-06-20%2013.57.12.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
lower bound를 다시 정의하자면 다음과 같습니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))&lt;/script&gt;

&lt;p&gt;이 lower bound 식은 evidence의 log 값인 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x^{(i)})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x^{(i)})&lt;/script&gt;의 lower bound이기 때문에 Evidence Lower Bound, ELBO라고 부릅니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;log(p_{\theta}(x^{(i)})) \geq \mathcal{L}(x^{(i)}, \theta, \phi)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;log(p_{\theta}(x^{(i)})) \geq \mathcal{L}(x^{(i)}, \theta, \phi)&lt;/script&gt;

&lt;p&gt;따라서 원래 &lt;code class=&quot;MathJax_Preview&quot;&gt;p_{\theta}(x)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;p_{\theta}(x)&lt;/script&gt;를 최대화하는 문제는 다음과 같이 바뀝니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\theta^*, \phi^* = argmax_{\theta, \phi}\sum_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta^*, \phi^* = argmax_{\theta, \phi}\sum_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)&lt;/script&gt;

&lt;p&gt;여기까지의 식을 한 번에 보자면 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/2ro3zzf3dgo2v31/Screenshot%202018-06-20%2014.09.01.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
이 ELBO를 구하는 과정은 다음 그림을 통해 이해해볼 수 있습니다. x를 encoder의 input으로 집어넣으면 encoder는 latent space 상에서의 mean과 variance를 내보냅니다(이 때, mean과 variance는 latent vector의 dimension마다 하나씩입니다). 그러면 이 mean과 variance가 posterior를 나타내게 되고 prior와의 KL을 구할 수 있습니다. 그 이후에 &lt;code class=&quot;MathJax_Preview&quot;&gt;z&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;로부터 decoder는 data의 space 상의 mean과 variance를 내보냅니다(만약 decoder의 output을 gaussian이라고 가정했다면. Bernoulli 분포라고 가정했다면 다른 형태). 그러면 ELBO의 첫번째 항 값을 구할 수 있고 ELBO가 구해집니다. 구한 값에 Backprop을 해서 업데이트하면 VAE의 학습과정이 완성됩니다.&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/geesw2b5yt21bx7/Screenshot%202018-06-20%2014.10.27.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
하지만 코드로 짜기에는 ELBO의 두 항을 구하는 과정이 명확하지 않습니다. 이 과정은 VAE의 원논문인 “Auto-Encoding Variational Bayes” 논문에 나와있습니다. 따라서 다음 포스트에서는 논문의 내용을 살펴보도록 하겠습니다.&lt;/p&gt;
</description>
        <pubDate>Tue, 19 Jun 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/06/19/vae/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/06/19/vae/</guid>
        
        <category>dl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>A Natural Policy Gradient 보충자료</title>
        <description>&lt;h1 id=&quot;natural-policy-gradient-보충-자료&quot;&gt;Natural Policy Gradient 보충 자료&lt;/h1&gt;

&lt;h2 id=&quot;1-cs294의-자료&quot;&gt;1. CS294의 자료&lt;/h2&gt;
&lt;p&gt;cs294에서도 NPG에 대한 내용을 소개한다. cs294 lecture 13에 해당하는 내용이다. 링크는 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;링크: &lt;a href=&quot;http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf&quot;&gt;http://rail.eecs.berkeley.edu/deeprlcourse-fa17/f17docs/lecture_13_advanced_pg.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;사실 이 피피티를 이해하려면 TRPO에 대한 내용도 알아야한다. 여기서 말하는 NPG는 Natural Policy Gradient에서 이야기하는 알고리즘이라기보다는 TRPO 저자 혹은 강의의 강사가 이야기하는 NPG이다. surrogate loss function이 들어가며 step size에 대한 구체적인 수식도 집어넣었다. 이 부분들을 감안하더라도 이 부분은 NPG에 대한 이해를 돕기 때문에 살펴보는 것에 의미가 있다.&lt;/p&gt;

&lt;h3 id=&quot;11-첫번째-페이지&quot;&gt;1.1 첫번째 페이지&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/mp134cb8kl6lkdb/Screenshot%202018-06-09%2000.04.04.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;여기서 이야기하는 &lt;code class=&quot;MathJax_Preview&quot;&gt;L&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;L&lt;/script&gt;은 NPG 논문에서 이야기하던 &lt;code class=&quot;MathJax_Preview&quot;&gt;\eta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt;와는 다르다. 어떻게 다른지는 TRPO 논문을 참고하거나 CS294 강의를 들어보기 바란다. 일단 그 부분은 이해했다치고 이 페이지에서 하고 있는 것을 설명해보자.&lt;/p&gt;

&lt;p&gt;NPG가 하고 싶은 건 steepest direction을 찾고 그 방향대로 policy를 update하고 싶은 것이다. steepest direction이라는 것의 정의는 현재 parameter space 상의 point로부터 일정 거리만큼 움직였을 때, 그 중 가장 작은 함수 값을 가지는 지점으로 가는 방향이다. 이 때, 일정한 거리라는 것을 parameter space 상에서 정의하기 위해 coordinate 변화에 invariant한 metric을 사용해서 나타냈었다. 즉 다음과 같은 식으로 나타낼 수 있다는 것이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\vert d\theta \vert^2=\sum_{ij}(\theta)d\theta_id\theta_i=d\theta^TG(\theta)d\theta&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vert d\theta \vert^2=\sum_{ij}(\theta)d\theta_id\theta_i=d\theta^TG(\theta)d\theta&lt;/script&gt;

&lt;p&gt;여기서는 결과적으로는 같지만 다른 방식으로 표현한다. optimize하고자 하는 함수에 대해서는 1st order로 local approximation을 취한다. 그리고 일정한 거리라는 것을 표현하기 위해 distribution의 변화를 의미하는 KL-divergence를 가져온다. 이 KL-divergence는 2nd-order로 local approximation한다. 현재 parameter space상의 점에서는 KL-divergence의 미분값이 0이 되므로 결국 2차 미분인 Hessian 항만 남는다. 그게 다음 수식이 의미하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/g48q5fsgw72h6tb/Screenshot%202018-06-09%2000.48.33.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제는 KL-divergence constraint가 있는 optimization 문제이지만 lagrange multiplier를 사용해서 penalty 문제로 전환할 수 있다. 그게 다음 부분이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/kkxwdmkna0gn72r/Screenshot%202018-06-09%2000.48.03.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 문제를 다 풀고 다면 다음과 같은 update rule을 구할 수 있다. 어떻게 구하는지는 이제 살펴보자.
&lt;img src=&quot;https://www.dropbox.com/s/8e2t1e46mneg7tf/Screenshot%202018-06-09%2000.49.40.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-두번째-페이지&quot;&gt;1.2 두번째 페이지&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/vzaerazk3dq9425/Screenshot%202018-06-09%2000.05.18.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 페이지에서 놀라운 사실은 Fisher Information Matrix가 사실 KL divergence의 Hessian이라는 사실이다!!. 그동안 왜 Fisher를 쓰는지 고민이 많았는데 이 부분을 보니까 갑자기 이해가 되는 듯 하다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://math.stackexchange.com/questions/2239040/show-that-fisher-information-matrix-is-the-second-order-gradient-of-kl-divergenc/2239159&quot;&gt;다음 글&lt;/a&gt;에서도 FIM이 KL-divergence의 hessian과 동일하다(&lt;code class=&quot;MathJax_Preview&quot;&gt;\theta'&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta'&lt;/script&gt;이 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;일 경우에 그렇다. Taylor expansion을 통해서 나오는 Hessian은 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;위치에서의 Hessian 값이다.)라고 이야기한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/mwrnf0c8rc2alyw/Screenshot%202018-06-09%2007.57.12.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 NPG에서 중요한 말인 “covariant”가 나온다. natural gradient &lt;code class=&quot;MathJax_Preview&quot;&gt;H^{-1}g&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H^{-1}g&lt;/script&gt;는 축의 변화에 invariant한 steepest direction을 가리킨다(이게 NPG가 하고 싶은 일이다). 이 다음부터는 이게 왜 covariant한지에 대한 증명이 나온다. 괴롭더라도 한 번 이해해놓으면 좋다.&lt;/p&gt;

&lt;h3 id=&quot;13-세번째-페이지&quot;&gt;1.3 세번째 페이지&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/pujcolzzuie1gvx/Screenshot%202018-06-09%2000.05.52.png?dl=1&quot; /&gt;
“covariant”하다는 것의 의미를 설명하는 부분이다. covariant를 이야기하기 위해 리만 공간에서의 거리(distance)를 이야기한다. 리만 공간에서 &lt;code class=&quot;MathJax_Preview&quot;&gt;v&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;v+\delta v&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;v+\delta v&lt;/script&gt;사이의 거리는 다음과 같이 표현할 수 있다. 이 때, G를 metric tensor라고 이야기한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/acrieq6cvo42ldf/Screenshot%202018-06-09%2008.02.10.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;좌표계가 변함에 따라 거리라는 양은 변하지 않아야한다. 다음 예시를 살펴보자. 일반적인 Cartesian coordinate와 polar coordinate에서의 거리를 이야기해보자. Cartesian coordinate에서 metric tensor는 identity matrix이다. (거리를 &lt;code class=&quot;MathJax_Preview&quot;&gt;dx^2+dy^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;dx^2+dy^2&lt;/script&gt;으로 나타낸다는 말이다) Cartesian coordinate에서의 x, y는 다음과 같이 &lt;code class=&quot;MathJax_Preview&quot;&gt;r, \theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;r, \theta&lt;/script&gt;에 대해서 나타낼 수 있다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\delta x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta x&lt;/script&gt;로 미분할 때는 &lt;code class=&quot;MathJax_Preview&quot;&gt;x&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;를 &lt;code class=&quot;MathJax_Preview&quot;&gt;r&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt;과 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;로 각각 편미분해서 더하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/6opt0zgyp6tn41g/Screenshot%202018-06-09%2008.08.33.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Cartesian coordinate에서의 거리는 각 component의 제곱의 합이다. 이것을 Polar coordinate로 변환한다. 그러면 Polar coordinate에서의 metric tensor를 구할 수 있다. 이는 특수한 경우에 “거리”라는 개념이 coordinate에 따라 invariant 하다라는 것을 보인것이다. 이 때, “metric tensor”라는 것이 각 coordinate마다 거리를 계산하는데 사용된다. 즉, 이 metric tensor가 coordinate의 변화에 invariant한 metric이 되어야한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/yo76t1a5ojxby7o/Screenshot%202018-06-09%2008.10.54.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;14-네번째-페이지&quot;&gt;1.4 네번째 페이지&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/ffgp47bobvreplc/Screenshot%202018-06-09%2000.06.28.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 gradient의 covariant에 대해 생각해보자. natural gradient가 정말 covariant한가? 같은 벡터를 다른 system에서 다른 coordinate로 표현한 경우를 생각한다. 이는 다음과 같이 표현할 수 있다. 같은 벡터이며 같은 거리를 움직였다고 생각해보자.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/zajf90pytb2ho1a/Screenshot%202018-06-09%2008.16.53.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이제 v의 미분을 생각해보자. 단순한 chain rule을 통해 다음과 같이 쓸 수 있다. 
&lt;img src=&quot;https://www.dropbox.com/s/vhaw3kcn123hj7n/Screenshot%202018-06-09%2008.20.07.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;두 개의 system에서 &lt;code class=&quot;MathJax_Preview&quot;&gt;\delta v&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta v&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;\delta w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta w&lt;/script&gt;는 같다고 가정을 하였다. 앞에서 이야기한 거리의 정의를 가져와서 쓰면 다음과 같다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\delta v&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta v&lt;/script&gt;를 &lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;에 대해서 치환한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/129o7f00c60ww56/Screenshot%202018-06-09%2008.22.36.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 때, 함수 f의 w에 대한 gradient는 다음과 같다.
&lt;img src=&quot;https://www.dropbox.com/s/cgmh1htfz36vtv8/Screenshot%202018-06-09%2008.25.16.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;15-다섯번째-페이지&quot;&gt;1.5 다섯번째 페이지&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/k606vh5iqv1jdn4/Screenshot%202018-06-09%2000.06.57.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;distance에 대한 식과 gradient 식을 사용해서 natural gradient가 covariant임을 보이자. NPG에서 봤듯이 natural gradient는 다음과 같이 쓸 수 있다. 두 개의 natural gradient가 같다면 covariant라고 할 수 있다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/ojp2nih0gwt98pp/Screenshot%202018-06-09%2008.26.42.png?dl=1&quot; width=&quot;200px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;이는 다음과 같이 증명할 수 있다. 만약 서로 다른 두 coordinate에 대해 natural gradient가 같다면 같은 &lt;code class=&quot;MathJax_Preview&quot;&gt;\delta v&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta v&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;\delta w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\delta w&lt;/script&gt;의 관계식을 만족할 것이다. 실제로 만족한다! 즉, natural gradient는 covariant한 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/a9o2c35hl5acfk6/Screenshot%202018-06-09%2008.30.26.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그렇다면 왜 Natural Policy Gradient에서는 Natural gradient가 covariant하지 않았던 걸까? 논문에서는 metric tensor의 문제라고 했는데 어떤 문제인걸까? 그것을 여기의 수식과 어떻게 연결할 수 있을까?&lt;/p&gt;

&lt;h3 id=&quot;16-여섯번째-페이지&quot;&gt;1.6 여섯번째 페이지&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/b9jd2rzww6wha1b/Screenshot%202018-06-09%2000.07.32.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이것이 NPG의 알고리즘이다. 마지막 식이 나오는 과정은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/taryh1wvqsqi1tx/Screenshot%202018-06-09%2018.03.36.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/uook5mwpwgf7q89/Screenshot%202018-06-09%2018.04.07.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/deh443s2xilfoc5/Screenshot%202018-06-09%2018.04.24.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;2-newtons-method&quot;&gt;2. Newton’s Method&lt;/h2&gt;
&lt;p&gt;–&lt;/p&gt;

&lt;p&gt;optimize할 함수를 2nd order로 approximation해서 optimize하는 방법이다. Natural Gradient 방법론에서는 1nd order로 objective funciton을 approximation 했다는 것을 기억하자. Newton’s method에서는 Hessian이 positive이면 convex 함수가 되어서 최소값을 구할 수 있다. 그러면 parameter를 이 최소값 지점으로 업데이트를 하고 이 과정을 반복하면 gradient 방법론 보다 훨씬 빠르게 수렴할 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton's%20Method.pdf&quot;&gt;ppt 출처&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/2dhqtkindfqzsdx/Screenshot%202018-06-10%2011.09.50.png?dl=1&quot; /&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 08 Jun 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/06/08/npg-suppliment/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/06/08/npg-suppliment/</guid>
        
        <category>rl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>Model-Ensemble Trust-Region Policy Optimization</title>
        <description>&lt;h1 id=&quot;model-ensemble-trust-region-policy-optimization-2018&quot;&gt;Model-Ensemble Trust-Region Policy Optimization [2018]&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/jnmnpkwpeq47ugi/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.21.05.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Thanard Kurutach(Berkely AI Research)&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://openreview.net/pdf?id=SJJinbWRZ&quot;&gt;https://openreview.net/pdf?id=SJJinbWRZ&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;함께 보면 좋을 논문:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1502.05477.pdf&quot;&gt;Trust Region Policy Optimization (2015)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;논문을 보는 이유: TRPO와 관련이 있을 것 같아서(결국은 TRPO에 대한 이야기는 거의 없다)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;개인적인 생각들로 재해석해서 요약한 거라고 생각하시면 될 것 같습니다(부정확할 수 있다는 이야기입니다).&lt;/p&gt;

&lt;h2 id=&quot;1-abstract&quot;&gt;1. Abstract&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;최근 많은 task에서 model-free RL은 성공적이었다. 하지만 실제 세상에 적용하기에는 학습 데이터가 너무 많이 필요하다.&lt;/li&gt;
  &lt;li&gt;model-based RL은 적은 학습 데이터로 학습 가능하다. 하지만 비교적 간단한 task에서만 잘 작동했었다.&lt;/li&gt;
  &lt;li&gt;model-based RL에서 DNN으로 model과 policy를 둘 다 학습하면 문제가 생긴다. model을 학습하기에 충분한 데이터가 없는 region에서 policy가 exploit하는 경향이 있는 것이다.&lt;/li&gt;
  &lt;li&gt;이런 문제를 해결하기 위해서 여러개의 모델(환경에 대한 모델)을 ensemble로 사용할건데 그러면 model uncertainty를 유지할 수 있으며 regularization 효과를 볼 수 있다.&lt;/li&gt;
  &lt;li&gt;model-based RL에서 backprop through time을 사용하지 않고 likelihood ratio derivative를 사용할 것이다. 그 말인즉슨 TRPO를 사용해서 policy를 학습할거라는 이야기.&lt;/li&gt;
  &lt;li&gt;이러한 알고리즘을 Model-Ensemble Trust-Region Policy Optimization(ME-TRPO)이라고 부를 것이다.&lt;/li&gt;
  &lt;li&gt;continuous control task에서 model-free RL에 비해 ME-TRPO가 더 적은 데이터로도 비슷한 성능을 내도록 학습했다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-model-based-vs-model-free-rl&quot;&gt;2. model-based vs model-free RL&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;결국 현실 세계에 강화학습을 적용하려면 학습에 들어가는 cost가 중요하다. 보통 사용하는 model-free RL은 제대로된 policy를 학습하려면 많은 데이터가 필요하다. 즉, 많은 시행착오가 필요하다는 것이고 이것이 bottle neck이 된다. 그래서 model-based RL에 대한 연구가 진행이 되어왔다. 간단히 세상에 대한 model을 학습한다고 생각하면 된다. 환경과의 직접적인 상호작용으로 policy를 학습하지 않고 에이전트가 학습한 model에서 상호작용해서 데이터를 모아서 policy를 학습한다. 이 때 환경의 model은 transition probability라고 생각하면 된다. 어떤 상태에서 어떤 행동을 했을 때 다음 어떤 상태가 될지에 대한 정보이다. 이 정보가 있으면 에이전트는 행동을 선택한 다음 다음 상태를 바로 알 수 있다. 그러면 또 다음 상태에서 행동을 선택하는 식으로 일종의 “imagination”이 가능하다고 볼 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;3-vanilla-model-based-rl&quot;&gt;3. vanilla model-based RL&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;31-model-learning&quot;&gt;3.1 model learning&lt;/h3&gt;
&lt;p&gt;이 때, model이 처음에는 부정확하기 때문에 적당히 model도 꾸준히 학습해야한다. 그래서 보통 vanilla model-based RL에서는 model learning과 policy learning(혹은 policy optimization)을 반복해서 진행했다. model learning은 지도학습이라고 생각하면 된다. 현재의 policy를 가지고 환경에서 상호작용하면서 데이터를 모은다. 데이터는 (s, a, s’)이라고 생각하면 된다. 이 데이터를 가지고 model(이 논문에서는 DNN을 사용한다)이 supervised learning으로 학습한다. (s, a)를 input으로 넣으면 s’을 모델이 출력하도록 학습하는 것이다.&lt;/p&gt;

&lt;p&gt;다음 상태 s’을 그대로 출력하도록 뉴럴넷을 학습하면 문제가 있다. s와 s’이 별 차이가 없는 경우 뉴럴넷은 그냥 s를 외워버린다. 따라서 s’이 아닌 상태의 변화인 (s’-s)를 예측하도록 모델을 학습한다. 환경의 모델을 학습하는 loss function은 다음과 같다. &lt;code class=&quot;MathJax_Preview&quot;&gt;f_{\phi}(s_t, a_t)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f_{\phi}(s_t, a_t)&lt;/script&gt;는 원래 상태 s에다가 뉴럴넷 아웃풋을 합친 것을 뜻한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/bs66h3itve37yya/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.45.56.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-policy-learning&quot;&gt;3.2 policy learning&lt;/h3&gt;
&lt;p&gt;보통 강화학습의 목표는 expected sum of rewards를 최대화하는 것이다. model-based RL이므로 환경을 approximate model이 있고 이 model을 base로 하는 approximate MDP가 있다. 이 MDP 상에서의 expected sum of rewards를 최대화하도록 policy를 optimize 한다.&lt;/p&gt;

&lt;p&gt;다음은 policy optimization의 objective function이다. 
&lt;img src=&quot;https://www.dropbox.com/s/q31swo1cmcfh69v/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.49.49.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;stochastic policy는 다음 논문의 본문에서와 같이 reparameterization trick을 사용해서 나타낸다. 평균과 분산 그리고 노이즈를 통해 gaussian distribution을 나타낸다. 이렇게 나타내는 정책의 형태는 continuous action에 대한 policy라는 것을 기억하자.
&lt;img src=&quot;https://www.dropbox.com/s/gk258p80ygvozcm/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.50.32.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;policy gradient는 다음과 같다. 이 미분을 하려면 backpropagation through time (BPTT)를 해야한다. 이 미분은 문제가 있다. gradient가 explode 하거나 vanish 할 수 있다는 것이다. 그래서 gradient clipping을 사용했다.
&lt;img src=&quot;https://www.dropbox.com/s/nco6his3ic7bbgo/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.53.15.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;model learning과 policy learning을 합쳐서 하나의 알고리즘으로 보자면 다음과 같다. real world에서 데이터를 모아서 model를 학습한 다음에 그 model을 사용해서 policy를 improve 한다. 
&lt;img src=&quot;https://www.dropbox.com/s/y269zvjofxq9jja/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.56.01.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-me-trpo&quot;&gt;4. ME-TRPO&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;41-vanilla-model-based-rl의-문제&quot;&gt;4.1 vanilla model-based RL의 문제&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;learned policy often exploits regions where scarce training data is available for the dynamics model&lt;/li&gt;
  &lt;li&gt;the predictions can be erroneous to the policy’s advantage –&amp;gt; overfitting&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;즉 model이 부정확해서 overfitting 된다는 것이다. 지도학습에서와 마찬가지로 이는 regularization의 필요를 의미한다.&lt;/p&gt;

&lt;h3 id=&quot;42-me-trpo&quot;&gt;4.2 ME-TRPO&lt;/h3&gt;
&lt;p&gt;이 논문에서는 regularization 방법으로 환경의 모델을 여러개 학습해서 ensemble로 사용하는 방법을 제안한다. 여러 모델이 서로 다른 것은 initial parameter이다. 또한 BPTT를 통해 학습하는 것이 아니라 TRPO를 통해 policy를 학습함으로서 policy optimization 과정도 개선했다. 또한 policy의 성능을 평가할 때도 여러개의 모델을 사용한다. 다음과 같이 K 개의 모델에 대해서 policy가 improve 된건지를 체크해서 70%가 improve되었다면 다음 단계로 넘어간다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/fvnzjllmcwbxdbn/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.05.58.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체 알고리즘은 다음과 같다. 심플하다.
&lt;img src=&quot;https://www.dropbox.com/s/5i1ff8akq0p05cr/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.08.28.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;5-experiment&quot;&gt;5. Experiment&lt;/h2&gt;
&lt;h3 id=&quot;51-무슨-실험&quot;&gt;5.1 무슨 실험?&lt;/h3&gt;
&lt;p&gt;세가지 실험을 했다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기존 model-free RL 중에 제일 잘 되는 놈이랑 성능 비교&lt;/li&gt;
  &lt;li&gt;기존 model-based RL이 어떻게 실패하는지 테스트, 같은 상황에서 ME-TRPO가 잘 하는지 테스트&lt;/li&gt;
  &lt;li&gt;성능에 ensemble로 사용하는 모델 개수가 미치는 영향&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;환경은 다음과 같은 전형적인 continuous control 환경을 사용했다.
&lt;img src=&quot;https://www.dropbox.com/s/8mlj27eu5jzga2t/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.11.38.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;52-실험-결과&quot;&gt;5.2 실험 결과&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;기존 model-free RL 중에 제일 잘 되는 놈이랑 성능 비교. 꽤나 잘 된다.
&lt;img src=&quot;https://www.dropbox.com/s/qaion8lqhmyegac/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.12.44.png?dl=1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;기존 model-based RL이 어떻게 실패하는지 테스트&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;같은 상황에서 ME-TRPO가 잘 하는지 테스트
&lt;img src=&quot;https://www.dropbox.com/s/b7xgibgnurl234a/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.13.50.png?dl=1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;성능에 ensemble로 사용하는 모델 개수가 미치는 영향
&lt;img src=&quot;https://www.dropbox.com/s/wlzrl9rxmp8hr5b/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.14.50.png?dl=1&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;6-discussion&quot;&gt;6. Discussion&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;여러개의 challengin domain에서 학습하는 간단하고 안정적인 model-based RL 알고리즘 = ME-TRPO. 핵심은 Model을 ensemble로 학습해서 policy 학습한 것과 policy 학습할 때 TRPO를 쓴 것.&lt;/li&gt;
  &lt;li&gt;model-free RL의 state-of-art랑 비교했을 때 sample complexity를 상당히 줄임, 동일한 performance&lt;/li&gt;
  &lt;li&gt;model bias를 model의 uncertainty를 유지함으로서 줄임&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[앞으로 가능한 연구]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;서로 다른 model이 disagree하는 state space를 더 탐험하도록 하는 것&lt;/li&gt;
  &lt;li&gt;ME-TRPO를 real world에 적용하는 것&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 30 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/05/30/me-trpo/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/05/30/me-trpo/</guid>
        
        <category>rl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>TRUST-PCL: An Off-policy Trust Region Method for Continuous Control</title>
        <description>&lt;h1 id=&quot;trust-pcl-an-off-policy-trust-region-method-for-continuous-control-2018&quot;&gt;TRUST-PCL: An Off-policy Trust Region Method for Continuous Control [2018]&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/nubod5d36dpy6yd/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.01.01.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Ofir Nachum(Google Brain)&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://openreview.net/pdf?id=HyrCWeWCb&quot;&gt;https://openreview.net/pdf?id=HyrCWeWCb&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;함께 보면 좋을 논문:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1502.05477.pdf&quot;&gt;Trust Region Policy Optimization (2015)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1702.08892&quot;&gt;Bridging the Gap Between Value and Policy Based Reinforcement Learning(2017)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;논문을 보는 이유: TRPO의 어떤 문제가 있고 그걸 어떤 방법으로 해결하고자 했는지 궁금해서&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-abstract&quot;&gt;1. Abstract&lt;/h2&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Trust region 최적화 방법은 policy optimization 과정을 안정화하기 위해서 자주 사용된다.&lt;/li&gt;
  &lt;li&gt;하지만 TRPO와 같은 Trust Region을 활용한 알고리즘들은 on-policy 알고리즘이기 때문에 학습 데이터가 많이 필요하다.&lt;/li&gt;
  &lt;li&gt;이 문제를 해결하기 위해서 TRPO + PCL인 Trust-PCL 알고리즘을 제안한다.&lt;/li&gt;
  &lt;li&gt;maximum reward objective에 relative-entropy regularizer를 더한 새로운 objective의 optimal policy와 state value가 multi-step pathwise consistency를 만족한다.&lt;/li&gt;
  &lt;li&gt;relative entropy regularization은 off-policy data로부터 학습하면서도 최적화 과정이 안정적이게 해준다.&lt;/li&gt;
  &lt;li&gt;여러 continuous control 문제에서 TRPO보다 Trust-PCL이 solution의 quality와 sample efficiency가 더 좋다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[개인생각]
기존의 PPO 등이 있지만 현재 continuous control 문제에서 성능만 놓고 봤을 때 state-of-art는 TRPO이다. 이 TRPO는 on-policy로 현재 policy로 모은 데이터로만 학습을 하기 때문에 데이터가 많이 필요하다는 단점이 있다. 따라서 저자가 이전에 작성한 “Bridging the Gap Between Value and Policy Based Reinforcement Learning”논문에서 소개했던 PCL을 TRPO와 잘 융합해서 off-policy가 가능하도록 만들겠다는 것이 논문의 내용이다.&lt;/p&gt;

&lt;p&gt;PCL의 논문 내용을 알아야하기 때문에 이 논문을 그냥 보고 이해하는 것은 어려울 것 같다. 논문에서 PCL + TRPO에 대해서는 잘 설명하다가 off-policy가 가능한 이유에 대해서는 그닥 잘 설명하는 느낌을 못 받았다. 아마 PCL 논문에 나와있을 것 같다(PCL 자체가 on-policy로 모은 데이터로도 학습할 수 있고 off-policy로 모은 데이터로도 학습할 수 있다고 한다.&lt;/p&gt;

&lt;p&gt;논문이 제시하는 방법은 잘 이해못하더라도 논문의 related work 부분에서 TRPO에 대해서 정리해주는 점은 참고할만 한 것 같다.&lt;/p&gt;

&lt;h2 id=&quot;2-conclusion&quot;&gt;2. Conclusion&lt;/h2&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;Trust-PCL을 제안한다. 한 문장으로 표현하자면 다음과 같다. “an off-policy algorithm employing a relative-entropy penalty to impose
a trust region on a maximum reward objective” (핵심은 “off-policy” “relative-entropy penalty” “trust region” 이라고 볼 수 있을 듯)&lt;/li&gt;
  &lt;li&gt;TRPO보다 Trust-PCL이 average reward도 더 좋고 sample-efficiency도 더 좋다.&lt;/li&gt;
  &lt;li&gt;그러므로 stability와 sample efficiency 두 마리 토끼를 다 잡았다.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-experiment&quot;&gt;3. Experiment&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;테스트 환경: Acrobot, HalfCheetah, Swimmer, Hopper, Walker2d, and Ant&lt;/li&gt;
  &lt;li&gt;TRPO는 한 번 업데이트하기 위해 25,000 개의 sample을 모았다.&lt;/li&gt;
  &lt;li&gt;Trust-PCL은 off-policy 이므로 sample을 모으는 것과 학습을 번갈아가면서 했다. 10 step 마다 한 번씩 64개의 mini-batch를 replay memory에서 추출해서 학습했다.&lt;/li&gt;
  &lt;li&gt;이 때, replay memory에서 더 최근에 더한 sample일수록 priority를 줬다.&lt;/li&gt;
  &lt;li&gt;다음 그래프에서 보듯이 Trust-PCL은 TRPO보다 점수도 더 높고 더 빠르게 학습하는 경향이 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/9vtr0k4vbyo5ct1/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.49.24.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다음 그래프는 on-policy Trust-PCL와 off-policy Trust-PCL의 차이를 보여준다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/l7zzu19shiv3jc5/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.57.25.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다음 표는 여러 알고리즘의 최종 성능을 비교한다. 10M step을 학습한 결과이다. Trust-PCL이 Walker 빼고 다 점수가 제일 높다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/faolncuwyza76uj/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2011.59.36.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-related-work&quot;&gt;4. Related Work&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;일단 gradient descent에 대해서 다음과 같이 정의한다. natural policy gradient 논문을 읽었다면 알겠지만 이것은 Euclidean space 일 때 성립하는 식이다. 한 가지 주목할 점은 그냥 gradient descent 또한 trust region으로 constraint가 주어진 optimization으로 본다는 것이다.
&lt;img src=&quot;https://www.dropbox.com/s/lh9n44gpwncpi50/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2012.01.17.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;보통 뉴럴넷을 사용할 경우 Euclidean geometry 보다는 Riemannian geometry가 더 적합하다. 이 경우에 loss surface에 대해서 Riemannian metric를 정의하는 것이 좋은데 이 metric을 통해 local에서의 steep descent direction을 구할 수 있다. Natural gradient를 이용할 경우 이 metric은 Fisher information matrix가 된다. FIM은 conditional probability model의 log-likelihood를 최적화할 때 유용하다 (&lt;code class=&quot;MathJax_Preview&quot;&gt;\nabla log \pi_{\theta}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\nabla log \pi_{\theta}&lt;/script&gt;의 형태를 떠올려보면 무슨 말인지 대충 느낌이 올 것이다). 다음은 Natural Policy Gradient에 대한 내용이다. Bregman divergence는 처음 듣는다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/zx6jfob1235brl2/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2012.08.43.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결국 다음 식을 보면 natural policy gradient에서 하고 있는 것은 KL-divergence를 fisher information matrix를 통해 approximation하고 있는 것이다.
&lt;img src=&quot;https://www.dropbox.com/s/66td1itm57s3lqv/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-28%2012.10.15.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;TRPO는 natural gradient에 몇 개의 approximation을 더해서 nonlinear policy optimization을 할 수 있도록 만들었다.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;이 이외의 내용은 PCL에 대한 이해를 필요로 한다. 후에 만약 PCL 논문을 읽게 되면 다시 읽어보면 좋을 것 같다.&lt;/p&gt;
</description>
        <pubDate>Mon, 28 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/05/28/trust-pcl/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/05/28/trust-pcl/</guid>
        
        <category>rl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>A Natural Policy Gradient</title>
        <description>&lt;h1 id=&quot;a-natural-policy-gradient-2001&quot;&gt;A Natural Policy Gradient [2001]&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/it82tfhfmhg9uwp/Screenshot%202018-06-10%2010.58.52.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;논문 저자: Sham Kakade&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf&quot;&gt;https://papers.nips.cc/paper/2073-a-natural-policy-gradient.pdf&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;함께 보면 좋을 논문:
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;hhttps://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf&quot;&gt;Policy Gradient Methods for
Reinforcement Learning with Function
Approximation (2000)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.452.7280&amp;amp;rep=rep1&amp;amp;type=pdf&quot;&gt;Natural Gradient Works Efficiently in Learning(1998)&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;논문을 보는 이유: TRPO와 NPG는 관련이 많기 때문에 TRPO를 더 잘 이해하기 위해 봄&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-abstract&quot;&gt;1. Abstract&lt;/h2&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;natural gradient method를 policy gradient에 적용&lt;/li&gt;
  &lt;li&gt;natural gradient는 steepest descent direction을 가짐&lt;/li&gt;
  &lt;li&gt;gradient descent는 parameter를 한 번에 많이 update 할 수 없는 반면, natural gradient는 가장 좋은 action을 고르도록 학습이 됌 (sutton 논문에서와 같이 compatible value function을 사용할 경우 policy iteration에서 policy improvement 1 step의 과정에서)&lt;/li&gt;
  &lt;li&gt;simple MDP와 tetris MDP에서 테스트함. 성능이 많이 향상&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-personal-interpretation-and-thinking&quot;&gt;2. Personal Interpretation and Thinking&lt;/h2&gt;
&lt;p&gt;(개인생각) 뉴럴넷을 사용할 경우 gradient가 steepest direction이 아닌 경우가 많다. 뉴럴넷의 parameter space가 우리가 보통 생각하는 직선으로 쭉쭉 뻗어있는 Euclidean space가 아니다. 좀 더 일반적으로는 구의 표면과 같이 휘어져있는 공간 즉, 리만 공간(Riemannian space)로 표현할 수 있다. 이와 같은 공간에서는 natural gradient가 steepest direction이 된다는 연구가 이뤄지고 있었다. 강화학습의 policy gradient은 objective function의 gradient를 따라 policy를 업데이트한다. 이 때, policy는 parameterized 되는데 이 경우에도 gradient 대신에 natural gradient가 좋다는 것을 실험해보는 논문이다.&lt;/p&gt;

&lt;p&gt;gradient가 non-covariant 해서 생기는 문제는 간단히 말하자면 다음과 같다. policy가 parameterized된 상황에서는 같은 policy라도 다른 parameter를 가질 수 있다. 이 때, steepest direction은 두 경우에 같은 방향을 가리켜야하는데 non-covariant한 경우 그렇지 못하다. 이것은 결국 느린 학습으로 연결이 된다.&lt;/p&gt;

&lt;p&gt;논문에서 2차미분 방법론들과 짧게 비교를 한다. 하지만 2차미분을 이용한 다른 방법들과의 비교가 생각보다 없는 점이 아쉽다.(Hessian을 이용한다거나 conjugate gradient method를 이용한다거나). 실험을 통해 FIM이 hessian에 수렴안하는 거라던지 Hessian 방법론이 local maxima 부근에서 상당히 느리다던지의 결과를 보여줬었으면 좋았을 것 같다.&lt;/p&gt;

&lt;p&gt;또한 natural gradient 만으로 업데이트하면 policy의 improvement보장이 안될 수 있다. policy의 improvement를 보장하기 위해 line search도 써야하는데 line search를 어떻게 쓰는지에 대한 자세한 언급이 없다. 즉, 자세한 algorithm 설명이 없다.&lt;/p&gt;

&lt;p&gt;natural policy gradient 논문은 natural gradient + policy gradient를 처음 적용했다는데 의의가 있다. 하지만 이 논문이 문제 삼은 gradient는 non-covariant하다라는 문제를 natural gradient를 통해 해결하지 못했다(Experiment를 통해 covariant gradient가 되지 못했다는 것이 보인다). NPG의 뒤를 잇는 논문이 “covariant policy search”와 “natural actor-critic”에서 covariant하지 못하다는 것을 해결하기 위해 Fisher Information Matrix를 sample 하나 하나에 대해서 구하는 것이 아니라 trajectory 전체에 대해서 구한다.&lt;/p&gt;

&lt;p&gt;또한 논문은 pg의 두 가지 세팅 중에 average-reward setting(infinite horizon)에서만 NPG를 다룬다. “covariant policy search” 논문에서는 average-reward setting과 start-state setting 모두에 대해서 npg를 적용한다.&lt;/p&gt;

&lt;p&gt;natural gradient + policy gradient를 처음 제시했다는 것은 좋지만 npg 학습의 과정을 자세하게 설명하지 않았고 다른 2차 미분 방법들과 비교를 많이 하지 않은 점이 아쉬운 논문이다.&lt;/p&gt;

&lt;h2 id=&quot;3-introduction&quot;&gt;3. Introduction&lt;/h2&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;direct policy gradient method는 future reward의 gradient를 따라 policy를 update함&lt;/li&gt;
  &lt;li&gt;하지만 gradient descent는 non-covariant&lt;/li&gt;
  &lt;li&gt;이 논문에서는 covarient gradient를 제시함 = natural gradient&lt;/li&gt;
  &lt;li&gt;natural gradient와 policy iteration의 연관성을 설명하겠음: natural policy gradient is moving toward choosing a greedy optimal action (이런 연결점은 아마도 step-size를 덜 신경쓰고 싶어서 그런게 아닌가 싶다)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;논문의 Introduction 부분에 다음 멘트가 있다. 이 글만 봐서는 이해가 안갔는데 Mackay 논문에 좀 더 자세히 나와있다. 
&lt;img src=&quot;https://www.dropbox.com/s/41xhhr7lgfk24a1/Screenshot%202018-06-10%2011.45.18.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://www.inference.org.uk/mackay/ica.pdf&quot;&gt;Mackay&lt;/a&gt;논문에서는 다음과 같이 언급하고 있다. Back-propagation을 사용할 경우에 learning rate를 dimension에 1/n로 사용하면 수렴한다는 것이 증명됐다. 하지만 너무 느리다. 
&lt;img src=&quot;https://www.dropbox.com/s/us9ezc7vxgrkez6/Screenshot%202018-06-10%2011.47.21.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;4-a-natural-gradient&quot;&gt;4. A Natural Gradient&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;41-환경에-대한-설정&quot;&gt;4.1 환경에 대한 설정&lt;/h3&gt;
&lt;p&gt;이 논문에서 제시하는 학습 환경은 다음과 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;MDP: tuple &lt;code class=&quot;MathJax_Preview&quot;&gt;(S, s_0, A, R, P)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;(S, s_0, A, R, P)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt;: a finite set of states&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;s_0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_0&lt;/script&gt;: a start state&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;A&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt;: a finite set of actions&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;R&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R&lt;/script&gt;: reward function &lt;code class=&quot;MathJax_Preview&quot;&gt;R: S \times A -&amp;gt; [0, R_{max}]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R: S \times A -&gt; [0, R_{max}]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\pi(a;s, \theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi(a;s, \theta)&lt;/script&gt;: stochastic policy parameterized by &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;모든 정책 &lt;code class=&quot;MathJax_Preview&quot;&gt;\pi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi&lt;/script&gt;는 ergodic: stationary distribution &lt;code class=&quot;MathJax_Preview&quot;&gt;\rho^{\pi}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho^{\pi}&lt;/script&gt;이 잘 정의되어있음&lt;/li&gt;
  &lt;li&gt;이 논문에서는 sutton의 pg 논문의 두 세팅(start-state formulation, average-reward formulation) 중에 두 번째인 average-reward formulation을 가정&lt;/li&gt;
  &lt;li&gt;performance or average reward: &lt;code class=&quot;MathJax_Preview&quot;&gt;\eta(\pi)=\sum_{s,a}\rho^{\pi}(s)\pi(a;s)R(s,a)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta(\pi)=\sum_{s,a}\rho^{\pi}(s)\pi(a;s)R(s,a)&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;state-action value: &lt;code class=&quot;MathJax_Preview&quot;&gt;Q^{\pi}(s,a)=E_{\pi}[\sum_{t=0}^{\infty}R(s_t, a_t)-\eta(\pi)\vert s_0=s, a_0=a]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s,a)=E_{\pi}[\sum_{t=0}^{\infty}R(s_t, a_t)-\eta(\pi)\vert s_0=s, a_0=a]&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;정책이 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;로 parameterize되어있으므로 performance는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\eta(\pi_{\theta})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta(\pi_{\theta})&lt;/script&gt;인데 &lt;code class=&quot;MathJax_Preview&quot;&gt;\eta(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta(\theta)&lt;/script&gt;로 쓸거임&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;42-natural-gradient&quot;&gt;4.2 Natural Gradient&lt;/h3&gt;
&lt;h4 id=&quot;421-policy-gradient-theorem&quot;&gt;4.2.1 Policy gradient Theorem&lt;/h4&gt;
&lt;p&gt;서튼 pg 논문의 policy gradient theorem에 따라 exact gradient of the average reward는 다음과 같다. 다음 수식이 어떻게 유도되었는지, 어떤 의미인지 모른다면 서튼 pg 논문을 통해 제대로 이해하는 것이 좋다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\nabla\eta(\theta)=\sum_{s,a}\rho^{\pi}(s)\nabla\pi(a;s,\theta)Q^{\pi}(s,a)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla\eta(\theta)=\sum_{s,a}\rho^{\pi}(s)\nabla\pi(a;s,\theta)Q^{\pi}(s,a)&lt;/script&gt;

&lt;p&gt;steepest descent direction of &lt;code class=&quot;MathJax_Preview&quot;&gt;\eta(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta(\theta)&lt;/script&gt;는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\eta(\theta + d\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta(\theta + d\theta)&lt;/script&gt;를 최소화하는 &lt;code class=&quot;MathJax_Preview&quot;&gt;d\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d\theta&lt;/script&gt;로 정의된다. 이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;\vert d\theta \vert^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\vert d\theta \vert^2&lt;/script&gt;가 일정 크기 이하인 것으로 제약조건을 준다(held to small constant). Euclidian space에서는 &lt;code class=&quot;MathJax_Preview&quot;&gt;\eta(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\eta(\theta)&lt;/script&gt;가 steepest direction이지만 Riemannian space에서는 natural gradient가 steepest direction이다.&lt;/p&gt;

&lt;h4 id=&quot;422-natural-gradient-증명&quot;&gt;4.2.2 Natural gradient 증명&lt;/h4&gt;
&lt;p&gt;Riemannian space에서 거리는 다음과 같이 정의된다. &lt;code class=&quot;MathJax_Preview&quot;&gt;G(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;G(\theta)&lt;/script&gt;는 특정한 양수로 이루어진 matrix이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\vert d\theta \vert^2=\sum_{ij}(\theta)d\theta_id\theta_i=d\theta^TG(\theta)d\theta&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vert d\theta \vert^2=\sum_{ij}(\theta)d\theta_id\theta_i=d\theta^TG(\theta)d\theta&lt;/script&gt;

&lt;p&gt;이 수식은 Natural Gradient Works Efficiently in Learning 논문에서 증명되어있다. 다음은 natural gradient 증명이다.&lt;/p&gt;

&lt;p&gt;steepest direction을 구할 때 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;의 크기를 제약조건을 준다. 제약조건은 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\vert d\theta \vert^2 = \epsilon^2&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vert d\theta \vert^2 = \epsilon^2&lt;/script&gt;

&lt;p&gt;그리고 steepest vector인 &lt;code class=&quot;MathJax_Preview&quot;&gt;d\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;d\theta&lt;/script&gt;는 다음과 같이 정의할 수 있다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;d\theta = \epsilon a&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;d\theta = \epsilon a&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\vert a \vert^2=a^TG(\theta)a = 1&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vert a \vert^2=a^TG(\theta)a = 1&lt;/script&gt;

&lt;p&gt;이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;가 steepest direction unit vector이 되려면 다음 수식을 최소로 만들어야 한다. (이 수식은 잘 모르겠지만 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;에서의 1차근사를 가정하는게 아닌가 싶다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\eta(\theta + d\theta) = \eta(\theta) + \epsilon\nabla\eta(\theta)^Ta&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\eta(\theta + d\theta) = \eta(\theta) + \epsilon\nabla\eta(\theta)^Ta&lt;/script&gt;

&lt;p&gt;위 수식이 제약조건 아래 최소가 되는 &lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;를 구하기 위해 Lagrangian method를 사용한다. Lagrangian method를 모른다면 &lt;a href=&quot;https://en.wikipedia.org/wiki/Lagrange_multiplier&quot;&gt;위키피디아&lt;/a&gt;를 참고하는 것을 추천한다. 위 수식이 최소라는 것은 &lt;code class=&quot;MathJax_Preview&quot;&gt;\nabla\eta(\theta)^Ta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\nabla\eta(\theta)^Ta&lt;/script&gt;가 최소라는 것이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\frac{\partial}{\partial a_i}(\nabla\eta(\theta)^Ta - \lambda a^TG(\theta)a)=0&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial}{\partial a_i}(\nabla\eta(\theta)^Ta - \lambda a^TG(\theta)a)=0&lt;/script&gt;

&lt;p&gt;따라서 &lt;code class=&quot;MathJax_Preview&quot;&gt;(\nabla\eta(\theta)^Ta - \lambda a^TG(\theta)a)=0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;(\nabla\eta(\theta)^Ta - \lambda a^TG(\theta)a)=0&lt;/script&gt;는 상수이다. 상수를 미분하면 0이므로 이 식을 &lt;code class=&quot;MathJax_Preview&quot;&gt;a&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;a&lt;/script&gt;로 미분한다. 그러면 다음과 같다. steepest direction을 구한 것이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\nabla\eta(\theta) = 2 \lambda G(\theta)a&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla\eta(\theta) = 2 \lambda G(\theta)a&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;a=\frac{1}{2\lambda}G^{-1}\nabla\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;a=\frac{1}{2\lambda}G^{-1}\nabla\eta(\theta)&lt;/script&gt;

&lt;p&gt;이 때, 다음 식을 natural gradient라고 정의한다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\bar{\nabla}\eta(\theta) = G^{-1}\nabla\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{\nabla}\eta(\theta) = G^{-1}\nabla\eta(\theta)&lt;/script&gt;

&lt;p&gt;natural gradient를 이용한 업데이트는 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\theta_{t+1}=\theta_t - \alpha_tG^{-1}\nabla\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t+1}=\theta_t - \alpha_tG^{-1}\nabla\eta(\theta)&lt;/script&gt;

&lt;p&gt;여기까지는 natural gradient의 증명이었다. 이 natural gradient를 policy gradient에 적용한 것이 natural policy gradient이다. natural policy gradient는 다음과 같이 정의된다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\bar{\nabla}\eta(\theta) = F^{-1}\nabla\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{\nabla}\eta(\theta) = F^{-1}\nabla\eta(\theta)&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;G&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;G&lt;/script&gt; 대신 &lt;code class=&quot;MathJax_Preview&quot;&gt;F&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;를 사용했는데 &lt;code class=&quot;MathJax_Preview&quot;&gt;F&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F&lt;/script&gt;는 Fisher information matix이다. 수식은 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;F(\theta) = E_{\rho^\pi(s)}[F_s(\theta)]&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\theta) = E_{\rho^\pi(s)}[F_s(\theta)]&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;F_s(\theta)=E_{\pi(a;s,\theta)}[\frac{\partial log \pi(a;s, \theta)}{\partial \theta_i}\frac{\partial log \pi(a;s, \theta)}{\partial\theta_j}]&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;F_s(\theta)=E_{\pi(a;s,\theta)}[\frac{\partial log \pi(a;s, \theta)}{\partial \theta_i}\frac{\partial log \pi(a;s, \theta)}{\partial\theta_j}]&lt;/script&gt;

&lt;p&gt;왜 G가 F가 되는지는 아직 잘 모르겠다. 거리라는 개념을 표현하려면&lt;/p&gt;

&lt;h2 id=&quot;5-the-natural-gradient-and-policy-iteration&quot;&gt;5. The Natural Gradient and Policy Iteration&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;51-theorem-1&quot;&gt;5.1 Theorem 1&lt;/h3&gt;
&lt;p&gt;sutton pg 논문에 따라 &lt;code class=&quot;MathJax_Preview&quot;&gt;Q^{\pi}(s,a)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Q^{\pi}(s,a)&lt;/script&gt;를 approximation한다. approximate하는 함수 &lt;code class=&quot;MathJax_Preview&quot;&gt;f^{\pi}(s,a;w)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f^{\pi}(s,a;w)&lt;/script&gt;는 다음과 같다.(compatible value function)&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;f^{\pi}(s,a;w)=w^T\psi^{\pi}(s,a)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^{\pi}(s,a;w)=w^T\psi^{\pi}(s,a)&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\psi^{\pi}(s,a) = \nabla log\pi(a;s,\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi^{\pi}(s,a) = \nabla log\pi(a;s,\theta)&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;는 원래 approximate하는 함수 &lt;code class=&quot;MathJax_Preview&quot;&gt;Q&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;Q&lt;/script&gt;와 &lt;code class=&quot;MathJax_Preview&quot;&gt;f&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f&lt;/script&gt;의 차이를 줄이도록 학습한다(mean square error). 수렴한 local minima의 &lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;를 &lt;code class=&quot;MathJax_Preview&quot;&gt;\bar{w}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{w}&lt;/script&gt;라고 하겠다. 에러는 다음과 같은 수식으로 나타낸다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\epsilon(w,\pi)\equiv\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)(f^{\pi}(s,a;w)-Q^{\pi}(s,a))^2&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon(w,\pi)\equiv\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)(f^{\pi}(s,a;w)-Q^{\pi}(s,a))^2&lt;/script&gt;

&lt;p&gt;위 식이 local minima이면 미분값이 0이다. &lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;에 대해서 미분하면 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)\psi^{\pi}(s,a)(\psi^{\pi}(s,a)^T\bar{w}-Q^{\pi}(s,a))=0&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)\psi^{\pi}(s,a)(\psi^{\pi}(s,a)^T\bar{w}-Q^{\pi}(s,a))=0&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;(\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)\psi^{\pi}(s,a)\psi^{\pi}(s,a)^T)\bar{w}=\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)\psi^{\pi}(s,a)Q^{\pi}(s,a))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;(\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)\psi^{\pi}(s,a)\psi^{\pi}(s,a)^T)\bar{w}=\sum_{s, a}\rho^{\pi}(s)\pi(a;s,\theta)\psi^{\pi}(s,a)Q^{\pi}(s,a))&lt;/script&gt;

&lt;p&gt;이 때, 위 식의 우변은 &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;의 정의에 의해 policy gradient가 된다. 또한 왼쪽 항에서는 Fisher information matrix가 나온다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;F(\theta)=\sum_{s,a}\pi(a;s,\theta)\psi^{\pi}(s,a)\psi^{\pi}(s,a)=E_{\rho^\pi(s)}[F_s(\theta)]&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\theta)=\sum_{s,a}\pi(a;s,\theta)\psi^{\pi}(s,a)\psi^{\pi}(s,a)=E_{\rho^\pi(s)}[F_s(\theta)]&lt;/script&gt;

&lt;p&gt;따라서 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;F(\theta)\bar{w}=\nabla\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;F(\theta)\bar{w}=\nabla\eta(\theta)&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\bar{w}=F(\theta)^{-1}\nabla\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{w}=F(\theta)^{-1}\nabla\eta(\theta)&lt;/script&gt;

&lt;p&gt;이 식은 natural gradient 식과 동일하다. 이 식은 policy가 update 될 때, value function approximator의 parameter 방향으로 이동한다는 것을 의미한다. function approximation이 정확하다면 그 parameter의 natural policy gradient와 inner product가 커야한다.&lt;/p&gt;

&lt;h3 id=&quot;52-theorem-2-greedy-polict-improvement&quot;&gt;5.2 Theorem 2: Greedy Polict Improvement&lt;/h3&gt;
&lt;p&gt;natural policy gradient가 단순히 더 좋은 행동을 고르도록 학습하는게 아니라 가장 좋은 (greedy) 행동을 고르도록 학습한다는 것을 증명하는 파트이다. 이것을 일반적인 형태의 policy에 대해서 증명하기 전에 exponential 형태의 policy에 대해서 증명하는 것이 Theorem 2이다.&lt;/p&gt;

&lt;p&gt;policy를 다음과 같이 정의한다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\pi(a;s,\theta) \propto exp(\theta^T\phi_{sa})&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(a;s,\theta) \propto exp(\theta^T\phi_{sa})&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\bar{\nabla}\eta(\theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{\nabla}\eta(\theta)&lt;/script&gt;가 0이 아니고 &lt;code class=&quot;MathJax_Preview&quot;&gt;\bar{w}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{w}&lt;/script&gt;는 approximation error를 최소화한 &lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;라고 가정한다. 이 상태에서 natural gradient update를 생각해보자. policy gradient는 gradient ascent임을 기억하자.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\theta_{t+1}=\theta_t + \alpha_t\bar{\nabla}\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{t+1}=\theta_t + \alpha_t\bar{\nabla}\eta(\theta)&lt;/script&gt;

&lt;p&gt;이 때 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;가 learning rate로 parameter를 얼마나 업데이트하는지를 결정한다. 이 값을 무한대로 늘렸을 때 policy가 어떻게 업데이트되는지 생각해보자.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\pi_{\infty}(a;s)=lim_{\alpha\rightarrow\infty}\pi(a;s,\theta+\alpha\bar{\nabla}\eta(\theta))-(1)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_{\infty}(a;s)=lim_{\alpha\rightarrow\infty}\pi(a;s,\theta+\alpha\bar{\nabla}\eta(\theta))-(1)&lt;/script&gt;

&lt;p&gt;function approximator는 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;f^{\pi}(s,a;w)=w^T\psi^{\pi}(s,a)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^{\pi}(s,a;w)=w^T\psi^{\pi}(s,a)&lt;/script&gt;

&lt;p&gt;Theorem 1에 의해 위 식은 아래와 같이 쓸 수 있다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;f^{\pi}(s,a;w)=\bar{\nabla}\eta(\theta)^T\psi^{\pi}(s,a)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^{\pi}(s,a;w)=\bar{\nabla}\eta(\theta)^T\psi^{\pi}(s,a)&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;의 정의에 의해 &lt;code class=&quot;MathJax_Preview&quot;&gt;\psi&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\psi&lt;/script&gt;는 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\psi^{\pi}(s,a)=\phi_{sa}-E_{\pi(a';s,\theta)}[\phi_{sa'}]&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\psi^{\pi}(s,a)=\phi_{sa}-E_{\pi(a';s,\theta)}[\phi_{sa'}]&lt;/script&gt;

&lt;p&gt;function approximator는 다음과 같이 다시 쓸 수 있다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;f^{\pi}(s,a;w)=\bar{\nabla}\eta(\theta)^T(\phi_{sa}-E_{\pi(a';s,\theta)}[\phi_{sa'}])&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;f^{\pi}(s,a;w)=\bar{\nabla}\eta(\theta)^T(\phi_{sa}-E_{\pi(a';s,\theta)}[\phi_{sa'}])&lt;/script&gt;

&lt;p&gt;greedy policy improvement가 Q function 값 중 가장 큰 값을 가지는 action을 선택하듯이 여기서도 function approximator의 값이 가장 큰 action을 선택하는 상황을 가정해본다. 이 때 function approximator의 argmax는 다음과 같이 쓸 수 있다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;argmax_{a'}f^{\pi}(s,a)=argmax_{a'}\bar{\nabla}\eta(\theta)^T\phi_{sa'}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;argmax_{a'}f^{\pi}(s,a)=argmax_{a'}\bar{\nabla}\eta(\theta)^T\phi_{sa'}&lt;/script&gt;

&lt;p&gt;(1) 식을 다시 살펴보자. policy의 정의에 따라 다음과 같이 쓸 수 있다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\pi(a;s,\theta + \alpha\bar{\nabla}\eta(\theta)) \propto exp(\theta^T\phi_{sa} + \alpha\bar{\nabla}\eta(\theta)^T\phi_{sa})&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(a;s,\theta + \alpha\bar{\nabla}\eta(\theta)) \propto exp(\theta^T\phi_{sa} + \alpha\bar{\nabla}\eta(\theta)^T\phi_{sa})&lt;/script&gt;

&lt;p&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;\bar{\nabla}\eta(\theta) \neq 0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{\nabla}\eta(\theta) \neq 0&lt;/script&gt;이고 &lt;code class=&quot;MathJax_Preview&quot;&gt;\alpha\rightarrow\infty&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha\rightarrow\infty&lt;/script&gt;이면 exp안의 항 중에서 뒤의 항이 dominate하게 된다. 여러 행동 중에 &lt;code class=&quot;MathJax_Preview&quot;&gt;\bar{\nabla}\eta(\theta)^T\phi_{sa}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{\nabla}\eta(\theta)^T\phi_{sa}&lt;/script&gt;가 가장 큰 행동이 있다면 이 행동의 policy probability가 1이 되고 나머지는 0이 된다. 따라서 다음이 성립한다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\pi_{\infty}=0&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi_{\infty}=0&lt;/script&gt;

&lt;p&gt;if and only if&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;a \notin argmax_{a'}\bar{\nabla}\eta(\theta)^T\phi_{sa'}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;a \notin argmax_{a'}\bar{\nabla}\eta(\theta)^T\phi_{sa'}&lt;/script&gt;

&lt;p&gt;이 결과로부터 natural policy gradient는 단지 더 좋은 action이 아니라 best action을 고르도록 학습이 된다. 하지만 non-covariant gradient(1차미분) 에서는 그저 더 좋은 action을 고르도록 학습이 된다. 하지만 이 natural policy gradient에 대한 결과는 infinite learning rate 세팅에서만 성립함. 좀 더 일반적인 경우에 대해서 살펴보자.&lt;/p&gt;

&lt;h4 id=&quot;43-theorem-3&quot;&gt;4.3 Theorem 3&lt;/h4&gt;
&lt;p&gt;Theorem 2에서와는 달리 일반적인 policy를 가정하자(general parameterized policy). Theorem 3는 이 상황에서 natural gradient를 통한 업데이트가 best action를 고르는 방향으로 학습이 된다는 것을 보여준다.&lt;/p&gt;

&lt;p&gt;natural gradien에 따른 policy parameter의 업데이트는 다음과 같다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\bar{w}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\bar{w}&lt;/script&gt;는 approximation error를 minimize하는 &lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\delta\theta = \theta' - \theta = \alpha\bar{\nabla}\eta(\theta)=\alpha\bar{w}&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta\theta = \theta' - \theta = \alpha\bar{\nabla}\eta(\theta)=\alpha\bar{w}&lt;/script&gt;

&lt;p&gt;policy에 대해서 1차근사를 하면 다음과 같다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\pi(a;s,\theta')=\pi(a;s,\theta)+\frac{\partial\pi(a;s,\theta)^T}{\partial\theta}\delta\theta + O(\delta\theta^2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(a;s,\theta')=\pi(a;s,\theta)+\frac{\partial\pi(a;s,\theta)^T}{\partial\theta}\delta\theta + O(\delta\theta^2)&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;=\pi(a;s,\theta)(1+\psi(s,a)^T\delta\theta) + O(\delta\theta^2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\pi(a;s,\theta)(1+\psi(s,a)^T\delta\theta) + O(\delta\theta^2)&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;=\pi(a;s,\theta)(1+\alpha\psi(s,a)^T\bar{w}) + O(\delta\theta^2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\pi(a;s,\theta)(1+\alpha\psi(s,a)^T\bar{w}) + O(\delta\theta^2)&lt;/script&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;=\pi(a;s,\theta)(1+\alpha f^{\pi}(s,a;\bar{w}) + O(\delta\theta^2)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\pi(a;s,\theta)(1+\alpha f^{\pi}(s,a;\bar{w}) + O(\delta\theta^2)&lt;/script&gt;

&lt;p&gt;policy 자체가 function approximator의 크기대로 업데이트가 되므로 local하게 best action의 probability는 커지고 다른 probability의 크기는 작아질 것이다. 하지만 만약 greedy improvement가 된다하더라도 그게 performance의 improvement를 보장하는 것은 아니다. 하지만 line search와 함께 사용할 경우 improvement를 보장할 수 있다.&lt;/p&gt;

&lt;h2 id=&quot;6-metrics-and-curvatures&quot;&gt;6. Metrics and Curvatures&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;다음 식에 해당하는 G는 Fisher Information Matrix만 사용할 수 있는 것이 아니다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\vert d\theta \vert^2=\sum_{ij}(\theta)d\theta_id\theta_i=d\theta^TG(\theta)d\theta&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\vert d\theta \vert^2=\sum_{ij}(\theta)d\theta_id\theta_i=d\theta^TG(\theta)d\theta&lt;/script&gt;

&lt;p&gt;이 파트에서는 FIM과 다른 metric 사이의 관계를 다룬다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the different setting of parameter estimation, the Fisher information converges to the &lt;code class=&quot;highlighter-rouge&quot;&gt;Hessian&lt;/code&gt;, so it is &lt;a href=&quot;https://en.wikipedia.org/wiki/Efficiency_(statistics)#Asymptotic_efficiency&quot;&gt;asymptotically efficient&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;이 논문의 경우, 아마리 논문의 ‘blind separation case’와 유사한데 이 때는 꼭 asymtotically efficient하지 않다. 이 말은 즉 2nd order 수렴이 보장되지 않는다는 것이다.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.inference.org.uk/mackay/ica.pdf&quot;&gt;Mackay&lt;/a&gt; 논문에서 hessian에서 data independant한 term을 metric으로 가져오는 방법을 제안했다. 그래서 performance를 2번 미분해보면 다음과 같다. 하지만 다음 식에서는 모든 항이 data dependent하다(Q가 있으니까). 첫 번째 항이 그나마 FIM과의 관련성이 있을 수 있지만 Q 값이 curvature에 weight를 주는 방식 때문에 다르다고 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\nabla^2\eta(\theta)=\sum_{sa}\rho^{\pi}(s)(\nabla^2\pi(a;s)Q^{\pi}(s,a)+\nabla\pi(a;s)\nabla Q^{\pi}(s,a)^T+\nabla Q^{\pi}(s,a)\nabla\pi(a;s)^T)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla^2\eta(\theta)=\sum_{sa}\rho^{\pi}(s)(\nabla^2\pi(a;s)Q^{\pi}(s,a)+\nabla\pi(a;s)\nabla Q^{\pi}(s,a)^T+\nabla Q^{\pi}(s,a)\nabla\pi(a;s)^T)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;hessian은 보통 positive definite가 아닐수도 있다. 따라서 local maxima가 될 때까지 Hessian이 사용하기 별로 안좋다. 그리고 local maxima에서는 Hessian보다는 Conjugate methods가 더 효율적이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;이 파트에서는 무엇을 말하고 있는지 알기가 어렵다. FIM과 Hessian이 관련이 있다는 것을 알겠다. 하지만 asymtotically efficient와 같은 내용을 모르므로 내용의 이해가 어려웠다.&lt;/p&gt;

&lt;p&gt;Mackay 논문에서 해당 부분은 다음과 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/x4n6z6pdyi7xtb9/Screenshot%202018-06-10%2012.04.13.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;7-experiment&quot;&gt;7. Experiment&lt;/h2&gt;
&lt;hr /&gt;
&lt;p&gt;논문에서는 natural gradient를 simple MDP와 tetris MDP에 대해서 테스트했다. practice에서는 Fisher information matrix는 다음과 같은 식으로 업데이트한다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;f\leftarrow f+\nabla log \pi(a_t; s_t, \theta)\nabla log \pi(a_t; s_t, \theta)^T&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;f\leftarrow f+\nabla log \pi(a_t; s_t, \theta)\nabla log \pi(a_t; s_t, \theta)^T&lt;/script&gt;

&lt;p&gt;T length trajectory에 대해서 f/T를 통해 F의 estimate를 구한다.&lt;/p&gt;

&lt;h3 id=&quot;71-linear-quadratic-regulator&quot;&gt;7.1 Linear Quadratic regulator&lt;/h3&gt;
&lt;p&gt;에이전트를 테스트할 환경은 다음과 같은 dynamics를 가지고 있다. &lt;code class=&quot;MathJax_Preview&quot;&gt;u(t)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;u(t)&lt;/script&gt;는 control signal로서 에이전트의 행동이라고 생각하면 된다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\epsilon&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;은 noise distribution으로 환경에 가해지는 노이즈이다. 에이전트의 목표는 적절한 &lt;code class=&quot;MathJax_Preview&quot;&gt;u(t)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;u(t)&lt;/script&gt;를 통해 
x(t)를 0으로 유지하는 것이다. 제어분야에서의 LQR controller 문제이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;x(t+1) = 0.7x(t)+u(t)+\epsilon(t)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;x(t+1) = 0.7x(t)+u(t)+\epsilon(t)&lt;/script&gt;

&lt;p&gt;x(t)를 0으로 유지하기 위해서 &lt;code class=&quot;MathJax_Preview&quot;&gt;x(t)^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x(t)^2&lt;/script&gt;를 cost로 잡고 이 cost를 최소화하도록 학습한다. 이 시스템을 linear라고 부르는 것은 아래 그림과 같이 선형의 형태를 띄기 때문이다. 이 논문에서 실험할 때는 이 그림에서의 system에 noise를 더해준 것이다. &lt;a href=&quot;https://stanford.edu/class/ee363/lectures/dlqr.pdf&quot;&gt;그림 출처&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/vz0q97lcek4oti5/Screenshot%202018-06-08%2014.21.10.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 실험에서 사용한 parameterized policy는 다음과 같다. parameter가 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt;과 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_2&lt;/script&gt; 밖에 없는 상당히 간단한 policy이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\pi(u;x,\theta) \propto exp(\theta_1 s_1 x^2 + \theta_2 s_2 x)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\pi(u;x,\theta) \propto exp(\theta_1 s_1 x^2 + \theta_2 s_2 x)&lt;/script&gt;

&lt;p&gt;이 policy를 간단히 numpy와 matplotlib를 이용해서 그려봤다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_1&lt;/script&gt;과 &lt;code class=&quot;MathJax_Preview&quot;&gt;theta_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;theta_2&lt;/script&gt;를 (0.5, 0.5), (1, 0), (0, 1)로 하고 &lt;code class=&quot;MathJax_Preview&quot;&gt;s_1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_1&lt;/script&gt;과 &lt;code class=&quot;MathJax_Preview&quot;&gt;s_2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s_2&lt;/script&gt;는 1로 두었다. x는 -1에서 1까지의 범위로 그렸다. x를 0으로 유지하려면 u(t)가 -와 +가 둘 다 가능해야할 것 같은데 위 식으로만 봐서는 action이 하나이고 그 action일 확률을 표시하는 것처럼 나왔다. 아마 -1과 +1이 u(t)가 될 수 있는데 그 중 +1을 선택할 확률이 위와 같이 되는게 아닌가 싶다.&lt;/p&gt;
&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/v69qyrwn7zurk8c/Screenshot%202018-06-08%2014.57.07.png?dl=1&quot; width=&quot;500px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;다음 그림은 1-d LQR을 학습한 그래프이다. cost가 &lt;code class=&quot;MathJax_Preview&quot;&gt;x^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x^2&lt;/script&gt;이기 때문에 cost가 0으로 갈수록 agent는 0에서 안정적으로 머무른다고 볼 수 있다. 6개의 선 중에서 오른쪽 세 개가 일반적인 gradient 방법을 사용해서 학습한 결과이다. 그리고 왼쪽의 세 개의 선이 natural policy gradient를 통해 학습한 학습 곡선이다. 일반 gradient 방법보다 natural gradient가 훨씬 빠르게 학습한다(time 축이 log scale인 것을 감안하자).&lt;/p&gt;

&lt;p&gt;하지만 문제가 있다. npg를 학습한 세 개의 곡선은 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;를 rescale 한 것이다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;앞에 곱해지는 숫자에 따라 학습의 과정이 다르다. 이 것은 coordinate에 따라 steepest gradient가 다르게 측정된다는 것이다. 즉, covariant gradient가 아니라는 뜻이다. 이 논문에서는 natural gradient를 통해 gradient가 covariant하도록 만들고 싶었는데 실패한 것이다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/fhn8cgje0rdws0i/Screenshot%202018-06-08%2023.13.37.png?dl=1&quot; width=&quot;300px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;natural gradient가 covariant하지 않은 이유는 Fisher Information Matrix가 예상했던 바와는 달리 invariant metric이 아니기 때문이다. 또한 FIM이 invariant metric이 아닌 이유는 FIM을 계산할 때 &lt;code class=&quot;MathJax_Preview&quot;&gt;\rho_s&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho_s&lt;/script&gt;가 곱해지기 때문이다(state distribution에 대한 expectation. &lt;code class=&quot;MathJax_Preview&quot;&gt;\rho_s&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho_s&lt;/script&gt;가 곱해지는 것이 invariant에 미치는 영향이 무엇인지는 모르겠다). 하지만 여전히 의의가 있는 것은 기존 gradient 방법들보다 훨씬 빠르게 학습한다는 것이다.&lt;/p&gt;

&lt;h3 id=&quot;72-simple-2-state-mdp&quot;&gt;7.2 simple 2-state MDP&lt;/h3&gt;
&lt;p&gt;이제 다른 예제에서 NPG를 테스트한다. 2개의 state만 가지는 MDP를 고려해보자. &lt;a href=&quot;http://repository.cmu.edu/cgi/viewcontent.cgi?article=1080&amp;amp;context=robotics&quot;&gt;그림출처&lt;/a&gt;. 그림으로보면 다음과 같다. x=0 상태와 x=1 상태 두 개가 존재한다. 에이전트는 각 상태에서 다시 자신의 상태로 되돌아오는 행동을 하거나 다른 상태로 가는 행동을 할 수 있다. 상태 x=0에서 다시 자기 자신으로 돌아오면 1의 보상을 받고 상태 x=1에서 자기 자신으로 돌아오면 2의 보상을 받는다. 따라서 결국 optimal policy는 상태 x=1에서 계속 자기 자신으로 돌아오는 행동을 취하는 것이다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/g1x9yknzsrip59i/Screenshot%202018-06-08%2023.06.50.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;문제를 좀 어렵게 만들기 위해 state distribution을 다음과 같이 설정한다. 즉, 대부분의 경우에 상태 x=0에서 에이전트가 시작하는 것이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\rho(x=0)=0.8,  \rho(x=1)=0.2&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\rho(x=0)=0.8,  \rho(x=1)=0.2&lt;/script&gt;

&lt;p&gt;일반적인 gradient 방법을 사용하면 다음과 같은 policy gradient 식에 따라서 업데이트를 하게 된다. 이 때, &lt;code class=&quot;MathJax_Preview&quot;&gt;\rho(s)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho(s)&lt;/script&gt;가 gradient에 곱해지므로 상태적으로 상태 0에서의 gradient 값이 커지게 된다. 따라서 에이전트는 상태 0에서의 gradient(상태 0에서 스스로에게 돌아오는 행동을 취하도록 정책을 업데이트하는 gradient)를 따라 parameterized policy를 update한다. 따라서 아래 그림의 첫번째 그림에서처럼 Reward가 1에서 오랜 시간동안 머무른다. 즉, 에이전트가 상태 0에서 self-loop를 계속 돌고 있다는 것이다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\rho(x=0)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho(x=0)&lt;/script&gt;가 &lt;code class=&quot;MathJax_Preview&quot;&gt;10^{-7}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;10^{-7}&lt;/script&gt;까지 떨어진다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\nabla\eta(\theta)=\sum_{s,a}\rho^{\pi}(s)\nabla\pi(a;s,\theta)Q^{\pi}(s,a)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla\eta(\theta)=\sum_{s,a}\rho^{\pi}(s)\nabla\pi(a;s,\theta)Q^{\pi}(s,a)&lt;/script&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/xtb77mfazbppnss/Screenshot%202018-06-08%2023.14.24.png?dl=1&quot; width=&quot;300px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;하지만 NPG를 사용할 경우에는 훨씬 빠르게 average reward가 2에 도달한다. gradient 방법이 &lt;code class=&quot;MathJax_Preview&quot;&gt;1.7X10^(7)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1.7X10^(7)&lt;/script&gt;정도의 시간만에 2에 도달한 반면 NPG는 2만에 도달한다. 또한 &lt;code class=&quot;MathJax_Preview&quot;&gt;\rho(x=0)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho(x=0)&lt;/script&gt;가 &lt;code class=&quot;MathJax_Preview&quot;&gt;10^{-5}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;10^{-5}&lt;/script&gt;이하로 떨어지지 않는다.&lt;/p&gt;

&lt;p&gt;한 가지 그래프를 더 살펴보자. 다음 그래프는 parameter &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta&lt;/script&gt;가 업데이트 되는 과정을 보여준다. 이 그래프에서는 parameter가 2개 있는 것이다. 일반적인 gradient가 아래 그래프에서 실선에 해당한다. 이 실선의 그래프는 보면 처음부터 중반까지 &lt;code class=&quot;MathJax_Preview&quot;&gt;\theta_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\theta_i&lt;/script&gt;만 거의 업데이트하는 것을 볼 수 있다. 그에 비해 NPG는 두 개의 parameter를 균등하게 업데이트하는 것을 볼 수 있다.&lt;/p&gt;

&lt;center&gt;&lt;img src=&quot;https://www.dropbox.com/s/g7pazozw2k6rd7x/Screenshot%202018-06-08%2023.23.25.png?dl=1&quot; width=&quot;300px&quot; /&gt;&lt;/center&gt;

&lt;p&gt;policy가 &lt;code class=&quot;MathJax_Preview&quot;&gt;\pi(a;s,\theta)\propto exp(\theta_{sa})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi(a;s,\theta)\propto exp(\theta_{sa})&lt;/script&gt;일 때, 다음과 같이 &lt;code class=&quot;MathJax_Preview&quot;&gt;F_{-1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;F_{-1}&lt;/script&gt;이 gradient 앞에 weight로 곱해지는데 이게 &lt;code class=&quot;MathJax_Preview&quot;&gt;\rho&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\rho&lt;/script&gt;와는 달리 각 parameter에 대해 균등하다. 따라서 위 그래프에서와 같이 각 parameter는 비슷한 비율로 업데이트가 되는 것이다.&lt;/p&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;\bar{\nabla}\eta(\theta) = F^{-1}\nabla\eta(\theta)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;\bar{\nabla}\eta(\theta) = F^{-1}\nabla\eta(\theta)&lt;/script&gt;

&lt;h3 id=&quot;73-tetris&quot;&gt;7.3 Tetris&lt;/h3&gt;
&lt;p&gt;NPG를 테스트할 tetris 예제는 Neuro Dynamic Programming 책에 소개되어있다. 다음 그림은 tetris 예제를 보여준다. 보통 그림에서와 같이 state의 feature를 정해준다. &lt;a href=&quot;http://slideplayer.com/slide/5215520/&quot;&gt;그림 출처&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/y1halso9yermy8s/Screenshot%202018-06-08%2023.44.34.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이 예제에서도 exponantial family로 policy를 표현한다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\pi(a;s,\theta) \propto exp(\theta^T\phi_{sa})&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi(a;s,\theta) \propto exp(\theta^T\phi_{sa})&lt;/script&gt; 로 표현한다.&lt;/p&gt;

&lt;p&gt;tetris는 linear function approximator와 greedy policy iteration을 사용할 경우 performance가 갑자기 떨어지는 현상이 있다. 밑의 그림에서 A의 spike가 있는 그래프가 이 경우이다. 그 밑에 낮게 누워있는 그래프는 일반적인 policy gradient 방법이다. 하지만 Natural policy gradient를 사용할 경우 B 그림에서 오른쪽 그래프와 같이 성능개선이 뚜렷하다. Policy Iteration 처럼 성능이 뚝 떨어지지 않고 안정적으로 유지한다. 또한 그림 C에서 보는 것처럼 오른쪽 그래프인 일반적인 gradient 방법보다 훨씬 빠르게 학습하는 것을 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/pr6s2qrqaic0wyj/Screenshot%202018-06-08%2023.40.16.png?dl=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;8-discussion&quot;&gt;8. Discussion&lt;/h2&gt;
&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;natural gradient method는 policy iteration에서와 같이 greedy action을 선택하도록 학습됌&lt;/li&gt;
  &lt;li&gt;line search와 함께 쓰면 natural gradient method는 더 policy iteration 같아짐&lt;/li&gt;
  &lt;li&gt;greedy policy iteration에서와는 달리 performance improvement가 보장됌&lt;/li&gt;
  &lt;li&gt;하지만 F(Fisher information matrix)가 asymtotically Hessian으로 수렴하지 않음. asymtotically conjugate gradient method(Hessian의 inverse를 approx.로 구하는 방법)가 더 좋아 보일 수 있음&lt;/li&gt;
  &lt;li&gt;하지만 Hessian이 항상 informative하지 않고(hessian이 어떤 정보를 주려면 positive definite와 같은 성질을 가져서 해당 함수가 convex인 것을 알 수 있다든지의 경우를 이야기하는데 hessian이 항상 positive definite가 아닐 수 있다는 것이다) tetris에서 봤듯이 natural gradient method가 더 효율적일 수 있음(pushing the policy toward choosing greedy optimal actions)&lt;/li&gt;
  &lt;li&gt;conjugate gradient method가 좀 더 maximum에 빠르게 수렴하지만, performance는 maximum에서 거의 안변하므로 좋다고 말하기 어려움(?). 이 부분에 대해서 추가적인 연구 필요.&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 22 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/05/22/natural-policy-gradient/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/05/22/natural-policy-gradient/</guid>
        
        <category>rl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>ICLR 2018 accepted RL papers</title>
        <description>&lt;h2 id=&quot;iclr-2018-accepted-rl-papers&quot;&gt;ICLR 2018 accepted RL papers&lt;/h2&gt;
&lt;p&gt;이번 ICLR 2018에 채택된 강화학습 논문 중에서 관심이 가는 논문들 리스트를 뽑아봤습니다. 다른 논문도 많지만 제 취향대로 뽑아봤습니다. 먼저 읽고 싶은 논문은 (2), (10), (12) 입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;(1) &lt;a href=&quot;https://openreview.net/forum?id=B1mvVm-C-&quot;&gt;Universal Agent for Disentangling Environments and Tasks&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: 어떤 task를 학습하는데 두 개로 agent를 쪼개는 방식이 흥미로워서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abtract&lt;/code&gt;: Recent state-of-the-art reinforcement learning algorithms are trained under the goal of excelling in one specific task. Hence, both environment and task specific knowledge are entangled into one framework. However, there are often scenarios where the environment (e.g. the physical world) is fixed while only the target task changes. Hence, borrowing the idea from hierarchical reinforcement learning, we propose a framework that disentangles task and environment specific knowledge by separating them into two units. The environment-specific unit handles how to move from one state to the target state; and the task-specific unit plans for the next target state given a specific task. The extensive results in simulators indicate that our method can efficiently separate and learn two independent units, and also adapt to a new task more efficiently than the state-of-the-art methods.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(2) &lt;a href=&quot;https://openreview.net/forum?id=rk07ZXZR&quot;&gt;Learning an Embedding Space for Transferable Robot Skills&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: embedding space, latent variable과 강화학습의 관련점을 찾고 있는데 아이디어를 줄 수 있을 것 같아서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstact&lt;/code&gt;: We present a method for reinforcement learning of closely related skills that are parameterized via a skill embedding space. We learn such skills by taking advantage of latent variables and exploiting a connection between reinforcement learning and variational inference. The main contribution of our work is an entropy-regularized policy gradient formulation for hierarchical policies, and an associated, data-efficient and robust off-policy gradient algorithm based on stochastic value gradients. We demonstrate the effectiveness of our method on several simulated robotic manipulation tasks. We find that our method allows for discovery of multiple solutions and is capable of learning the minimum number of distinct skills that are necessary to solve a given set of tasks. In addition, our results indicate that the hereby proposed technique can interpolate and/or sequence previously learned skills in order to accomplish more complex tasks, even in the presence of sparse rewards.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(3) &lt;a href=&quot;https://openreview.net/forum?id=S1ANxQW0b&quot;&gt;Maximum a Posteriori Policy Optimisation&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: 강화학습에서의 최적화에 대해서 더 공부할 수 있을 것 같아서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: We introduce a new algorithm for reinforcement learning called Maximum a-posteriori Policy Optimisation (MPO) based on coordinate ascent on a relative-entropy objective. We show that several existing methods can directly be related to our derivation. We develop two off-policy algorithms and demonstrate that they are competitive with the state-of-the-art in deep reinforcement learning. In particular, for continuous control, our method outperforms existing methods with respect to sample efficiency, premature convergence and robustness to hyperparameter settings.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(4) &lt;a href=&quot;https://openreview.net/forum?id=BkUp6GZRW&quot;&gt;Boosting the Actor with Dual Critic&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: bellman equation을 새로운 관점에서 볼 수 있지 않을까해서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: This paper proposes a new actor-critic-style algorithm called Dual Actor-Critic or Dual-AC.  It is derived in a principled way from the Lagrangian dual form of the Bellman optimality equation, which can be viewed as a two-player game between the actor and a critic-like function, which is named as dual critic.  Compared to its actor-critic relatives, Dual-AC has the desired property that the actor and dual critic are updated cooperatively to optimize the same objective function, providing a more transparent way for learning the critic that is directly related to the objective function of the actor. We then provide a concrete algorithm that can effectively solve the minimax optimization problem, using techniques of multi-step bootstrapping, path regularization, and stochastic dual ascent algorithm. We demonstrate that the proposed algorithm achieves the state-of-the-art performances across several benchmarks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(5) &lt;a href=&quot;https://openreview.net/forum?id=rJwelMbR-&quot;&gt;Divide-and-Conquer Reinforcement Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: initial state의 변화라는 새로운 강화학습의 문제에 대해 생각해볼 수 있을 것 같아서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: Standard model-free deep reinforcement learning (RL) algorithms sample a new initial state for each trial, allowing them to optimize policies that can perform well even in highly stochastic environments. However, problems that exhibit considerable initial state variation typically produce high-variance gradient estimates for model-free RL, making direct policy or value function optimization challenging. In this paper, we develop a novel algorithm that instead partitions the initial state space into “slices”, and optimizes an ensemble of policies, each on a different slice. The ensemble is gradually unified into a single policy that can succeed on the whole state space. This approach, which we term divide-and-conquer RL, is able to solve complex tasks where conventional deep RL methods are ineffective. Our results show that divide-and-conquer RL greatly outperforms conventional policy gradient methods on challenging grasping, manipulation, and locomotion tasks, and exceeds the performance of a variety of prior methods. Videos of policies learned by our algorithm can be viewed at https://sites.google.com/view/dnc-rl/&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(6) &lt;a href=&quot;https://openreview.net/forum?id=S1CChZ-CZ&quot;&gt;Ask the Right Questions: Active Question Reformulation with Reinforcement Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: NLP 분야에서의 강화학습에 대해서 알 수 있어서(이미 봤지만 정리를 해보고 싶음), 재밌는 환경(제퍼디쇼)&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: We frame Question Answering (QA) as a Reinforcement Learning task, an approach that we call Active Question Answering. We propose an agent that sits between the user and a black box QA system and learns to reformulate questions to elicit the best possible answers. The agent probes the system with, potentially many, natural language reformulations of an initial question and aggregates the returned evidence to yield the best answer. The reformulation system is trained end-to-end to maximize answer quality using policy gradient. We evaluate on SearchQA, a dataset of complex questions extracted from Jeopardy!. The agent outperforms a state-of-the-art base model, playing the role of the environment, and other benchmarks. We also analyze the language that the agent has learned while interacting with the question answering system. We find that successful question reformulations look quite different from natural language paraphrases. The agent is able to discover non-trivial reformulation strategies that resemble classic information retrieval techniques such as term re-weighting (tf-idf) and stemming.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(7) &lt;a href=&quot;https://openreview.net/forum?id=SJJinbWRZ&quot;&gt;Model-Ensemble Trust-Region Policy Optimization&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: TRPO의 발전버전이라서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: Model-free reinforcement learning (RL) methods are succeeding in a growing number of tasks, aided by recent advances in deep learning.  However, they tend to suffer from high sample complexity, which hinders their use in real-world domains.  Alternatively, model-based reinforcement learning promises to reduce sample complexity, but tends to require careful tuning and to date have succeeded mainly in restrictive domains where simple models are sufficient for learning. In this paper, we analyze the behavior of vanilla model-based reinforcement learning methods when deep neural networks are used to learn both the model and the policy, and show that the learned policy tends to exploit regions where insufficient data is available for the model to be learned, causing instability in training. To overcome this issue, we propose to use an ensemble of models to maintain the model uncertainty and regularize the learning process. We further show that the use of likelihood ratio derivatives yields much more stable learning than backpropagation through time. Altogether, our approach Model-Ensemble Trust-Region Policy Optimization (ME-TRPO) significantly reduces the sample complexity compared to model-free deep RL methods on challenging continuous control benchmark tasks.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(8) &lt;a href=&quot;https://openreview.net/forum?id=Skw0n-W0Z&quot;&gt;Temporal Difference Models: Model-Free Deep RL for Model-Based Control&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: Model-free RL과 model-based rl이 맞는 방향 같아서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: Model-free reinforcement learning (RL) has been proven to be a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even for off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(9) &lt;a href=&quot;https://openreview.net/forum?id=rJvJXZb0W&quot;&gt;An efficient framework for learning sentence representations&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: representation 관련 내용이라서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstact&lt;/code&gt;: In this work we propose a simple and efficient framework for learning sentence representations from unlabelled data. Drawing inspiration from the distributional hypothesis and recent work on learning sentence representations, we reformulate the problem of predicting the context in which a sentence appears as a classification problem. Given a sentence and the context in which it appears, a classifier distinguishes context sentences from other contrastive sentences based on their vector representations. This allows us to efficiently learn different types of encoding functions, and we show that the model learns high-quality sentence representations. We demonstrate that our sentence representations outperform state-of-the-art unsupervised and supervised representation learning methods on several downstream NLP tasks that involve understanding sentence semantics while achieving an order of magnitude speedup in training time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(10) &lt;a href=&quot;https://openreview.net/forum?id=HJewuJWCZ&quot;&gt;Learning to teach&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: 얼마전에 읽었던 kickstarting과 teaching이라는 면에서 뭐가 같고 다른지 궁금해서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstact&lt;/code&gt;: Teaching plays a very important role in our society, by spreading human knowledge and educating our next generations. A good teacher will select appropriate teaching materials, impact suitable methodologies, and set up targeted examinations, according to the learning behaviors of the students. In the field of artificial intelligence, however, one has not fully explored the role of teaching, and pays most attention to machine \emph{learning}. In this paper, we argue that equal attention, if not more, should be paid to teaching, and furthermore, an optimization framework (instead of heuristics) should be used to obtain good teaching strategies. We call this approach ``learning to teach’’. In the approach, two intelligent agents interact with each other: a student model (which corresponds to the learner in traditional machine learning algorithms), and a teacher model (which determines the appropriate data, loss function, and hypothesis space to facilitate the training of the student model). The teacher model leverages the feedback from the student model to optimize its own teaching strategies by means of reinforcement learning, so as to achieve teacher-student co-evolution. To demonstrate the practical value of our proposed approach, we take the training of deep neural networks (DNN) as an example, and show that by using the learning to teach techniques, we are able to use much less training data and fewer iterations to achieve almost the same accuracy for different kinds of DNN models (e.g., multi-layer perceptron, convolutional neural networks and recurrent neural networks) under various machine learning tasks (e.g., image classification and text understanding).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(11) &lt;a href=&quot;https://openreview.net/forum?id=HyrCWeWCb&quot;&gt;Trust-PCL: An Off-Policy Trust Region Method for Continuous Control&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: TRPO의 발전버전이라서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstact&lt;/code&gt;: Trust region methods, such as TRPO, are often used to stabilize policy optimization algorithms in reinforcement learning (RL). While current trust region strategies are effective for continuous control, they typically require a large amount of on-policy interaction with the environment. To address this problem, we propose an off-policy trust region method, Trust-PCL, which exploits an observation that the optimal policy and state values of a maximum reward objective with a relative-entropy regularizer satisfy a set of multi-step pathwise consistencies along any path. The introduction of relative entropy regularization allows Trust-PCL to maintain optimization stability while exploiting off-policy data to improve sample efficiency. When evaluated on a number of continuous control tasks, Trust-PCL significantly improves the solution quality and sample efficiency of TRPO.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(12) &lt;a href=&quot;https://openreview.net/forum?id=Bk9zbyZCZ&quot;&gt;Neural map: Structured Memory for Deep Reinforcement Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: 강화학습에서 메모리의 문제를 다루기 때문&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: A critical component to enabling intelligent reasoning in partially observable environments is memory. Despite this importance, Deep Reinforcement Learning (DRL) agents have so far used relatively simple memory architectures, with the main methods to overcome partial observability being either a temporal convolution over the past k frames or an LSTM layer. More recent work (Oh et al., 2016) has went beyond these architectures by using memory networks which can allow more sophisticated addressing schemes over the past k frames. But even these architectures are unsatisfactory due to the reason that they are limited to only remembering information from the last k frames. In this paper, we develop a memory system with an adaptable write operator that is customized to the sorts of 3D environments that DRL agents typically interact with. This architecture, called the Neural Map, uses a spatially structured 2D memory image to learn to store arbitrary information about the environment over long time lags. We demonstrate empirically that the Neural Map surpasses previous DRL memories on a set of challenging 2D and 3D maze environments and show that it is capable of generalizing to environments that were not seen during training.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(13) &lt;a href=&quot;https://openreview.net/forum?id=HyiAuyb0b&quot;&gt;TD or not TD: Analyzing the Role of Temporal Differencing in Deep Reinforcement Learning&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: TD에 대한 이해를 더 깊게 할 것 같아서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: Our understanding of reinforcement learning (RL) has been shaped by theoretical and empirical results that were obtained decades ago using tabular representations and linear function approximators. These results suggest that RL methods that use temporal differencing (TD) are superior to direct Monte Carlo estimation (MC). How do these results hold up in deep RL, which deals with perceptually complex environments and deep nonlinear models? In this paper, we re-examine the role of TD in modern deep RL, using specially designed environments that control for specific factors that affect performance, such as reward sparsity, reward delay, and the perceptual complexity of the task. When comparing TD with infinite-horizon MC, we are able to reproduce classic results in modern settings. Yet we also find that finite-horizon MC is not inferior to TD, even when rewards are sparse or delayed. This makes MC a viable alternative to TD in deep RL.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;(14) &lt;a href=&quot;https://openreview.net/forum?id=H1meywxRW&quot;&gt;DCN+: Mixed Objective And Deep Residual Coattention for Question Answering&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;읽고 싶은 이유&lt;/code&gt;: NLP 분야에서의 강화학습이라서&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;abstract&lt;/code&gt;: Traditional models for question answering optimize using cross entropy loss, which encourages exact answers at the cost of penalizing nearby or overlapping answers that are sometimes equally accurate. We propose a mixed objective that combines cross entropy loss with self-critical policy learning, using rewards derived from word overlap to solve the misalignment between evaluation metric and optimization objective. In addition to the mixed objective, we introduce a deep residual coattention encoder that is inspired by recent work in deep self-attention and residual networks. Our proposals improve model performance across question types and input lengths, especially for long questions that requires the ability to capture long-term dependencies. On the Stanford Question Answering Dataset, our model achieves state of the art results with 75.1% exact match accuracy and 83.1% F1, while the ensemble obtains 78.9% exact match accuracy and 86.0% F1.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Sat, 19 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/05/19/lclr2018-rlpapers/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/05/19/lclr2018-rlpapers/</guid>
        
        <category>rl</category>
        
        
        <category>paper</category>
        
      </item>
    
      <item>
        <title>CS231n Lecture 3: Loss Functions and Optimization</title>
        <description>&lt;p&gt;강의링크: &lt;a href=&quot;https://www.youtube.com/watch?v=h7iBpEHGVNc&amp;amp;list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&amp;amp;t=0s&amp;amp;index=4&quot;&gt;youtube&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;cs231n-lecture-3-loss-functions-and-optimization&quot;&gt;CS231n Lecture 3: Loss Functions and Optimization&lt;/h1&gt;

&lt;h2 id=&quot;1-last-lecture-summary&quot;&gt;1. Last Lecture Summary&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;image 데이터에 대한 linear classification을 가정&lt;/li&gt;
  &lt;li&gt;간단하게 다음 그림같이 &lt;code class=&quot;MathJax_Preview&quot;&gt;32x32x3&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;32x32x3&lt;/script&gt;의 이미지가 입력으로 들어왔을 때 그 그림이 10개의 클래스 중에 어디에 분류되는지를 맞추는 모델을 생각&lt;/li&gt;
  &lt;li&gt;모델은 다음 수식으로 정의 : &lt;code class=&quot;MathJax_Preview&quot;&gt;f(x,W)=Wx+b&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f(x,W)=Wx+b&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;W&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;가 학습의 대상이 되는 parameters of weight&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/c56azyty1kx2uc3/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2022.50.18.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다음과 같은 그림으로 간단히 어떤 식으로 이미지를 분류하는지 알 수 있음&lt;/li&gt;
  &lt;li&gt;이미지 데이터의 형태를 유지하지 않고 일자로 펴서 weight matrix와 곱함&lt;/li&gt;
  &lt;li&gt;그로부터 나온 값이 &lt;code class=&quot;highlighter-rouge&quot;&gt;score&lt;/code&gt;이고 입력으로 들어온 이미지가 각 클래스에 얼마나 맞는지를 수치화함&lt;/li&gt;
  &lt;li&gt;다른 표현 : learning templates for the class. each row of weight matrix is template for the class&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;또 다른 표현 : linear classification is learning linear decision boundaries(svm이 떠오르는 말이군) between pixels in some high dimensional space (각 dimension은 pixel)
&lt;img src=&quot;https://www.dropbox.com/s/81gqavenf5xu2vu/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2022.55.03.png?raw=1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;lecture 3에서 다룰 내용
    &lt;ul&gt;
      &lt;li&gt;어떻게 좋은 &lt;code class=&quot;MathJax_Preview&quot;&gt;W&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;를 얻을 수 있는지?&lt;/li&gt;
      &lt;li&gt;어떤 score가 더 안 좋고 어떤 건 더 좋은데 그것을 어떻게 수치적으로 측정할 것인가: &lt;code class=&quot;highlighter-rouge&quot;&gt;loss function&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;모든 가능한 &lt;code class=&quot;MathJax_Preview&quot;&gt;W&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;W&lt;/script&gt;에 대해서 효율적인 과정을 통해 search 하는 것: &lt;code class=&quot;highlighter-rouge&quot;&gt;optimization&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-loss-function&quot;&gt;2. Loss Function&lt;/h2&gt;
&lt;hr /&gt;

&lt;h3 id=&quot;21-problem-definition&quot;&gt;2.1 Problem Definition&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;scores : &lt;code class=&quot;MathJax_Preview&quot;&gt;f(x,W)=Wx&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;f(x,W)=Wx&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;given dataset of examples : &lt;code class=&quot;MathJax_Preview&quot;&gt;{(x_i, y_i)}_{i=1}^N&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;{(x_i, y_i)}_{i=1}^N&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;x_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;는 image, &lt;code class=&quot;MathJax_Preview&quot;&gt;y_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;y_i&lt;/script&gt;는 label(integer)&lt;/li&gt;
  &lt;li&gt;다음 그림과 같은 예시를 가정. 그림이 3개가 있고 라벨도 3개가 있음. 클래스 종류는 cat, car, frog 임
&lt;img src=&quot;https://www.dropbox.com/s/cn497no4n01eg1a/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2023.03.54.png?raw=1&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22-multiclass-svm-loss&quot;&gt;2.2 multiclass svm loss&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;loss function 정의
    &lt;ul&gt;
      &lt;li&gt;이 때, loss function은 다음과 같음. 모든 example에 대한 loss의 average&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;L=\frac1N\sum_iL_i(f(x_i, W), y_i)&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;L=\frac1N\sum_iL_i(f(x_i, W), y_i)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;multiclass svm loss function: &lt;code class=&quot;highlighter-rouge&quot;&gt;hinge loss&lt;/code&gt;
    &lt;ul&gt;
      &lt;li&gt;수식은 다음과 같음&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;s=f(x,W)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;s=f(x,W)&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;S_{y_i}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S_{y_i}&lt;/script&gt;: score of correct category, &lt;code class=&quot;MathJax_Preview&quot;&gt;S_{j}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;S_{j}&lt;/script&gt;: score of incorrect category&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;1&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;1&lt;/script&gt;은 safety margin&lt;/li&gt;
      &lt;li&gt;이 loss function은 다음 그래프와 같이 hinge와 같은 모양이므로 hinge loss라고도 부름&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/dkp9tr64cda7vtx/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2023.16.31.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;svm loss 직접 계산해보자
&lt;img src=&quot;https://www.dropbox.com/s/vtyecwx7x3dnmbs/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2023.20.17.png?raw=1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;각 example에 대한 loss를 구한 다음에 평균 취한게 최종 loss
&lt;img src=&quot;https://www.dropbox.com/s/c5svqblqhh2kigp/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2023.21.14.png?raw=1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Questions&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;Q1. what is min/max of hinge loss: min=0, max=&lt;code class=&quot;MathJax_Preview&quot;&gt;\infty&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;Q2. W가 너무 작아서 score가 다 0에 가깝다면 loss는 얼마인지?: number of classes - 1&lt;/li&gt;
      &lt;li&gt;Q3. what if we used mean instead of sum?
        &lt;ul&gt;
          &lt;li&gt;변하는 건 없다. 그냥 loss 전체를 rescaling 하는 것&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Q4. what if we used &lt;code class=&quot;MathJax_Preview&quot;&gt;L_i=\sum_{j\neq y_i}max(0, S_j-S_{y_i}+1)^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;L_i=\sum_{j\neq y_i}max(0, S_j-S_{y_i}+1)^2&lt;/script&gt;?
        &lt;ul&gt;
          &lt;li&gt;loss를 nonlinear한 방법으로 바꾸는 것은 문제를 아예 다른 문제로 만드는 것임.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Q5. L=0이 되는 W를 찾았다면 이 W는 unique 한지?
        &lt;ul&gt;
          &lt;li&gt;그렇지 않다. 왜냐하면 2W에서도 L=0이기 때문이다. (margins between correct and incorrect scores bigger)&lt;/li&gt;
          &lt;li&gt;이미 margin이 1보다 큰 상황이므로 더 커져봤자 0이다.&lt;/li&gt;
          &lt;li&gt;“inconsistency”&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;23-regularization&quot;&gt;2.3 Regularization&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;방금 구한 loss는 data loss로서 train data에 대한 loss&lt;/li&gt;
  &lt;li&gt;하지만 그 loss만 가지고 학습을 하면 train data에 대해서만 잘 되고 test data에 대해서는 분류를 잘 못할 것임&lt;/li&gt;
  &lt;li&gt;따라서 “regularization” loss 항이 더 필요함. 다음 수식에서 뒤 부분인데 regularization은 model이 simple해지도록 하는 것임. simple 할수록 overfitting의 요소는 줄어들고 test data에 대해서 분류를 잘 하게 됌.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;penelize the complexity of the model rather than explicitly trying to fit the training data
&lt;img src=&quot;https://www.dropbox.com/s/vcy2kv3leiwfy87/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2023.34.19.png?raw=1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;L2 regularization : &lt;code class=&quot;MathJax_Preview&quot;&gt;R(W)=\sum_k\sum_lW_{k,l}^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R(W)=\sum_k\sum_lW_{k,l}^2&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;MAP inference using a Gaussian prior on W –&amp;gt; 무슨 뜻인지..&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;L1 regularization : &lt;code class=&quot;MathJax_Preview&quot;&gt;R(W)=\sum_k\sum_l \vert W_{k,l} \vert&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R(W)=\sum_k\sum_l \vert W_{k,l} \vert&lt;/script&gt;
    &lt;ul&gt;
      &lt;li&gt;generally preter sparse solution&lt;/li&gt;
      &lt;li&gt;increasing sparsity of matrix W&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Elastic net(L2 + L1)&lt;/li&gt;
  &lt;li&gt;Max norm regularization&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;Batch normalization&lt;/li&gt;
  &lt;li&gt;Question
    &lt;ul&gt;
      &lt;li&gt;Q1. How L2 regularization measure the complexity of model?
        &lt;ul&gt;
          &lt;li&gt;어떤 label인지 판단하는데 있어서 x 벡터의 특정 element만 보지 않고 spreadout inference&lt;/li&gt;
          &lt;li&gt;다음 그림과 같은 경우에서 w1보다 w2의 regular 텀의 크기가 작음. L2는 따라서 w2를 더 선호함. L1은 w1과 같은 sparse한 경우를 더 선호함&lt;/li&gt;
          &lt;li&gt;간단히 생각해보면 L2를 미분하면 w가 남아서 w의 크기에 따라서 decay를 할텐데 L1은 w의 크기에 상관없이 다 decay를 하기 때문에 쉽게 weight가 0이 되어버린다.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/hkbw2l3cuqh7cpi/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2023.44.53.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;24-softmax-classifier-multinomial-logistic-regression&quot;&gt;2.4 Softmax Classifier: Multinomial Logistic Regression&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;scores = unnormalized log probabilities of the classes&lt;/li&gt;
  &lt;li&gt;loss는 다음과 같이 계산함. exponential을 취한 다음에 normalize를 한 이후 나온 값 중에서 label에 해당하는 값만 가져온 것이 &lt;code class=&quot;MathJax_Preview&quot;&gt;L_i&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;L_i&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/s7e9v4ong4f3pk4/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2023.48.38.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Question
    &lt;ul&gt;
      &lt;li&gt;Q1. what is the min/max possible loss L_i
        &lt;ul&gt;
          &lt;li&gt;min= 0 / max = &lt;code class=&quot;MathJax_Preview&quot;&gt;\infty&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\infty&lt;/script&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Q2. w is small so all s~0, what is the loss?
        &lt;ul&gt;
          &lt;li&gt;log C&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;svm loss와 cross entropy loss를 둘 다 나타내면 다음 그림&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/y98rdp1bagtw5gg/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-19%2000.01.41.png?raw=1&quot; /&gt;
&lt;img src=&quot;https://www.dropbox.com/s/g9thqkefq7x4yml/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-19%2000.03.26.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;3-optimization&quot;&gt;3. Optimization&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;31-gradient-descent&quot;&gt;3.1 gradient descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;random search
    &lt;ul&gt;
      &lt;li&gt;take a bunch of Ws randomly, see how well they do&lt;/li&gt;
      &lt;li&gt;don’t use&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Follow the slop
    &lt;ul&gt;
      &lt;li&gt;함수의 미분값을 따라서 조금씩 이동해서 최적점에 도달하는 것&lt;/li&gt;
      &lt;li&gt;1-dimension은 그냥 미분&lt;/li&gt;
      &lt;li&gt;multi-dimension은 gradient &lt;code class=&quot;MathJax_Preview&quot;&gt;\nabla&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\nabla&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;the direction of steepest descent is the negative gradient&lt;/li&gt;
      &lt;li&gt;linear 1st order approximation to your function at your current point&lt;/li&gt;
      &lt;li&gt;gradient를 계산하고 그 값을 가지고 parameter vector를 iteratively update&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;how to compute gradient?
    &lt;ul&gt;
      &lt;li&gt;finite difference method: 각 element를 조금씩 바꾸면서 loss의 변화를 보고 gradient 계산 –&amp;gt; 겁나 느림&lt;/li&gt;
      &lt;li&gt;보통은 수식적으로 미분식을 구해서 gradient 구함. numerical gradient는 gradient check 용도로 사용 가능함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;gradient descent&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/s7v814joi928whw/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-19%2000.13.56.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;32-stochastic-gradient-descent&quot;&gt;3.2 stochastic gradient descent&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;모든 데이터에 대해서 한꺼번에 gradient를 계산하는 건 too expensive&lt;/li&gt;
  &lt;li&gt;따라서 minibatch를 만들어서 이 데이터에 대해 gradient 계산. 보통 32, 64, 128을 많이 사용함.&lt;/li&gt;
  &lt;li&gt;(random하게 minibatch를 sampling할 경우에 전체 데이터셋에 대해 계산한 gradient와 expectation 값이 같을 것이므로 결국 비슷하게 수렴할 수 있음. 또한 현재 데이터에 대해서 잘 학습하는 것보다 일반화 성능을 높이는 것이 머신러닝과 딥러닝의 목표임. stochastic하게 gradient를 계산하는 것 자체가 일반화 성능에 도움을 준다는 내용도 어딘가에서 봤음.) 
&lt;img src=&quot;https://www.dropbox.com/s/ggkzs4p4mahlp95/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-19%2000.16.36.png?raw=1&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Fri, 18 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/lecture/2018/05/18/cs231n-lecture3/</link>
        <guid isPermaLink="true">http://localhost:4003/lecture/2018/05/18/cs231n-lecture3/</guid>
        
        <category>cs231n</category>
        
        
        <category>lecture</category>
        
      </item>
    
      <item>
        <title>Kickstarting Deep Reinforcement Learning</title>
        <description>&lt;h2 id=&quot;논문-제목-kickstartking-deep-reinforcement-learning-2018-march&quot;&gt;논문 제목: Kickstartking Deep Reinforcement Learning [2018 March]&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/b0dp6tse95zudzw/Screenshot%202018-10-09%2020.54.11.png?dl=1&quot; /&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;논문 저자: Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun(Microsoft Research)&lt;/li&gt;
  &lt;li&gt;논문 링크: &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;https://arxiv.org/pdf/1512.03385.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;1-abstract&quot;&gt;1. Abstract&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;이전에 학습한 “teacher” agent가 새롭게 학습하는 “student” agent를 kickstart&lt;/li&gt;
  &lt;li&gt;이를 위해 policy distillation, population-based training의 아이디어를 차용&lt;/li&gt;
  &lt;li&gt;본 논문에서 제시하는 방법은 teacher와 student agent의 architecture에 제약을 가하지 않음. student가 스스로 teacher를 surpass 하도록 함&lt;/li&gt;
  &lt;li&gt;kickstart된 새로운 agent의 data efficiency 향상&lt;/li&gt;
  &lt;li&gt;각각의 task에 최적화된 여러 teacher로부터 하나의 student가 kickstarting 가능&lt;/li&gt;
  &lt;li&gt;바닥부터 학습한 agent에 비해서 10배 빠른 학습, 42% 높은 성능&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;2-conclusion&quot;&gt;2. Conclusion&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;policy-based RL 환경에서 implement하기 쉬움&lt;/li&gt;
  &lt;li&gt;policy distillation에 비교해서 student가 스스로 teacher로부터의 조언을 가지고 learning objective를 균형잡음. 따라서 teacher보다 더 좋은 성능을 냄.&lt;/li&gt;
  &lt;li&gt;이전에 학습한 agent의 지식을 흡수하고 사용하는 새로운 agent –&amp;gt; 새로운 연구 방향&lt;/li&gt;
  &lt;li&gt;혼자서 학습하지 못하는 complex task도 학습 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;3-introduction&quot;&gt;3. Introduction&lt;/h2&gt;
&lt;hr /&gt;
&lt;ul&gt;
  &lt;li&gt;환경에서 여러가지를 경험하고 그 경험을 모으는 과정이 time-consuming &amp;amp; risk가 있음&lt;/li&gt;
  &lt;li&gt;complex task를 학습하려 할 때 이런 점이 문제가 됌. 거의 billion 단위의 학습 step 소요.&lt;/li&gt;
  &lt;li&gt;다른 agent를 teacher로 해서 어떻게 하면 배울 수 있을까가 관심사
    &lt;ul&gt;
      &lt;li&gt;supervised learning에서는 weight transfer라는 게 있긴 함(보통 transfer 	learning 이라고 부르는 것). 하지만 강화학습에서는 잘 안되었음(왜인지 궁금하다!)&lt;/li&gt;
      &lt;li&gt;그래서 그 잘 안되는 문제를 해결한게 이 논문에서 제시하는 “kickstarting” 방법&lt;/li&gt;
      &lt;li&gt;expert 자체(teacher의 다른 표현이다)가 아닌 그 경험 자체로부터 학습하던 imitation 	learning 과는 다름(imitation learning이 문제가 있었으니 다른 방법인 kickstarting을	제시했겠지? 그렇다면 imitation learning의 문제가 무엇이었을까? DARLA 논문을 읽어보면 	좋을 듯)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;policy distillation과 population based training의 아이디어를 합쳐봄(각각의 아이디어가 뭔지를 아는 것도 좋을 듯). policy distillation 과는 달리 student와 teacher architecture에 제약을 안주고 자동으로 teacher가 student에게 영향을 주도록 함.&lt;/li&gt;
  &lt;li&gt;experiment는 뒤에서 언급&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;4-kickstarking-rl&quot;&gt;4. Kickstarking RL&lt;/h2&gt;
&lt;hr /&gt;
&lt;h3 id=&quot;41-knowledge-transfer&quot;&gt;4.1 Knowledge transfer&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;기본적으로 pre-trained agent가 이용가능하다고 가정 (혹은 특정 specific task에 특화된 expert agent)&lt;/li&gt;
  &lt;li&gt;특정되어있지 않은 구조를 가진 새로운 agent를 학습하고 싶은데 teacher를 이용해서 (1) faster learning (2) higher performance 를 가지게 하고 싶음&lt;/li&gt;
  &lt;li&gt;간단히 말하자면 student가 샘플링한 trajectory에 대해서 student policy와 teacher policy의 차이를 나타내는 auxiliary loss function을 고려한다.
    &lt;ul&gt;
      &lt;li&gt;이 loss function은 기존 objective function에 더해져서 뉴럴넷 학습에 사용된다.&lt;/li&gt;
      &lt;li&gt;대신 auxiliary loss function에 weight를 곱해준다.&lt;/li&gt;
      &lt;li&gt;weight의 역할은 전체 학습 비중이 점점 teacher의 policy를 따라하는게 아닌 expected reward를 높이는 방향으로 이동하도록 한다.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;kickstarting의 핵심은 knowledge transfer mechanism 이다.
    &lt;ul&gt;
      &lt;li&gt;가장 유명한 knowledge transfer mechanism이 policy distillation이다.&lt;/li&gt;
      &lt;li&gt;teacher policy: &lt;code class=&quot;MathJax_Preview&quot;&gt;\pi_T&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi_T&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;teacher가 generate한 trajectory: &lt;code class=&quot;MathJax_Preview&quot;&gt;(x_t)_{t&amp;gt;=0}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;(x_t)_{t&gt;=0}&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;student policy: &lt;code class=&quot;MathJax_Preview&quot;&gt;\pi_S&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\pi_S&lt;/script&gt; parameterized by &lt;code class=&quot;MathJax_Preview&quot;&gt;w&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;w&lt;/script&gt;&lt;/li&gt;
      &lt;li&gt;student policy를 teacher policy에 가깝게 만드는 loss function: distill loss, 다음과 같다.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;H&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; 은 cross entropy를 의미함.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;l_{distill}(w,x,t)=H(\pi_T(a \vert x_t) \Vert \pi_S(a \vert x_t, w))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_{distill}(w,x,t)=H(\pi_T(a \vert x_t) \Vert \pi_S(a \vert x_t, w))&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;하지만 단지 이 loss function은 student가 teacher를 모방하게 만들 뿐임.&lt;/li&gt;
  &lt;li&gt;student가 teacher로부터 도움을 받으면서 스스로 standard RL objective를 높이도록 하고 싶음&lt;/li&gt;
  &lt;li&gt;보통 expected return을 많이 objective로 사용함: &lt;code class=&quot;MathJax_Preview&quot;&gt;E_{\pi_S}[R]&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;E_{\pi_S}[R]&lt;/script&gt;,  &lt;code class=&quot;MathJax_Preview&quot;&gt;R=\sum_{t&amp;gt;=0}\gamma^tr_t&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;R=\sum_{t&gt;=0}\gamma^tr_t&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;따라서 expected return에 대한 loss term과 distill loss를 weighted sum한다. &lt;code class=&quot;MathJax_Preview&quot;&gt;\lambda_k &amp;gt;= 0&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\lambda_k &gt;= 0&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;l_{kick}^k=l_{RL}(w, x, t) + \lambda_kH(\pi_T(a \vert x_t) \Vert \pi_S(a \vert x_t, w))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_{kick}^k=l_{RL}(w, x, t) + \lambda_kH(\pi_T(a \vert x_t) \Vert \pi_S(a \vert x_t, w))&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;policy distillation과는 달리 trajectory를 student policy에 따라 sampling 함&lt;/li&gt;
  &lt;li&gt;auxiliary loss는 다른 관점에서 보면 A3C의 entropy regularization과 같은 맥락으로 볼 수 있음
    &lt;ul&gt;
      &lt;li&gt;A3C loss: &lt;code class=&quot;MathJax_Preview&quot;&gt;D_{KL}(\pi_S(a \vert x_t, w) \Vert U)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;D_{KL}(\pi_S(a \vert x_t, w) \Vert U)&lt;/script&gt;, &lt;code class=&quot;MathJax_Preview&quot;&gt;U&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;U&lt;/script&gt;는 uniform distribution&lt;/li&gt;
      &lt;li&gt;distill loss: &lt;code class=&quot;MathJax_Preview&quot;&gt;D_{KL}(\pi_T(a \vert x_t, w) \Vert \pi_S(a \vert x_t, w))&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;D_{KL}(\pi_T(a \vert x_t, w) \Vert \pi_S(a \vert x_t, w))&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;다음 그림에서 첫 번째 그림은 모든 task를 한꺼번에 학습하는 보통의 RL agent 그림임. 두 번째 그림은 student 하나, teacher 하나인 에이전트임. 세 번째는 student 하나, teacher 3인 상황에서의 학습을 그린 것임. knowledge transfer의 흐름을 보기. 
&lt;img src=&quot;https://www.dropbox.com/s/jd85p8mjbkp6yta/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-17%2018.06.09.png?raw=1&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;42-kickstarting-actor-critic&quot;&gt;4.2 Kickstarting Actor-Critic&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;A3C의 loss를 다시 표현해보겠음
    &lt;ul&gt;
      &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;v_{t+1}&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;v_{t+1}&lt;/script&gt; : value function target&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;MathJax_Preview&quot;&gt;V(x_t \vert \theta)&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;V(x_t \vert \theta)&lt;/script&gt; : value approximation computed by critic network&lt;/li&gt;
      &lt;li&gt;critic의 loss function: &lt;code class=&quot;MathJax_Preview&quot;&gt;\Vert V(x_t \vert \theta)-v_{t}\Vert_2^2&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\Vert V(x_t \vert \theta)-v_{t}\Vert_2^2&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;l_{A3C}(w, x, t)=log\pi_S(a_t \vert s_t, w)(r_t + \gamma v_{t+1} - V(x_t \vert \theta)) - \beta H(\pi_S(a \vert x_t, w))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_{A3C}(w, x, t)=log\pi_S(a_t \vert s_t, w)(r_t + \gamma v_{t+1} - V(x_t \vert \theta)) - \beta H(\pi_S(a \vert x_t, w))&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;이 때, A3C Kickstarting loss는 다음과 같음&lt;/li&gt;
&lt;/ul&gt;

&lt;pre class=&quot;MathJax_Preview&quot;&gt;&lt;code&gt;l_{A3C}(w, x, t) + \lambda_kH(\pi_T(a \vert x_t) \Vert \pi_S(a \vert x_t, w))&lt;/code&gt;&lt;/pre&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;l_{A3C}(w, x, t) + \lambda_kH(\pi_T(a \vert x_t) \Vert \pi_S(a \vert x_t, w))&lt;/script&gt;

&lt;h3 id=&quot;43-population-based-training&quot;&gt;4.3 Population based training&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Kickstarting에서 중요한 것은 바로 loss에서 &lt;code class=&quot;MathJax_Preview&quot;&gt;\lambda_k&lt;/code&gt;&lt;script type=&quot;math/tex&quot;&gt;\lambda_k&lt;/script&gt;의 자동 스케줄링임&lt;/li&gt;
  &lt;li&gt;기존에는 사람이 직접 schedule을 짰는데 그러면 또 추가로 전문지식이 필요함&lt;/li&gt;
  &lt;li&gt;만약 teacher가 한 agent가 아니고 여러 agent면 손으로 schedule 짜기가 더 어려움&lt;/li&gt;
  &lt;li&gt;population based training이 이것을 자동으로 해줌
    &lt;ul&gt;
      &lt;li&gt;다양한 hyper parameter를 가지는 population을 만듬&lt;/li&gt;
      &lt;li&gt;이 중에서 랜덤으로 골라서 학습&lt;/li&gt;
      &lt;li&gt;성능이 다른 놈보다 월등히 높은 놈이 있으먼 선택&lt;/li&gt;
      &lt;li&gt;선택한 놈의 hyper parameter로 바로 대체하기보다는 조금 이동함&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;5-experiment&quot;&gt;5. Experiment&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;IMPALA 에이전트로 DMLab-30 task에서 테스트함&lt;/li&gt;
  &lt;li&gt;IMPALA 에이전트
    &lt;ul&gt;
      &lt;li&gt;visual input에 대해서는 convolution + lstm&lt;/li&gt;
      &lt;li&gt;language input에 대해서는 lstm&lt;/li&gt;
      &lt;li&gt;두 개의 output을 concat, 그 다음 fully connected로 actor, critic output&lt;/li&gt;
      &lt;li&gt;small agent는 2개의 conv layer&lt;/li&gt;
      &lt;li&gt;large agent는 15개의 conv layer&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DMLab-30 task
    &lt;ul&gt;
      &lt;li&gt;딥마인드에서 만든 30개의 간단한 task 들&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/1tstdy9c0tivlgk/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2010.16.04.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;1 high-capacity learner worker&lt;/li&gt;
  &lt;li&gt;150 actor worker&lt;/li&gt;
  &lt;li&gt;worker는 task당 5개로 distributed (어마어마한 실험 환경임)&lt;/li&gt;
  &lt;li&gt;학습 평가는 사람이 했을 때의 점수와 비교함&lt;/li&gt;
  &lt;li&gt;첫 번째 그림은 바닥부터 학습한 에이전트와 single teacher를 통해 학습한 student agent의 성능을 비교한 그래프임. 주황색 그래프가 teacher agent가 되었다고 생각했을 때 teacher의 final score에 도달하는 시간에 비해 student는 1/10의 속도로 도달함.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;두 번째 그래프는 kickstarting distillation weight의 evolution을 보여줌
&lt;img src=&quot;https://www.dropbox.com/s/7jn1jg0ibvx9nvi/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2010.21.47.png?raw=1&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;distillation weight의 scheduling의 효과는 다음 그래프에서 볼 수 있음. 생각보다 critical 한 것 같음. constant로 사용할 경우 supervised learning인 policy distillation과 거의 차이 없음.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://www.dropbox.com/s/ozhs48i8so4cevm/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-18%2010.29.04.png?raw=1&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;6-evaluation&quot;&gt;6. Evaluation&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;기존 knowledge transfer 방법이 거의 imitation learning에 가까웠다면 kickstarting은 reinforcement learning과 적절히 섞은 점이 장점&lt;/li&gt;
  &lt;li&gt;기존 A3C나 IMPALA 같은 에이전트에 쉽게 implement 가능&lt;/li&gt;
  &lt;li&gt;추가적으로 weight scheduling이 필요한데 이게 성능에 영향을 많이 주는 것을 봐서는 현재 방법이 아닌 다른 방법으로 추가적으로 성능 개선이 가능하지 않을까 싶음&lt;/li&gt;
  &lt;li&gt;teacher가 있는 상황이 많지 않을 것 같다는 생각이 듬(현실적일까..?)&lt;/li&gt;
  &lt;li&gt;multiple teacher를 두는 것은 자원이 충분하지 않은 상황에서 오히려 전반적인 프로세스의 크기를 키우는 게 아닐까 싶음.&lt;/li&gt;
  &lt;li&gt;바닥부터 스스로 학습하면서 자아를 분리해서 하나는 teacher로 하나는 student가 되는 방법은 어떨까 싶음&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Thu, 17 May 2018 00:00:00 +0900</pubDate>
        <link>http://localhost:4003/paper/2018/05/17/kickstarting-deeprl/</link>
        <guid isPermaLink="true">http://localhost:4003/paper/2018/05/17/kickstarting-deeprl/</guid>
        
        <category>rl</category>
        
        
        <category>paper</category>
        
      </item>
    
  </channel>
</rss>
