<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v6.4.0 (https://qwtel.com/hydejack/)
-->









<head>
  <!-- =============== -->
<!-- META            -->
<!-- =============== -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="naver-site-verification" content="22c5b4ef3bfda7fc6100671413989219de7a4ac8"/>
<meta property="og:title" content="Model-Ensemble Trust-Region Policy Optimization">
<meta property="og:type" content="article">





  <meta property="og:image" content="http://localhost:4003/assets/img/logo.png">


<meta property="og:image:width" content="640" />
<meta property="og:image:height" content="360" />



  <title>Model-Ensemble Trust-Region Policy Optimization &middot; Woongwon Lee</title>



<meta name="description" content="Model-Ensemble Trust-Region Policy Optimization [2018]

">
<meta property="og:description" content="Model-Ensemble Trust-Region Policy Optimization [2018]

">



<!-- =============== -->
<!-- LINKS           -->
<!-- =============== -->
<link rel="canonical" href="http://localhost:4003/paper/2018/05/30/me-trpo/">
<meta property="og:url" content="http://localhost:4003/paper/2018/05/30/me-trpo/">

<link rel="alternate" type="application/atom+xml" title="Woongwon Lee Feed" href="http://localhost:4003/feed.xml">


  <link rel="prev" href="http://localhost:4003/paper/2018/05/28/trust-pcl/">



  <link rel="next" href="http://localhost:4003/paper/2018/06/08/npg-suppliment/">


<link rel="apple-touch-icon" href="http://localhost:4003/apple-touch-icon.png">
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
<!-- Place favicon.ico in the root directory -->

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->
<script>
  !function(n,e){function t(n,e){n.onload=function(){this.onerror=this.onload=null,e(null,n)},n.onerror=function(){this.onerror=this.onload=null,e(new Error("Failed to load "+this.src),n)}}function o(n,e){n.onreadystatechange=function(){"complete"!=this.readyState&&"loaded"!=this.readyState||(this.onreadystatechange=null,e(null,n))}}n._loaded=!1,n.loadJSDeferred=function(a,d){function r(){n._loaded=!0;var r=e.createElement("script");r.src=a,d&&(("onload"in r?t:o)(r,d),r.onload||t(r,d));var l=e.getElementsByTagName("script")[0];l.parentNode.insertBefore(r,l)}n._loaded?r():n.addEventListener?n.addEventListener("load",r,!1):n.attachEvent?n.attachEvent("onload",r):n.onload=r}}(window,document);

  !function(e){"use strict";var n=function(n,t,o){function i(e){if(a.body)return e();setTimeout(function(){i(e)})}function r(){l.addEventListener&&l.removeEventListener("load",r),l.media=o||"all"}var d,a=e.document,l=a.createElement("link");if(t)d=t;else{var f=(a.body||a.getElementsByTagName("head")[0]).childNodes;d=f[f.length-1]}var s=a.styleSheets;l.rel="stylesheet",l.href=n,l.media="only x",i(function(){d.parentNode.insertBefore(l,t?d:d.nextSibling)});var u=function(e){for(var n=l.href,t=s.length;t--;)if(s[t].href===n)return e();setTimeout(function(){u(e)})};return l.addEventListener&&l.addEventListener("load",r),l.onloadcssdefined=u,u(r),l};"undefined"!=typeof exports?exports.loadCSS=n:e.loadCSS=n}("undefined"!=typeof global?global:this);

  !function(t){if(t.loadCSS){var e=loadCSS.relpreload={};if(e.support=function(){try{return t.document.createElement("link").relList.supports("preload")}catch(t){return!1}},e.poly=function(){for(var e=t.document.getElementsByTagName("link"),r=0;r<e.length;r++){var n=e[r];"preload"===n.rel&&"style"===n.getAttribute("as")&&(t.loadCSS(n.href,n,n.getAttribute("media")),n.rel=null)}},!e.support()){e.poly();var r=t.setInterval(e.poly,300);t.addEventListener&&t.addEventListener("load",function(){e.poly(),t.clearInterval(r)}),t.attachEvent&&t.attachEvent("onload",function(){t.clearInterval(r)})}}}(this);

  window.disablePushState = false;
  window.disableDrawer = false;
</script>
<!--
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-5105555504153863",
    enable_page_level_ads: true
  });
</script>
-->
<!--[if lt IE 9]>
<script src="https://unpkg.com/html5shiv/dist/html5shiv.min.js"></script>
<![endif]-->

<!-- =============== -->
<!-- STYLES          -->
<!-- =============== -->
<!--[if gt IE 8]><!---->
<style>
  
  article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}*{box-sizing:border-box}html,body{margin:0;padding:0}html{font-size:16px;line-height:1.75}body{color:#333;background-color:#fff;overflow-y:scroll}a{text-decoration:none}.lead{margin-left:-1rem;margin-right:-1rem}img,.img{display:block;max-width:100%;margin-bottom:1rem;border:none}img.lead,.img.lead{max-width:calc(100% + 2rem);width:calc(100% + 2rem)}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-weight:bold;text-rendering:optimizeLegibility}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6{margin:1.6rem 0 1rem;line-height:1.6}h1,.h1{font-size:2rem;line-height:1.25}h2,.h2{font-size:1.5rem}h3,.h3{font-size:1.17em}p{margin-top:0;margin-bottom:1rem}p.lead{font-size:1.25rem;font-weight:300;padding:0 1rem}ul,ol,dl{margin-top:0;margin-bottom:1rem}ul,ol{padding-left:1.25rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee}.hr{padding-bottom:.5rem;border-bottom:1px solid #eee;margin-bottom:1.5rem}h4,h5,h6,.h4,.h5,.h6{margin-bottom:0.5rem;font-size:1rem}table{margin-bottom:1rem;width:100%;width:calc(100% + 2rem);margin-left:-1rem;border:1px solid #e5e5e5;border-collapse:collapse;border-spacing:0}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}td:first-child,th:first-child{padding-left:1rem}td:last-child,th:last-child{padding-right:1rem}thead+tbody,tbody+tbody,tfoot{border-top:3px double #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}footer{margin-bottom:2rem}.page,.post{margin-bottom:3em}.page li+li,.post li+li{margin-top:.25rem}.page>header,.post>header{margin-bottom:2rem}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-0.5rem;margin-bottom:1rem;color:#9a9a9a}.related-posts{padding-left:0;list-style:none}.related-posts>li,.related-posts>li+li{margin-top:1rem}.related-posts>li>small,.related-posts>li+li>small{font-size:75%;color:#9a9a9a}.message{margin-bottom:1rem;padding:1rem;color:#787878;background-color:#f9f9f9;margin-left:-1rem;margin-right:-1rem}body,main{position:relative;overflow-x:hidden}@media screen{body::before{content:'';background:#e5e5e5;position:absolute;left:0;top:0;bottom:0}}@media screen and (min-width: 40em){html{font-size:17px}}@media screen and (min-width: 54em){html{font-size:16px}}@media screen and (min-width: 88em){html{font-size:17px}}@media screen and (min-width: 125em){html{font-size:18px}}.sr-only{display:none}.clearfix,.sidebar-social::after,.clearafter::after{content:"";display:table;clear:both}a,.a{position:relative;padding-bottom:.15rem;border-style:hidden}.img{overflow:hidden;background-color:#f9f9f9}.img>img{margin:0;width:100%;height:100%}.sixteen-nine{position:relative}.sixteen-nine::before{display:block;content:"";width:100%;padding-top:56.25%}.sixteen-nine>*{position:absolute;top:0;left:0;right:0;bottom:0}h1+hr,h2+hr,h3+hr,h4+hr,h5+hr,h6+hr{margin-top:0}.fade-in{animation-duration:500ms;animation-name:fade-in;animation-fill-mode:forwards}@keyframes fade-in{from{transform:translateY(-2rem);opacity:0}50%{transform:translateY(-2rem);opacity:0}to{transform:translateY(0);opacity:1}}.mb6{margin-bottom:10rem}.sidebar{color:rgba(255,255,255,0.75);text-align:left}.sidebar::before{content:"";position:absolute;z-index:2;top:0;left:0;bottom:0;right:0;background:linear-gradient(to bottom, rgba(32,32,32,0) 0%, rgba(32,32,32,0.5) 100%)}.sidebar a{color:#fff;border-bottom-color:rgba(255,255,255,0.2)}.right-side{width:100%;margin-left:auto;margin-right:auto}.right-side .ad-first{text-align:center}.right-side .ad-second{text-align:center}@media screen{.right-side{max-width:38rem;min-height:100vh}}@media screen and (min-width: 54em){.right-side{margin-left:20rem;margin-right:1rem;padding:4rem 1rem 12rem}}@media screen and (min-width: 72em){.right-side{margin-left:22rem;max-width:42rem}}@media screen and (min-width: 88em){.right-side{width:162px;margin-left:0rem;margin-right:0rem;padding:0rem;margin-top:10rem;display:block;float:left}}@media screen and (min-width: 96em){.right-side{width:300px;margin-right:0rem}}#_yDrawer{position:relative}@media screen{#_yDrawer{min-height:640px;min-height:100vh}}@media screen and (min-width: 54em){#_yDrawer{width:18rem;margin-left:0}}.sidebar-bg{position:absolute;height:100%;overflow:hidden;top:0;right:0;bottom:0;left:0;background:#202020 center / cover}.sidebar-box{display:flex;justify-content:center}.sidebar-sticky{position:relative;z-index:3}@media screen{.sidebar-sticky{-ms-overflow-style:none;overflow:-moz-scrollbars-none;height:100%;overflow:auto;position:absolute;padding:3rem 0rem;right:2.5rem;left:2.5rem}}.sidebar-sticky::-webkit-scrollbar{display:none}.sidebar-about>h1{color:#fff;font-size:2rem}.sidebar-nav>ul{list-style:none;padding-left:0;margin-bottom:.5rem}a.sidebar-nav-item{width:100%;font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}a.sidebar-nav-subitem{font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}@media screen{.y-drawer-scrim{z-index:2}.y-drawer-content{width:18rem;left:-18rem;z-index:3}}.sidebar-social{margin-bottom:.5rem}.sidebar-social>ul{list-style:none;padding-left:0;margin:0 -.25rem}.sidebar-social>ul>li{float:left}.sidebar-social>ul>li>a{display:inline-block;text-align:center;font-size:1.6rem;line-height:3rem;width:3.1249rem;height:4rem;padding:.5rem 0}.sidebar-social>ul li+li{margin-top:0}.fixed-top{position:fixed;top:0;left:0;width:100%;z-index:1}.navbar>.content{padding-top:0;padding-bottom:0;min-height:0;height:0}.menu{display:inline-block;padding:1.75rem 1.5rem;border-bottom:none;margin-left:-1.5rem;color:#9a9a9a !important}.menu::after{content:"\2630"}@media screen and (min-width: 54em){.menu{padding:1.25rem 1.5rem;position:absolute;left:-9999px}.menu:focus{position:static}}.animation-main{pointer-events:none}.loading{display:none}@media print{.menu{display:none}}.animation-main{opacity:0;will-change:opacity}.loading{position:absolute;top:0;right:0;padding:5.25rem 4.5rem;transform-origin:top right;transform:scale(0.33)}.content{position:relative;margin-left:auto;margin-right:auto;padding:5rem 1rem 12rem}@media screen{.content{min-height:100vh}}@media screen and (min-width: 54em){.content{padding:4rem 1rem 12rem;margin-left:19rem;margin-right:3rem}}@media screen and (min-width: 72em){.content{max-width:42rem;margin-left:21rem}}@media screen and (min-width: 88em){.content{float:left;width:100%;margin-left:22rem;margin-right:5rem}}@media screen and (min-width: 96em){.content{max-width:44rem}}@media screen and (min-width: 102em){.content{margin-left:25rem;margin-right:8rem}}.me{width:6.5rem;height:6.5rem;align-self:center;margin-right:20px;border-radius:100%;position:relative}@media screen and (min-width: 40em){.me{width:7rem;height:7rem}}@media screen and (min-width: 54em){.me{width:6.5rem;height:6.5rem}}@media screen and (min-width: 72em){.me{width:7rem;height:7rem}}main>footer{width:100%;position:absolute;bottom:0;left:0;right:0;padding:0 1rem;color:#9a9a9a;font-size:smaller;text-align:center}main>footer>p{margin-bottom:0}html{font-family:'Sans-serif'}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-family:'Sans-serif'}

</style>


<link rel="preload" href="http://localhost:4003/assets/css/hydejack.css?v=6.4.0" as="style" onload="this.rel='stylesheet'">

<style id="_pageStyle">

.content a{color:#4f86aa;border-color:rgba(79,134,170,0.2)}.content a:hover{border-color:#4f86aa}:focus{outline-color:#4f86aa}::selection{color:#fff;background:#4f86aa}::-moz-selection{color:#fff;background:#4f86aa}

</style>


<noscript>
  <link rel="stylesheet" href="http://localhost:4003/assets/css/hydejack.css?v=6.4.0">
  
  
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
    <style>
      html { font-family: 'Lato', 'Sans-serif' }
      h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6, .heading { font-family: 'Lato', 'Sans-serif' }
    </style>
  

  
  <link rel="stylesheet" href="http://localhost:4003/assets/icomoon/style.css">
</noscript>
<!--<![endif]-->

</head>

<body>
  <!-- =============== -->
<!-- MENU            -->
<!-- =============== -->
<div class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <a id="_menu" class="menu no-hover" href="#_title">
      <span class="sr-only">Menu</span>
    </a>
  </div>
</div>

<!-- =============== -->
<!-- CONTENT         -->
<!-- =============== -->
<div id="_yPushState">
  <div class="fade-in">
    <main id="_main" class="content" role="main" data-color="#4f86aa" data-image="/assets/img/nap.jpg">
      

<article id="post-paper/2018/05/30/me-trpo" class="post" role="article">
  <header>
    <h1 class="post-title">
      
        Model-Ensemble Trust-Region Policy Optimization
      
    </h1>

    <p class="post-date heading">
      <time datetime="2018-05-30T00:00:00+09:00">30 May 2018</time>
      









in <a href="/category/paper/" data-flip="title">Paper</a>

      









on <a href="/tag/paper-rl/" data-flip="title">Reinforcement-Learning</a>

    </p>

    <script data-ad-client="ca-pub-2252618542833203" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    
  <div class="hr" style="padding-bottom:0"></div>


  </header>
  
    <!--
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  -->
  
    <div class="markdown-body">


        <!--
<style>
.myAd1190 { display:block; width:98%; height: 280px; }
@media(min-width: 600px) { .myAd1190 { display: none; } }
</style>
<ins class="adsbygoogle myAd1190"
    data-ad-client="ca-pub-5105555504153863"
    data-ad-slot="6481487416"
    ></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->


<br/>
      <h1 id="model-ensemble-trust-region-policy-optimization-2018">Model-Ensemble Trust-Region Policy Optimization [2018]</h1>

<p><img src="https://www.dropbox.com/s/jnmnpkwpeq47ugi/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.21.05.png?dl=1" /></p>

<ul>
  <li>논문 저자: Thanard Kurutach(Berkely AI Research)</li>
  <li>논문 링크: <a href="https://openreview.net/pdf?id=SJJinbWRZ">https://openreview.net/pdf?id=SJJinbWRZ</a></li>
  <li>함께 보면 좋을 논문:
    <ul>
      <li><a href="https://arxiv.org/pdf/1502.05477.pdf">Trust Region Policy Optimization (2015)</a></li>
    </ul>
  </li>
  <li>논문을 보는 이유: TRPO와 관련이 있을 것 같아서(결국은 TRPO에 대한 이야기는 거의 없다)</li>
</ul>

<p>개인적인 생각들로 재해석해서 요약한 거라고 생각하시면 될 것 같습니다(부정확할 수 있다는 이야기입니다).</p>

<h2 id="1-abstract">1. Abstract</h2>
<hr />
<ul>
  <li>최근 많은 task에서 model-free RL은 성공적이었다. 하지만 실제 세상에 적용하기에는 학습 데이터가 너무 많이 필요하다.</li>
  <li>model-based RL은 적은 학습 데이터로 학습 가능하다. 하지만 비교적 간단한 task에서만 잘 작동했었다.</li>
  <li>model-based RL에서 DNN으로 model과 policy를 둘 다 학습하면 문제가 생긴다. model을 학습하기에 충분한 데이터가 없는 region에서 policy가 exploit하는 경향이 있는 것이다.</li>
  <li>이런 문제를 해결하기 위해서 여러개의 모델(환경에 대한 모델)을 ensemble로 사용할건데 그러면 model uncertainty를 유지할 수 있으며 regularization 효과를 볼 수 있다.</li>
  <li>model-based RL에서 backprop through time을 사용하지 않고 likelihood ratio derivative를 사용할 것이다. 그 말인즉슨 TRPO를 사용해서 policy를 학습할거라는 이야기.</li>
  <li>이러한 알고리즘을 Model-Ensemble Trust-Region Policy Optimization(ME-TRPO)이라고 부를 것이다.</li>
  <li>continuous control task에서 model-free RL에 비해 ME-TRPO가 더 적은 데이터로도 비슷한 성능을 내도록 학습했다.</li>
</ul>

<h2 id="2-model-based-vs-model-free-rl">2. model-based vs model-free RL</h2>
<hr />
<p>결국 현실 세계에 강화학습을 적용하려면 학습에 들어가는 cost가 중요하다. 보통 사용하는 model-free RL은 제대로된 policy를 학습하려면 많은 데이터가 필요하다. 즉, 많은 시행착오가 필요하다는 것이고 이것이 bottle neck이 된다. 그래서 model-based RL에 대한 연구가 진행이 되어왔다. 간단히 세상에 대한 model을 학습한다고 생각하면 된다. 환경과의 직접적인 상호작용으로 policy를 학습하지 않고 에이전트가 학습한 model에서 상호작용해서 데이터를 모아서 policy를 학습한다. 이 때 환경의 model은 transition probability라고 생각하면 된다. 어떤 상태에서 어떤 행동을 했을 때 다음 어떤 상태가 될지에 대한 정보이다. 이 정보가 있으면 에이전트는 행동을 선택한 다음 다음 상태를 바로 알 수 있다. 그러면 또 다음 상태에서 행동을 선택하는 식으로 일종의 “imagination”이 가능하다고 볼 수 있다.</p>

<h2 id="3-vanilla-model-based-rl">3. vanilla model-based RL</h2>
<hr />
<h3 id="31-model-learning">3.1 model learning</h3>
<p>이 때, model이 처음에는 부정확하기 때문에 적당히 model도 꾸준히 학습해야한다. 그래서 보통 vanilla model-based RL에서는 model learning과 policy learning(혹은 policy optimization)을 반복해서 진행했다. model learning은 지도학습이라고 생각하면 된다. 현재의 policy를 가지고 환경에서 상호작용하면서 데이터를 모은다. 데이터는 (s, a, s’)이라고 생각하면 된다. 이 데이터를 가지고 model(이 논문에서는 DNN을 사용한다)이 supervised learning으로 학습한다. (s, a)를 input으로 넣으면 s’을 모델이 출력하도록 학습하는 것이다.</p>

<p>다음 상태 s’을 그대로 출력하도록 뉴럴넷을 학습하면 문제가 있다. s와 s’이 별 차이가 없는 경우 뉴럴넷은 그냥 s를 외워버린다. 따라서 s’이 아닌 상태의 변화인 (s’-s)를 예측하도록 모델을 학습한다. 환경의 모델을 학습하는 loss function은 다음과 같다. <code class="MathJax_Preview">f_{\phi}(s_t, a_t)</code><script type="math/tex">f_{\phi}(s_t, a_t)</script>는 원래 상태 s에다가 뉴럴넷 아웃풋을 합친 것을 뜻한다.</p>

<p><img src="https://www.dropbox.com/s/bs66h3itve37yya/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.45.56.png?dl=1" /></p>

<h3 id="32-policy-learning">3.2 policy learning</h3>
<p>보통 강화학습의 목표는 expected sum of rewards를 최대화하는 것이다. model-based RL이므로 환경을 approximate model이 있고 이 model을 base로 하는 approximate MDP가 있다. 이 MDP 상에서의 expected sum of rewards를 최대화하도록 policy를 optimize 한다.</p>

<p>다음은 policy optimization의 objective function이다. 
<img src="https://www.dropbox.com/s/q31swo1cmcfh69v/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.49.49.png?dl=1" /></p>

<p>stochastic policy는 다음 논문의 본문에서와 같이 reparameterization trick을 사용해서 나타낸다. 평균과 분산 그리고 노이즈를 통해 gaussian distribution을 나타낸다. 이렇게 나타내는 정책의 형태는 continuous action에 대한 policy라는 것을 기억하자.
<img src="https://www.dropbox.com/s/gk258p80ygvozcm/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.50.32.png?dl=1" /></p>

<p>policy gradient는 다음과 같다. 이 미분을 하려면 backpropagation through time (BPTT)를 해야한다. 이 미분은 문제가 있다. gradient가 explode 하거나 vanish 할 수 있다는 것이다. 그래서 gradient clipping을 사용했다.
<img src="https://www.dropbox.com/s/nco6his3ic7bbgo/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.53.15.png?dl=1" /></p>

<p>model learning과 policy learning을 합쳐서 하나의 알고리즘으로 보자면 다음과 같다. real world에서 데이터를 모아서 model를 학습한 다음에 그 model을 사용해서 policy를 improve 한다. 
<img src="https://www.dropbox.com/s/y269zvjofxq9jja/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-29%2023.56.01.png?dl=1" /></p>

<h2 id="4-me-trpo">4. ME-TRPO</h2>
<hr />
<h3 id="41-vanilla-model-based-rl의-문제">4.1 vanilla model-based RL의 문제</h3>
<ul>
  <li>learned policy often exploits regions where scarce training data is available for the dynamics model</li>
  <li>the predictions can be erroneous to the policy’s advantage –&gt; overfitting</li>
</ul>

<p>즉 model이 부정확해서 overfitting 된다는 것이다. 지도학습에서와 마찬가지로 이는 regularization의 필요를 의미한다.</p>

<h3 id="42-me-trpo">4.2 ME-TRPO</h3>
<p>이 논문에서는 regularization 방법으로 환경의 모델을 여러개 학습해서 ensemble로 사용하는 방법을 제안한다. 여러 모델이 서로 다른 것은 initial parameter이다. 또한 BPTT를 통해 학습하는 것이 아니라 TRPO를 통해 policy를 학습함으로서 policy optimization 과정도 개선했다. 또한 policy의 성능을 평가할 때도 여러개의 모델을 사용한다. 다음과 같이 K 개의 모델에 대해서 policy가 improve 된건지를 체크해서 70%가 improve되었다면 다음 단계로 넘어간다.</p>

<p><img src="https://www.dropbox.com/s/fvnzjllmcwbxdbn/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.05.58.png?dl=1" /></p>

<p>전체 알고리즘은 다음과 같다. 심플하다.
<img src="https://www.dropbox.com/s/5i1ff8akq0p05cr/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.08.28.png?dl=1" /></p>

<h2 id="5-experiment">5. Experiment</h2>
<h3 id="51-무슨-실험">5.1 무슨 실험?</h3>
<p>세가지 실험을 했다.</p>

<ul>
  <li>기존 model-free RL 중에 제일 잘 되는 놈이랑 성능 비교</li>
  <li>기존 model-based RL이 어떻게 실패하는지 테스트, 같은 상황에서 ME-TRPO가 잘 하는지 테스트</li>
  <li>성능에 ensemble로 사용하는 모델 개수가 미치는 영향</li>
</ul>

<p>환경은 다음과 같은 전형적인 continuous control 환경을 사용했다.
<img src="https://www.dropbox.com/s/8mlj27eu5jzga2t/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.11.38.png?dl=1" /></p>

<h3 id="52-실험-결과">5.2 실험 결과</h3>
<ul>
  <li>
    <p>기존 model-free RL 중에 제일 잘 되는 놈이랑 성능 비교. 꽤나 잘 된다.
<img src="https://www.dropbox.com/s/qaion8lqhmyegac/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.12.44.png?dl=1" /></p>
  </li>
  <li>기존 model-based RL이 어떻게 실패하는지 테스트</li>
  <li>
    <p>같은 상황에서 ME-TRPO가 잘 하는지 테스트
<img src="https://www.dropbox.com/s/b7xgibgnurl234a/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.13.50.png?dl=1" /></p>
  </li>
  <li>성능에 ensemble로 사용하는 모델 개수가 미치는 영향
<img src="https://www.dropbox.com/s/wlzrl9rxmp8hr5b/%EC%8A%A4%ED%81%AC%EB%A6%B0%EC%83%B7%202018-05-30%2000.14.50.png?dl=1" /></li>
</ul>

<h3 id="6-discussion">6. Discussion</h3>
<ul>
  <li>여러개의 challengin domain에서 학습하는 간단하고 안정적인 model-based RL 알고리즘 = ME-TRPO. 핵심은 Model을 ensemble로 학습해서 policy 학습한 것과 policy 학습할 때 TRPO를 쓴 것.</li>
  <li>model-free RL의 state-of-art랑 비교했을 때 sample complexity를 상당히 줄임, 동일한 performance</li>
  <li>model bias를 model의 uncertainty를 유지함으로서 줄임</li>
</ul>

<p>[앞으로 가능한 연구]</p>

<ul>
  <li>서로 다른 model이 disagree하는 state space를 더 탐험하도록 하는 것</li>
  <li>ME-TRPO를 real world에 적용하는 것</li>
</ul>

      <br/>
      <br/>
        <!--블로그-하단-반응형
      <ins class="adsbygoogle"
          style="display:block; width:100%; height:300px;"
          data-ad-client="ca-pub-5105555504153863"
          data-ad-slot="1037589043"
          data-ad-format="auto"></ins>
      <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      -->
    </div>
  

</article>

  <hr class="dingbat" />

  <div class="share">
      <h2>Share this post</h2>
      <div class="share-body">
        <a href="http://twitter.com/share?text=Model-Ensemble Trust-Region Policy Optimization&amp;url=http://localhost:4003/paper/2018/05/30/me-trpo/"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
            <span class="icon-twitter">
            </span>
        </a>
        <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4003/paper/2018/05/30/me-trpo/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
            <span class="icon-facebook">
            </span>
        </a>
    </div>
  </div>
  <br/>






  <aside class="author" role="complementary">
    <div class="author">
  <h2 class="page-title hr">
    About
  </h2>
<div class="author-body">
  
    
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="woongwon lee"
    srcset="/assets/img/me.jpeg 1x,/assets/img/me.jpeg 2x"
    
  />


  
  <div class="author-body-description">
    <p>Machine Learning Engineer</p>

  </div>
</div>
</div>

  </aside>





<aside class="related" role="complementary">
  <h2 class="hr">Related Posts</h2>

  <ul class="related-posts">
    
      
      
      
        
        
          


<li class="h4">
  <a href="/paper/2018/10/14/arxiv_new/" data-flip="title">
    <span>Arxiv New 2018.10.14</span>
  </a>
  <small><time datetime="2018-10-14T00:00:00+09:00">
    14 Oct 2018
  </time></small>
</li>

        
      
        
        
          


<li class="h4">
  <a href="/paper/2018/07/29/nkn/" data-flip="title">
    <span>Neural Kinematic Networks for Unsupervised Motion Retargetting</span>
  </a>
  <small><time datetime="2018-07-29T00:00:00+09:00">
    29 Jul 2018
  </time></small>
</li>

        
      
        
        
          


<li class="h4">
  <a href="/paper/2018/07/19/youtube/" data-flip="title">
    <span>Playing hard exploration games by watching YouTube</span>
  </a>
  <small><time datetime="2018-07-19T00:00:00+09:00">
    19 Jul 2018
  </time></small>
</li>

        
      
        
        
      
    
  </ul>
</aside>



      
        <aside class="comments" role="complementary">
  <h2>Comments</h2>
  <hr/>

  <div id="disqus_thread"></div>

  <script>
    !function(s,i){function e(e){var t=s.pageYOffset||i.body.scrollTop;s.DISQUS&&!s._disqusThis&&!s._disqusFirst&&t+s.innerHeight>=s._disqusThreadOffsetTop&&(s._disqusThis=!0,s.DISQUS.reset({reload:!0,config:d}))}var d=function(){this.page.title="Model-Ensemble Trust-Region Policy Optimization",this.page.identifier="/paper/2018/05/30/me-trpo",this.page.url="http://localhost:4003/paper/2018/05/30/me-trpo/"};s._disqusFirst=void 0===s._disqusFirst||s._disqusFirst,s._disqusLoading=void 0!==s._disqusLoading&&s._disqusLoading,s._disqusThis=!1,s._disqusThreadOffsetTop=i.getElementById("disqus_thread").offsetTop,s._disqusLoading?s._disqusFirst=!1:(s._disqusLoading=!0,loadJSDeferred("//dnddnjs.disqus.com/embed.js"),s.addEventListener?s.addEventListener("scroll",e,{passive:!0}):s.attachEvent?s.attachEvent("onscroll",e):s.onscroll=e)}(window,document);

  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</aside>

      

      <footer>
  <hr/>
  
    <p>© 2018. by Woongwon Lee</p>

  
  <p>
    <code>Powered by <a href="https://dnddnjs.github.io/">dnddnjs</a></code>
  </p>
</footer>

    </main>
    <!--<div class="right-side">
  <div class="ad-first">
    <!--
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- 블로그-상단-모바일
    <ins class="adsbygoogle"
         style="display:inline-block;width:100%;"
         data-ad-client="ca-pub-5105555504153863"
         data-ad-slot="9090558636"
         data-ad-format="auto"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
    -->
  </div>
<br/>
<br/>
  <!--
  <div class="ad-second">
    <!-- 블로그-스카이스크래퍼
    <ins class="adsbygoogle"
         style="display:inline-block;max-width:320px;width:100%;height:600px"
         data-ad-client="ca-pub-5105555504153863"
         data-ad-slot="3646660262"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
    -->
  </div>
</div>

-->
  </div>
  <div id="_yDrawer">
  <div id="_sidebar" class="sidebar">
    <div class="sidebar-bg" style="background-color:#4f86aa;background-image:url(/assets/img/nap.jpg)"></div>
    <header class="sidebar-sticky" role="banner">
      <br/>
      <div class="sidebar-about">
        <h1><a id="_title" href="/">Woongwon Lee</a></h1>
        <p>Machine Learning Engineer</p>

      </div>

      <br/>
      <br/>
      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  

  

  
  
  
  
  
    <li>
      <input type="checkbox" id="list-item-1"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/paper/">Paper</a>
       <label class="folder" for="list-item-1">▾</label>
    </div>
     <ul class="list-body">
       
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/paper-rl/">Reinforcement-Learning</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/paper-dl/">Deep-Learning</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/paper-thinking/">Thinking</a>
             </li>
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-2"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/about/">About</a>
       
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
</ul>

      </nav>
    <br/>
    <br/>
      <div class="sidebar-box">
        
          
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="woongwon lee"
    srcset="/assets/img/me.jpeg 1x,/assets/img/me.jpeg 2x"
    
  />


        
      </div>
      <p>Reinforce Yourself</p>

      
      
        <div class="sidebar-social">
          <span class="sr-only">Social:</span>
<ul>
  
    









<li>
  <a href="https://facebook.com/dnddnjs">
    <span class="icon-facebook" title="Facebook"></span>
    <span class="sr-only">Facebook</span>
  </a>
</li>

  
    









<li>
  <a href="https://github.com/dnddnjs">
    <span class="icon-github" title="GitHub"></span>
    <span class="sr-only">GitHub</span>
  </a>
</li>

  
    









<li>
  <a href="mailto:dnddnjs11@naver.com">
    <span class="icon-mail" title="Email"></span>
    <span class="sr-only">Email</span>
  </a>
</li>

  
</ul>

        </div>
      
    </header>
  </div>
</div>

</div>

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->

<script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-120080215-1', 'auto');
  ga('send', 'pageview');
  loadJSDeferred('https://www.google-analytics.com/analytics.js');
</script>





<!--[if gt IE 8]><!---->
<script src="//ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script>
<script>
  WebFont.load({
    
    google: {
      families: 'Lato'.split('|')
    },
    

    custom: {
      families: ['icomoon'],
      urls: ['/assets/icomoon/style.css']
    }
  });
</script>
<!--<![endif]-->


  <!--[if gt IE 9]><!---->
  
  <script>loadJSDeferred('/assets/js/hydejack.js?v=6.4.0');</script>

  
  <!--<![endif]-->



</body>

</html>
