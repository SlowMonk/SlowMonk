<!DOCTYPE html>
<html lang="en"><!--
 __  __                __                                     __
/\ \/\ \              /\ \             __                    /\ \
\ \ \_\ \   __  __    \_\ \      __   /\_\      __       ___ \ \ \/'\
 \ \  _  \ /\ \/\ \   /'_` \   /'__`\ \/\ \   /'__`\    /'___\\ \ , <
  \ \ \ \ \\ \ \_\ \ /\ \L\ \ /\  __/  \ \ \ /\ \L\.\_ /\ \__/ \ \ \\`\
   \ \_\ \_\\/`____ \\ \___,_\\ \____\ _\ \ \\ \__/.\_\\ \____\ \ \_\ \_\
    \/_/\/_/ `/___/> \\/__,_ / \/____//\ \_\ \\/__/\/_/ \/____/  \/_/\/_/
                /\___/                \ \____/
                \/__/                  \/___/

Powered by Hydejack v6.4.0 (https://qwtel.com/hydejack/)
-->









<head>
  <!-- =============== -->
<!-- META            -->
<!-- =============== -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="format-detection" content="telephone=no">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="naver-site-verification" content="22c5b4ef3bfda7fc6100671413989219de7a4ac8"/>
<meta property="og:title" content="VAE Tutorial 2">
<meta property="og:type" content="article">





  <meta property="og:image" content="http://localhost:4003/assets/img/logo.png">


<meta property="og:image:width" content="640" />
<meta property="og:image:height" content="360" />



  <title>VAE Tutorial 2 &middot; Woongwon Lee</title>



<meta name="description" content="
  VAE Tutorial 목차
    
      Tutorial 1: CS231n 강의 내용
      Tutorial 2: VAE 논문 &amp; 코드 리뷰
      Tutorial 3: SentenceVAE
      Tutorial 4: MusicVAE
    
  


">
<meta property="og:description" content="
  VAE Tutorial 목차
    
      Tutorial 1: CS231n 강의 내용
      Tutorial 2: VAE 논문 &amp; 코드 리뷰
      Tutorial 3: SentenceVAE
      Tutorial 4: MusicVAE
    
  


">



<!-- =============== -->
<!-- LINKS           -->
<!-- =============== -->
<link rel="canonical" href="http://localhost:4003/paper/2018/06/20/vae2/">
<meta property="og:url" content="http://localhost:4003/paper/2018/06/20/vae2/">

<link rel="alternate" type="application/atom+xml" title="Woongwon Lee Feed" href="http://localhost:4003/feed.xml">


  <link rel="prev" href="http://localhost:4003/paper/2018/06/19/vae/">



  <link rel="next" href="http://localhost:4003/paper/2018/06/21/vae3/">


<link rel="apple-touch-icon" href="http://localhost:4003/apple-touch-icon.png">
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?">
<!-- Place favicon.ico in the root directory -->

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->
<script>
  !function(n,e){function t(n,e){n.onload=function(){this.onerror=this.onload=null,e(null,n)},n.onerror=function(){this.onerror=this.onload=null,e(new Error("Failed to load "+this.src),n)}}function o(n,e){n.onreadystatechange=function(){"complete"!=this.readyState&&"loaded"!=this.readyState||(this.onreadystatechange=null,e(null,n))}}n._loaded=!1,n.loadJSDeferred=function(a,d){function r(){n._loaded=!0;var r=e.createElement("script");r.src=a,d&&(("onload"in r?t:o)(r,d),r.onload||t(r,d));var l=e.getElementsByTagName("script")[0];l.parentNode.insertBefore(r,l)}n._loaded?r():n.addEventListener?n.addEventListener("load",r,!1):n.attachEvent?n.attachEvent("onload",r):n.onload=r}}(window,document);

  !function(e){"use strict";var n=function(n,t,o){function i(e){if(a.body)return e();setTimeout(function(){i(e)})}function r(){l.addEventListener&&l.removeEventListener("load",r),l.media=o||"all"}var d,a=e.document,l=a.createElement("link");if(t)d=t;else{var f=(a.body||a.getElementsByTagName("head")[0]).childNodes;d=f[f.length-1]}var s=a.styleSheets;l.rel="stylesheet",l.href=n,l.media="only x",i(function(){d.parentNode.insertBefore(l,t?d:d.nextSibling)});var u=function(e){for(var n=l.href,t=s.length;t--;)if(s[t].href===n)return e();setTimeout(function(){u(e)})};return l.addEventListener&&l.addEventListener("load",r),l.onloadcssdefined=u,u(r),l};"undefined"!=typeof exports?exports.loadCSS=n:e.loadCSS=n}("undefined"!=typeof global?global:this);

  !function(t){if(t.loadCSS){var e=loadCSS.relpreload={};if(e.support=function(){try{return t.document.createElement("link").relList.supports("preload")}catch(t){return!1}},e.poly=function(){for(var e=t.document.getElementsByTagName("link"),r=0;r<e.length;r++){var n=e[r];"preload"===n.rel&&"style"===n.getAttribute("as")&&(t.loadCSS(n.href,n,n.getAttribute("media")),n.rel=null)}},!e.support()){e.poly();var r=t.setInterval(e.poly,300);t.addEventListener&&t.addEventListener("load",function(){e.poly(),t.clearInterval(r)}),t.attachEvent&&t.attachEvent("onload",function(){t.clearInterval(r)})}}}(this);

  window.disablePushState = false;
  window.disableDrawer = false;
</script>
<!--
<script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<script>
  (adsbygoogle = window.adsbygoogle || []).push({
    google_ad_client: "ca-pub-5105555504153863",
    enable_page_level_ads: true
  });
</script>
-->
<!--[if lt IE 9]>
<script src="https://unpkg.com/html5shiv/dist/html5shiv.min.js"></script>
<![endif]-->

<!-- =============== -->
<!-- STYLES          -->
<!-- =============== -->
<!--[if gt IE 8]><!---->
<style>
  
  article,aside,dialog,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}mark{background:#FF0;color:#000}*{box-sizing:border-box}html,body{margin:0;padding:0}html{font-size:16px;line-height:1.75}body{color:#333;background-color:#fff;overflow-y:scroll}a{text-decoration:none}.lead{margin-left:-1rem;margin-right:-1rem}img,.img{display:block;max-width:100%;margin-bottom:1rem;border:none}img.lead,.img.lead{max-width:calc(100% + 2rem);width:calc(100% + 2rem)}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-weight:bold;text-rendering:optimizeLegibility}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6{margin:1.6rem 0 1rem;line-height:1.6}h1,.h1{font-size:2rem;line-height:1.25}h2,.h2{font-size:1.5rem}h3,.h3{font-size:1.17em}p{margin-top:0;margin-bottom:1rem}p.lead{font-size:1.25rem;font-weight:300;padding:0 1rem}ul,ol,dl{margin-top:0;margin-bottom:1rem}ul,ol{padding-left:1.25rem}hr{position:relative;margin:1.5rem 0;border:0;border-top:1px solid #eee}.hr{padding-bottom:.5rem;border-bottom:1px solid #eee;margin-bottom:1.5rem}h4,h5,h6,.h4,.h5,.h6{margin-bottom:0.5rem;font-size:1rem}table{margin-bottom:1rem;width:100%;width:calc(100% + 2rem);margin-left:-1rem;border:1px solid #e5e5e5;border-collapse:collapse;border-spacing:0}td,th{padding:.25rem .5rem;border:1px solid #e5e5e5}td:first-child,th:first-child{padding-left:1rem}td:last-child,th:last-child{padding-right:1rem}thead+tbody,tbody+tbody,tfoot{border-top:3px double #e5e5e5}tbody tr:nth-child(odd) td,tbody tr:nth-child(odd) th{background-color:#f9f9f9}footer{margin-bottom:2rem}.page,.post{margin-bottom:3em}.page li+li,.post li+li{margin-top:.25rem}.page>header,.post>header{margin-bottom:2rem}.page-title,.post-title{margin-top:0}.post-date{display:block;margin-top:-0.5rem;margin-bottom:1rem;color:#9a9a9a}.related-posts{padding-left:0;list-style:none}.related-posts>li,.related-posts>li+li{margin-top:1rem}.related-posts>li>small,.related-posts>li+li>small{font-size:75%;color:#9a9a9a}.message{margin-bottom:1rem;padding:1rem;color:#787878;background-color:#f9f9f9;margin-left:-1rem;margin-right:-1rem}body,main{position:relative;overflow-x:hidden}@media screen{body::before{content:'';background:#e5e5e5;position:absolute;left:0;top:0;bottom:0}}@media screen and (min-width: 40em){html{font-size:17px}}@media screen and (min-width: 54em){html{font-size:16px}}@media screen and (min-width: 88em){html{font-size:17px}}@media screen and (min-width: 125em){html{font-size:18px}}.sr-only{display:none}.clearfix,.sidebar-social::after,.clearafter::after{content:"";display:table;clear:both}a,.a{position:relative;padding-bottom:.15rem;border-style:hidden}.img{overflow:hidden;background-color:#f9f9f9}.img>img{margin:0;width:100%;height:100%}.sixteen-nine{position:relative}.sixteen-nine::before{display:block;content:"";width:100%;padding-top:56.25%}.sixteen-nine>*{position:absolute;top:0;left:0;right:0;bottom:0}h1+hr,h2+hr,h3+hr,h4+hr,h5+hr,h6+hr{margin-top:0}.fade-in{animation-duration:500ms;animation-name:fade-in;animation-fill-mode:forwards}@keyframes fade-in{from{transform:translateY(-2rem);opacity:0}50%{transform:translateY(-2rem);opacity:0}to{transform:translateY(0);opacity:1}}.mb6{margin-bottom:10rem}.sidebar{color:rgba(255,255,255,0.75);text-align:left}.sidebar::before{content:"";position:absolute;z-index:2;top:0;left:0;bottom:0;right:0;background:linear-gradient(to bottom, rgba(32,32,32,0) 0%, rgba(32,32,32,0.5) 100%)}.sidebar a{color:#fff;border-bottom-color:rgba(255,255,255,0.2)}.right-side{width:100%;margin-left:auto;margin-right:auto}.right-side .ad-first{text-align:center}.right-side .ad-second{text-align:center}@media screen{.right-side{max-width:38rem;min-height:100vh}}@media screen and (min-width: 54em){.right-side{margin-left:20rem;margin-right:1rem;padding:4rem 1rem 12rem}}@media screen and (min-width: 72em){.right-side{margin-left:22rem;max-width:42rem}}@media screen and (min-width: 88em){.right-side{width:162px;margin-left:0rem;margin-right:0rem;padding:0rem;margin-top:10rem;display:block;float:left}}@media screen and (min-width: 96em){.right-side{width:300px;margin-right:0rem}}#_yDrawer{position:relative}@media screen{#_yDrawer{min-height:640px;min-height:100vh}}@media screen and (min-width: 54em){#_yDrawer{width:18rem;margin-left:0}}.sidebar-bg{position:absolute;height:100%;overflow:hidden;top:0;right:0;bottom:0;left:0;background:#202020 center / cover}.sidebar-box{display:flex;justify-content:center}.sidebar-sticky{position:relative;z-index:3}@media screen{.sidebar-sticky{-ms-overflow-style:none;overflow:-moz-scrollbars-none;height:100%;overflow:auto;position:absolute;padding:3rem 0rem;right:2.5rem;left:2.5rem}}.sidebar-sticky::-webkit-scrollbar{display:none}.sidebar-about>h1{color:#fff;font-size:2rem}.sidebar-nav>ul{list-style:none;padding-left:0;margin-bottom:.5rem}a.sidebar-nav-item{width:100%;font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}a.sidebar-nav-subitem{font-weight:normal;display:block;line-height:1.75;padding:.25rem 0;border-bottom:1px solid rgba(255,255,255,0.2)}@media screen{.y-drawer-scrim{z-index:2}.y-drawer-content{width:18rem;left:-18rem;z-index:3}}.sidebar-social{margin-bottom:.5rem}.sidebar-social>ul{list-style:none;padding-left:0;margin:0 -.25rem}.sidebar-social>ul>li{float:left}.sidebar-social>ul>li>a{display:inline-block;text-align:center;font-size:1.6rem;line-height:3rem;width:3.1249rem;height:4rem;padding:.5rem 0}.sidebar-social>ul li+li{margin-top:0}.fixed-top{position:fixed;top:0;left:0;width:100%;z-index:1}.navbar>.content{padding-top:0;padding-bottom:0;min-height:0;height:0}.menu{display:inline-block;padding:1.75rem 1.5rem;border-bottom:none;margin-left:-1.5rem;color:#9a9a9a !important}.menu::after{content:"\2630"}@media screen and (min-width: 54em){.menu{padding:1.25rem 1.5rem;position:absolute;left:-9999px}.menu:focus{position:static}}.animation-main{pointer-events:none}.loading{display:none}@media print{.menu{display:none}}.animation-main{opacity:0;will-change:opacity}.loading{position:absolute;top:0;right:0;padding:5.25rem 4.5rem;transform-origin:top right;transform:scale(0.33)}.content{position:relative;margin-left:auto;margin-right:auto;padding:5rem 1rem 12rem}@media screen{.content{min-height:100vh}}@media screen and (min-width: 54em){.content{padding:4rem 1rem 12rem;margin-left:19rem;margin-right:3rem}}@media screen and (min-width: 72em){.content{max-width:42rem;margin-left:21rem}}@media screen and (min-width: 88em){.content{float:left;width:100%;margin-left:22rem;margin-right:5rem}}@media screen and (min-width: 96em){.content{max-width:44rem}}@media screen and (min-width: 102em){.content{margin-left:25rem;margin-right:8rem}}.me{width:6.5rem;height:6.5rem;align-self:center;margin-right:20px;border-radius:100%;position:relative}@media screen and (min-width: 40em){.me{width:7rem;height:7rem}}@media screen and (min-width: 54em){.me{width:6.5rem;height:6.5rem}}@media screen and (min-width: 72em){.me{width:7rem;height:7rem}}main>footer{width:100%;position:absolute;bottom:0;left:0;right:0;padding:0 1rem;color:#9a9a9a;font-size:smaller;text-align:center}main>footer>p{margin-bottom:0}html{font-family:'Sans-serif'}h1,h2,h3,h4,h5,h6,.h1,.h2,.h3,.h4,.h5,.h6,.heading{font-family:'Sans-serif'}

</style>


<link rel="preload" href="http://localhost:4003/assets/css/hydejack.css?v=6.4.0" as="style" onload="this.rel='stylesheet'">

<style id="_pageStyle">

.content a{color:#4f86aa;border-color:rgba(79,134,170,0.2)}.content a:hover{border-color:#4f86aa}:focus{outline-color:#4f86aa}::selection{color:#fff;background:#4f86aa}::-moz-selection{color:#fff;background:#4f86aa}

</style>


<noscript>
  <link rel="stylesheet" href="http://localhost:4003/assets/css/hydejack.css?v=6.4.0">
  
  
  

  
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato">
    <style>
      html { font-family: 'Lato', 'Sans-serif' }
      h1, h2, h3, h4, h5, h6, .h1, .h2, .h3, .h4, .h5, .h6, .heading { font-family: 'Lato', 'Sans-serif' }
    </style>
  

  
  <link rel="stylesheet" href="http://localhost:4003/assets/icomoon/style.css">
</noscript>
<!--<![endif]-->

</head>

<body>
  <!-- =============== -->
<!-- MENU            -->
<!-- =============== -->
<div class="navbar fixed-top">
  <div class="content">
    <span class="sr-only">Jump to:</span>
    <a id="_menu" class="menu no-hover" href="#_title">
      <span class="sr-only">Menu</span>
    </a>
  </div>
</div>

<!-- =============== -->
<!-- CONTENT         -->
<!-- =============== -->
<div id="_yPushState">
  <div class="fade-in">
    <main id="_main" class="content" role="main" data-color="#4f86aa" data-image="/assets/img/nap.jpg">
      

<article id="post-paper/2018/06/20/vae2" class="post" role="article">
  <header>
    <h1 class="post-title">
      
        VAE Tutorial 2
      
    </h1>

    <p class="post-date heading">
      <time datetime="2018-06-20T00:00:00+09:00">20 Jun 2018</time>
      









in <a href="/category/paper/" data-flip="title">Paper</a>

      









on <a href="/tag/paper-dl/" data-flip="title">Deep-Learning</a>

    </p>

    <script data-ad-client="ca-pub-2252618542833203" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    
  <div class="hr" style="padding-bottom:0"></div>


  </header>
  
    <!--
  <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  -->
  
    <div class="markdown-body">


        <!--
<style>
.myAd1190 { display:block; width:98%; height: 280px; }
@media(min-width: 600px) { .myAd1190 { display: none; } }
</style>
<ins class="adsbygoogle myAd1190"
    data-ad-client="ca-pub-5105555504153863"
    data-ad-slot="6481487416"
    ></ins>
<script>
(adsbygoogle = window.adsbygoogle || []).push({});
</script>
-->


<br/>
      <ul>
  <li><strong>VAE Tutorial 목차</strong>
    <ul>
      <li><a href="https://dnddnjs.github.io/paper/2018/06/19/vae/">Tutorial 1: CS231n 강의 내용</a></li>
      <li><a href="https://dnddnjs.github.io/paper/2018/06/20/vae2/">Tutorial 2: VAE 논문 &amp; 코드 리뷰</a></li>
      <li><a href="https://dnddnjs.github.io/paper/2018/06/21/vae3/">Tutorial 3: SentenceVAE</a></li>
      <li><a href="https://dnddnjs.github.io/paper/2018/06/21/vae4/">Tutorial 4: MusicVAE</a></li>
    </ul>
  </li>
</ul>

<h1 id="vae-tutorial-2-vae-논문--코드-리뷰">VAE Tutorial 2: VAE 논문 &amp; 코드 리뷰</h1>

<p><img src="https://www.dropbox.com/s/1niug5qggbfatg7/Screenshot%202018-06-19%2021.36.15.png?dl=1" /></p>

<ul>
  <li>논문 저자: Diederik P. Kingma (Universiteit van Amsterdam)</li>
  <li>논문 링크: <a href="https://arxiv.org/pdf/1312.6114.pdf">https://arxiv.org/pdf/1312.6114.pdf</a></li>
  <li>참고한 자료:
    <ul>
      <li><a href="https://arxiv.org/pdf/1606.05908.pdf">https://arxiv.org/pdf/1606.05908.pdf</a></li>
      <li><a href="https://blog.evjang.com/2016/08/variational-bayes.html">https://blog.evjang.com/2016/08/variational-bayes.html</a></li>
      <li><a href="https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf">https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture13.pdf</a></li>
    </ul>
  </li>
</ul>

<p>이번 포스트에서는 VAE의 원래 논문인 “Auto-Encoding Variational Bayes”의 내용 중 일부를 다루고 Pytorch VAE example code를 리뷰해봅니다.</p>

<hr />
<h2 id="1-variational-inference--reparameterization-trick">1. Variational Inference &amp; Reparameterization Trick</h2>
<p>논문의 Abstract에서는 다음과 같은 말을 던지면서 시작합니다. 이 포스트에서도 기본적으로 MNIST 데이터셋에 대한 generative model의 전제하에 이야기합니다.</p>

<blockquote>
  <p>How can we perform efficient inference and learning in directed probabilistic
models, in the presence of continuous latent variables with intractable posterior
distributions, and large datasets?</p>
</blockquote>

<p>VAE가 하고 싶은 것은 명확합니다. 또한 그것을 가로막는 문제도 명확히 제시합니다.</p>

<ul>
  <li><strong>목표</strong>: efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables</li>
  <li><strong>문제</strong>: intractable posterior, large dataset</li>
</ul>

<p>이것을 이해하기 위해 이전 포스트에서 언급했던 식을 다시 살펴보겠습니다. directed probabilistic model 이라는 말은 explicit density estimation이라고도 볼 수 있습니다. <code class="MathJax_Preview">p_{\theta}(z)</code><script type="math/tex">p_{\theta}(z)</script>에서 latent variable을 sampling 한다면 대부분의 <code class="MathJax_Preview">z</code><script type="math/tex">z</script>에 대해 <code class="MathJax_Preview">p_{\theta}(x \vert z)</code><script type="math/tex">p_{\theta}(x \vert z)</script>는 거의 0의 값을 가질 것입니다. 그렇다면 다음 식처럼 <code class="MathJax_Preview">p_{\theta}(x)</code><script type="math/tex">p_{\theta}(x)</script>에 대한 Monte-Carlo estimation을 하는데 sample이 너무 많이 필요하게됩니다. 데이터포인트 하나당 많은 sample을 필요로 하므로 large dataset에 대해서 이렇게 estimation을 하면 학습과정이 너무 느려집니다.</p>

<pre class="MathJax_Preview"><code>p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz \approx \frac{1}{N}\sum_{i=1}^N p_{\theta}(x|z^{(i)})</code></pre>
<script type="math/tex; mode=display">p_{\theta}(x)=\int p_{\theta}(z)p_{\theta}(x|z)dz \approx \frac{1}{N}\sum_{i=1}^N p_{\theta}(x|z^{(i)})</script>

<p>따라서 데이터에 dependent하게 <code class="MathJax_Preview">z</code><script type="math/tex">z</script>를 sampling하기 위해 posterior <code class="MathJax_Preview">p_{\theta}(z \vert x)</code><script type="math/tex">p_{\theta}(z \vert x)</script>를 정의했었는데 이 posterior는 intractable 합니다. 따라서 이 posterior를 approximate하는 새로운 posterior <code class="MathJax_Preview">q_{\phi}(z \vert x)</code><script type="math/tex">q_{\phi}(z \vert x)</script>를 정의했었습니다. 이렇게 우리가 다루기 쉬운 paramterized된 posterior를 대신 사용하고 이 posterior가 원래의 posterior와 최대한 가깝게 만드는 것이 <strong>variational inference</strong> 입니다. 또는 <strong>variational bayes</strong>라고도 합니다. 이것은 다음 그림과 같이 파란색 분포에 초록색 분포를 최대한 맞추는 것과 같습니다.</p>

<p><img src="https://www.dropbox.com/s/ocojvekrigo247z/Screenshot%202018-06-20%2023.45.41.png?dl=1" /></p>

<center>그림출처 https://blog.evjang.com/2016/08/variational-bayes.html</center>

<p><br />
최적화과정을 거쳐 approximate된 posterior와 posterior가 최대한 비슷해지면 <code class="MathJax_Preview">q_{\phi}(z \vert x)</code><script type="math/tex">q_{\phi}(z \vert x)</script>를 통해 inference 할 수 있습니다. Inference 하는 것을 variational parameter <code class="MathJax_Preview">\phi</code><script type="math/tex">\phi</script>를 통해 하는 것입니다. 이 최적화과정은 이전 포스트에서 구한 ELBO를 최대화하는 것입니다.</p>

<pre class="MathJax_Preview"><code>\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z)) - (1)</code></pre>
<script type="math/tex; mode=display">\mathcal{L}(x^{(i)}, \theta, \phi) = \mathbb{E}_{z}[log p_{\theta}(x^{(i)} \vert z)] - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z)) - (1)</script>

<pre class="MathJax_Preview"><code>\theta^*, \phi^* = argmax_{\theta, \phi}\sum_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)</code></pre>
<script type="math/tex; mode=display">\theta^*, \phi^* = argmax_{\theta, \phi}\sum_{i=1}^N \mathcal{L}(x^{(i)}, \theta, \phi)</script>

<p>이 ELBO의 값을 maximize하는 parameter는 (1) analytic하게 구하거나 (2) stochastic gradient ascent를 통해 구할 수 있습니다. Analytic하게 구하는 방식 중에 Mean-Field Variational Bayes가 있습니다. 논문에서는 이 방법이 likelihood function인 <code class="MathJax_Preview">p_{\theta}(x \vert z)</code><script type="math/tex">p_{\theta}(x \vert z)</script>이 뉴럴넷과 같은 복잡한 함수로 표현될 경우 intractable 하다고 말합니다. 논문에서 VAE의 방법과 Mean-Field Variational Bayes 사이의 차이에 대해서 다음과 같이 언급합니다.</p>
<blockquote>
  <p>Note that in contrast with the approximate
posterior in mean-field variational inference, it is not necessarily factorial and its parameters φ are
not computed from some closed-form expectation</p>
</blockquote>

<p>따라서 (1)식의 gradient를 구해서 stochastic하게 parameter를 업데이트하는 방식을 사용할 것입니다. 이 때 (1) 식을 <code class="MathJax_Preview">\theta</code><script type="math/tex">\theta</script>에 대해서 미분하는 것은 문제가 없으나 <code class="MathJax_Preview">\phi</code><script type="math/tex">\phi</script>에 대해서 미분하는 것은 문제가 있습니다. (1) 식 중에서도 첫번째 항이 문제가 있습니다. 첫번째 항의 expectation 안에 있는 함수를 <code class="MathJax_Preview">f(z)</code><script type="math/tex">f(z)</script>라고 가정해보겠습니다. 이 함수의 expectation에 대한 미분은 다음과 같이 쓸 수 있습니다.</p>

<pre class="MathJax_Preview"><code>\nabla_{\phi}\mathbb{E}_{q_{\phi}(z)}[f(z)] = \int \nabla_{\phi} q_{\phi}(z) f(z) dz</code></pre>
<script type="math/tex; mode=display">\nabla_{\phi}\mathbb{E}_{q_{\phi}(z)}[f(z)] = \int \nabla_{\phi} q_{\phi}(z) f(z) dz</script>

<pre class="MathJax_Preview"><code>= \int q_{\phi}(z)\frac{\nabla_{\phi} q_{\phi}(z)}{q_{\phi}(z)} f(z) dz</code></pre>
<script type="math/tex; mode=display">= \int q_{\phi}(z)\frac{\nabla_{\phi} q_{\phi}(z)}{q_{\phi}(z)} f(z) dz</script>

<pre class="MathJax_Preview"><code>=\mathbb{E}_{q_{\phi}(z)}[f(z)\nabla_{\phi}log q_{\phi}(z)]</code></pre>
<script type="math/tex; mode=display">=\mathbb{E}_{q_{\phi}(z)}[f(z)\nabla_{\phi}log q_{\phi}(z)]</script>

<p>이 미분값은 monte-carlo estimation을 통해 estimate 할 수 있습니다. 이 때, <code class="MathJax_Preview">z^{(i)}</code><script type="math/tex">z^{(i)}</script>는 <code class="MathJax_Preview">q_{\phi}(z \vert x^{(i)})</code><script type="math/tex">q_{\phi}(z \vert x^{(i)})</script>로부터 sampling 합니다. 따라서 다음 gradient 값은 상당히 variance가 높습니다. 이 경우 impractical 합니다.</p>

<pre class="MathJax_Preview"><code>\frac{1}{L}\sum_{l=1}^L f(z^{(l)})\nabla_{\phi}log q_{\phi}(z^{(l)})</code></pre>
<script type="math/tex; mode=display">\frac{1}{L}\sum_{l=1}^L f(z^{(l)})\nabla_{\phi}log q_{\phi}(z^{(l)})</script>

<p>이러한 문제를 해결하기 위해 VAE는 reparameterization trick이라는 technique을 사용합니다. <code class="MathJax_Preview">z</code><script type="math/tex">z</script>를 posterior <code class="MathJax_Preview">q_{\phi}(z \vert x)</code><script type="math/tex">q_{\phi}(z \vert x)</script>로부터 sampling 하는 것이 아니라 differentiable 한 함수 <code class="MathJax_Preview">g_{\phi}(\epsilon, x)</code><script type="math/tex">g_{\phi}(\epsilon, x)</script>로부터 deterministic하게 정해진다고 보는 것입니다. 이 때, <code class="MathJax_Preview">\epsilon</code><script type="math/tex">\epsilon</script>은 noise variable입니다.</p>

<pre class="MathJax_Preview"><code>\tilde{z} = g_{\phi}(\epsilon, x) \qquad where \quad \epsilon \sim p(\epsilon)</code></pre>
<script type="math/tex; mode=display">\tilde{z} = g_{\phi}(\epsilon, x) \qquad where \quad \epsilon \sim p(\epsilon)</script>

<p>이 경우 다음과 같이 <code class="MathJax_Preview">f(z)</code><script type="math/tex">f(z)</script>의  <code class="MathJax_Preview">q_{\phi}(z)</code><script type="math/tex">q_{\phi}(z)</script> 대한 expectation을 <code class="MathJax_Preview">epsilon</code><script type="math/tex">epsilon</script>에 대한 expectation으로 바꿀 수 있습니다. 이제 바뀐 expectation에 대해 monte carlo estimation을 적용할 수 있습니다. <code class="MathJax_Preview">f(g_{\phi}(z^{(l)}, x^{(i)}))</code><script type="math/tex">f(g_{\phi}(z^{(l)}, x^{(i)}))</script>는 <code class="MathJax_Preview">\theta</code><script type="math/tex">\theta</script>와 <code class="MathJax_Preview">\phi</code><script type="math/tex">\phi</script>에 대해 미분가능하기 때문에 바로 미분할 수 있습니다.</p>

<pre class="MathJax_Preview"><code>\mathbb{E}_{q_{\phi}(z \vert x^{(i)})}[f(z)] = \mathbb{E}_{\epsilon}[f(g_{\phi}(\epsilon, x^{(i)}))] = \frac{1}{L}\sum_{l=1}^L f(g_{\phi}(z^{(l)}, x^{(i)}))</code></pre>
<script type="math/tex; mode=display">\mathbb{E}_{q_{\phi}(z \vert x^{(i)})}[f(z)] = \mathbb{E}_{\epsilon}[f(g_{\phi}(\epsilon, x^{(i)}))] = \frac{1}{L}\sum_{l=1}^L f(g_{\phi}(z^{(l)}, x^{(i)}))</script>

<p>이 수식을 이용해서 ELBO를 고쳐쓸 수 있습니다. 이 식을 SGVB(Stochastic Gradient Variational Bayes) estimator라고 합니다. 이 때, <code class="MathJax_Preview">z^{(i,l)}=g_{\phi}(\epsilon^{(l)}, x^{(i)})</code><script type="math/tex">z^{(i,l)}=g_{\phi}(\epsilon^{(l)}, x^{(i)})</script>이고 <code class="MathJax_Preview">\epsilon^{(l)} \sim p(\theta)</code><script type="math/tex">\epsilon^{(l)} \sim p(\theta)</script> 입니다. 보통은 <code class="MathJax_Preview">g_{\phi}(\epsilon, x) = \mu + \sigma\epsilon</code><script type="math/tex">g_{\phi}(\epsilon, x) = \mu + \sigma\epsilon</script>으로 많이 사용합니다(univariate gaussian case).</p>

<pre class="MathJax_Preview"><code>\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))</code></pre>
<script type="math/tex; mode=display">\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))</script>

<p>이러한 reparameterization trick을 그림으로 보자면 다음과 같습니다. 원래는 encoder로부터 구한 data dependent한 mean과 variance를 가지고 posterior를 만듭니다. 그 posterior로부터 <code class="MathJax_Preview">z</code><script type="math/tex">z</script>를 샘플링한 다음에 그 <code class="MathJax_Preview">z</code><script type="math/tex">z</script>를 가지고 decoder는 data를 generation 했습니다. 하지만 reparametization을 하면 computation graph 내의 sampling 과정이 noise sampling이 되어 옆으로 빠져버립니다. 따라서 Back propagation을 통해 decoder output으로부터 encoder까지 gradient가 전달될 수 있습니다.</p>

<p><img src="https://www.dropbox.com/s/5249ixq6r4t38l8/Screenshot%202018-06-21%2000.57.08.png?dl=1" /></p>
<center>그림출처 https://arxiv.org/pdf/1606.05908.pdf</center>

<p>이렇게 업데이트를 하는 알고리즘이 Auto-Encoding Variational Bayes이며 다음과 같습니다. 
<img src="https://www.dropbox.com/s/hxacd2bhz1hi3yl/Screenshot%202018-06-21%2001.05.46.png?dl=1" /></p>

<h2><br /><br /></h2>
<h2 id="2-vae-code-example">2. VAE code example</h2>
<h3 id="21-vae-example-of-paper">2.1 VAE example of paper</h3>
<p>prior와 posterior를 모두 gaussian으로 가정하고 likelihood를 Bernoulli라고 가정하면 ELBO 식은 다음과 같이 쓸 수 있습니다. <code class="MathJax_Preview">y</code><script type="math/tex">y</script>는 <code class="MathJax_Preview">z</code><script type="math/tex">z</script>와 decoder를 통해 나온 값입니다. 마지막 식의 첫번째 항은 잘 보면 cross-entropy 인 것을 알 수 있습니다.</p>

<pre class="MathJax_Preview"><code>\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))</code></pre>
<script type="math/tex; mode=display">\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) - D_{KL}(q_{\phi}(z \vert x^{(i)}) \Vert p_{\theta}(z))</script>

<pre class="MathJax_Preview"><code>\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)</code></pre>
<script type="math/tex; mode=display">\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (log p_{\theta}(x^{(i)} \vert z^{(i, l)})) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)</script>

<pre class="MathJax_Preview"><code>\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)</code></pre>
<script type="math/tex; mode=display">\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)</script>

<p><br />
논문의 실험결과는 다음과 같습니다. 이 그림은 <code class="MathJax_Preview">z</code><script type="math/tex">z</script>를 임의로 변형시켜보면서 data를 생성해낸 결과입니다. <code class="MathJax_Preview">z</code><script type="math/tex">z</script>에 따라 data가 continuous하게 변화하는 것을 볼 수 있습니다. 또한 비슷한 숫자끼리는 서로 뭉쳐있음을 알 수 있습니다. 이를 통해 VAE가 의미있는 representation을 학습하는 것을 확인합니다.</p>

<p><img src="https://www.dropbox.com/s/f2dpiyr9rptcgwe/Screenshot%202018-06-21%2001.27.20.png?dl=1" /></p>

<p>이 마지막 식을 가지고 이제 우리는 VAE 코드를 살펴볼 수 있습니다. Pytorch는 공식적으로 VAE에 대한 simple한 example을 제공합니다. 지금까지 살펴본 VAE의 이론에 충실한 코드입니다.</p>

<ul>
  <li>코드링크: <a href="https://github.com/pytorch/examples/tree/master/vae">https://github.com/pytorch/examples/tree/master/vae</a></li>
</ul>

<p><br /></p>
<h3 id="22-vae-network-class">2.2 VAE network class</h3>
<p>VAE의 network 구조는 다음과 같습니다. 28x28의 이미지를 일렬로 펴서 784 크기의 vector로 만듭니다. 이 vector를 입력으로 받기 때문에 self.fc1이 784에서 400개의 hidden unit으로의 fully connected인 것을 알 수 있습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">VAE</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">VAE</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="p">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc21</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc22</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">fc4</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">400</span><span class="p">,</span> <span class="mi">784</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc21</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc22</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">eps</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">std</span><span class="p">).</span><span class="n">add_</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">mu</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
        <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
        <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>
</code></pre></div></div>

<p><br />
Encoder의 구조는 다음과 같습니다. 784-d vector가 input으로 들어와서 400-d hidden layer를 통과하고 이 때 activation function은 relu를 사용합니다. 그 이후에 20개의 gaussian 분포에 대한 mean과 variance를 내보낼 것입니다. Mean은 self.fc21(h1)이며 linear 연산을 통한 output입니다. Variance의 경우 항상 0보다 크거나 같아야하는데 linear 연산을 한 self.fc22(h1)의 경우 -값이 될 수 있습니다. 따라서 이 값을 variance로 보지 않고 log variance라고 보는 것입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">encode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">h1</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc21</span><span class="p">(</span><span class="n">h1</span><span class="p">),</span> <span class="bp">self</span><span class="p">.</span><span class="n">fc22</span><span class="p">(</span><span class="n">h1</span><span class="p">)</span>
</code></pre></div></div>

<p><br />
latent space 상에서의 mean과 log variance를 구했다면 reparameterization을 통해 latent vector <code class="MathJax_Preview">z</code><script type="math/tex">z</script>를 sampling 할 수 있습니다. Noise로부터 <code class="MathJax_Preview">\epsilon</code><script type="math/tex">\epsilon</script>인 eps를 구하고 이 eps와 standard deviation을 곱하고 mean을 더해줍니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">reparameterize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="p">.</span><span class="n">training</span><span class="p">:</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="mf">0.5</span><span class="o">*</span><span class="n">logvar</span><span class="p">)</span>
        <span class="n">eps</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">std</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">eps</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">std</span><span class="p">).</span><span class="n">add_</span><span class="p">(</span><span class="n">mu</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">mu</span>
</code></pre></div></div>

<p><code class="MathJax_Preview">z</code><script type="math/tex">z</script>를 구하고나면 이 latent 값으로부터 decoder를 통해 data에 대한 Bernoulli distribution을 출력할 수 있습니다. Bernoulli 분포는 0에서 1 사이이므로 sigmoid 함수를 output layer의 activation function으로 사용합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">z</span><span class="p">):</span>
    <span class="n">h3</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc3</span><span class="p">(</span><span class="n">z</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span><span class="n">fc4</span><span class="p">(</span><span class="n">h3</span><span class="p">))</span>
</code></pre></div></div>

<p><br />
결국 VAE의 forward path는 다음과 같습니다. x.view(-1, 784)는 이미지를 784-D vector로 만드는 부분입니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">encode</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">))</span>
    <span class="n">z</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">reparameterize</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">z</span><span class="p">),</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span>
</code></pre></div></div>

<p><br /></p>
<h3 id="23-train-vae">2.3 Train VAE</h3>
<p>학습에 관련된 가장 중요한 부분 중의 하나인 loss function 정의 부분입니다. 다음 수식을 생각해서 코드로 어떻게 구현되었는지 보면 됩니다.</p>

<pre class="MathJax_Preview"><code>\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)</code></pre>
<script type="math/tex; mode=display">\mathcal{\tilde{L^B}}(x^{(i)}, \theta, \phi) = \frac{1}{L}\sum_{l=1}^L (x_i log y_{(i, l)} + (1 - x_i)(1 - y_{(i,l)}) + \frac{1}{2}\sum_{j=1}^J(1 + log((\sigma_j^{(i)})^2) - (\mu_{j}^{(i)})^2 - (\sigma_j^{(i)})^2)</script>

<p>첫번째 항은 F.binary_cross_entropy로 구현되었으며 두번째 항은 KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())로 구현되었습니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Reconstruction + KL divergence losses summed over all elements and batch
</span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">):</span>
    <span class="n">BCE</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">recon_x</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">784</span><span class="p">),</span> <span class="n">size_average</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

    <span class="c1"># see Appendix B from VAE paper:
</span>    <span class="c1"># Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014
</span>    <span class="c1"># https://arxiv.org/abs/1312.6114
</span>    <span class="c1"># 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)
</span>    <span class="n">KLD</span> <span class="o">=</span> <span class="o">-</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">logvar</span> <span class="o">-</span> <span class="n">mu</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">-</span> <span class="n">logvar</span><span class="p">.</span><span class="n">exp</span><span class="p">())</span>

    <span class="k">return</span> <span class="n">BCE</span> <span class="o">+</span> <span class="n">KLD</span>
</code></pre></div></div>

<p><br />
학습 코드 부분은 상당히 간단합니다. model.train()을 통해 현재 학습할 것이라는 것을 선언합니다. 그 이후에 train_loader를 통해 mini_batch를 추출합니다. 그 다음에 model에 data를 입력으로 넣어서 출력을 받습니다. 그 출력을 통해 loss function 값을 구할 수 있고 loss.backward()를 통해 back-propagation으로 각 parameter의 gradient 값을 구합니다. 그 이후에 optimizer(Adam optimizer)를 통해 각 parameter를 update 합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">batch_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_loader</span><span class="p">):</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">recon_batch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">)</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">batch_idx</span> <span class="o">%</span> <span class="n">args</span><span class="p">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="s">'Train Epoch: {} [{}/{} ({:.0f}%)]</span><span class="si">\</span><span class="se">t</span><span class="s">Loss: {:.6f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
                <span class="n">epoch</span><span class="p">,</span> <span class="n">batch_idx</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">),</span>
                <span class="mf">100.</span> <span class="o">*</span> <span class="n">batch_idx</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">),</span>
                <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)))</span>

    <span class="k">print</span><span class="p">(</span><span class="s">'====&gt; Epoch: {} Average loss: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span>
          <span class="n">epoch</span><span class="p">,</span> <span class="n">train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)))</span>
</code></pre></div></div>

<p><br /></p>
<h3 id="24-evaluate-vae">2.4 Evaluate VAE</h3>
<p>학습과정을 evaluate 하는 것은 더 간단합니다. model.eval()을 통해 평가 중이라는 것을 선언합니다. 그 이후에 test dataset에 대해 reconstruction을 출력합니다. 그 이후에 loss function 값을 출력해서 학습이 어떻게 진행되고 있는지 평가합니다. 그리고 각 평가과정마다 생성된 하나의 sample을 저장합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">epoch</span><span class="p">):</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
    <span class="n">test_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">_</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test_loader</span><span class="p">):</span>
            <span class="n">data</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">recon_batch</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
            <span class="n">test_loss</span> <span class="o">+=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">recon_batch</span><span class="p">,</span> <span class="n">data</span><span class="p">,</span> <span class="n">mu</span><span class="p">,</span> <span class="n">logvar</span><span class="p">).</span><span class="n">item</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">n</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">8</span><span class="p">)</span>
                <span class="n">comparison</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">([</span><span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">],</span>
                                      <span class="n">recon_batch</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="n">args</span><span class="p">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)[:</span><span class="n">n</span><span class="p">]])</span>
                <span class="n">save_image</span><span class="p">(</span><span class="n">comparison</span><span class="p">.</span><span class="n">cpu</span><span class="p">(),</span>
                         <span class="s">'results/reconstruction_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.png'</span><span class="p">,</span> <span class="n">nrow</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>

    <span class="n">test_loss</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_loader</span><span class="p">.</span><span class="n">dataset</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'====&gt; Test set loss: {:.4f}'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">test_loss</span><span class="p">))</span>
</code></pre></div></div>

<h3 id="25-main-loop">2.5 Main loop</h3>

<p>epoch마다 train을 하고 test를 한 다음에 64개의 sample 이미지를 생성합니다. latent는 임의로 20개를 normal distribution에서 sampling 합니다. 이 sampling된 latent variable을 Decoder에 통과시키면 decoder는 이미지를 생성해냅니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">args</span><span class="p">.</span><span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">train</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="n">test</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">20</span><span class="p">).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">sample</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">decode</span><span class="p">(</span><span class="n">sample</span><span class="p">).</span><span class="n">cpu</span><span class="p">()</span>
        <span class="n">save_image</span><span class="p">(</span><span class="n">sample</span><span class="p">.</span><span class="n">view</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">),</span>
                   <span class="s">'results/sample_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">epoch</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.png'</span><span class="p">)</span>
</code></pre></div></div>

<p><br />
이 코드는 python main.py를 하면 바로 실행이 되며 실행 화면은 다음과 같다.
<img src="https://www.dropbox.com/s/15kxe3fiuvgnu25/Screenshot%202018-06-21%2002.13.39.png?dl=1" /></p>

<p>첫 epoch 때 test set에 대한 loss는 119.4120이며 real data reconstruction과 z sampled recontruction은 다음 사진과 같습니다.</p>
<center><img src="https://www.dropbox.com/s/7o8i909pf6jgnt9/Screenshot%202018-06-21%2002.15.50.png?dl=1" width="300px" /></center>
<center><img src="https://www.dropbox.com/s/gk7y171bbgefwnc/Screenshot%202018-06-21%2002.16.46.png?dl=1" width="300px" /></center>

<p>50 epoch 때 test set에 대한 loss는 93.3473이며 real data reconstruction과 z sampled recontruction은 다음 사진과 같습니다.</p>
<center><img src="https://www.dropbox.com/s/wk93oi0ahh0t54n/Screenshot%202018-06-21%2002.19.13.png?dl=1" width="300px" /></center>
<center><img src="https://www.dropbox.com/s/jnmrzjpyw36nh9l/Screenshot%202018-06-21%2002.19.39.png?dl=1" width="300px" /></center>

<p>학습이 잘 된 것을 확인할 수 있습니다. 이제 다음 post에서 VAE를 사용해 Music generation이라는 도메인에 적용한 MusicVAE 를 살펴보겠습니다.</p>

      <br/>
      <br/>
        <!--블로그-하단-반응형
      <ins class="adsbygoogle"
          style="display:block; width:100%; height:300px;"
          data-ad-client="ca-pub-5105555504153863"
          data-ad-slot="1037589043"
          data-ad-format="auto"></ins>
      <script>
      (adsbygoogle = window.adsbygoogle || []).push({});
      </script>
      -->
    </div>
  

</article>

  <hr class="dingbat" />

  <div class="share">
      <h2>Share this post</h2>
      <div class="share-body">
        <a href="http://twitter.com/share?text=VAE Tutorial 2&amp;url=http://localhost:4003/paper/2018/06/20/vae2/"
    onclick="window.open(this.href, 'twitter-share', 'width=550,height=235');return false;">
            <span class="icon-twitter">
            </span>
        </a>
        <a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4003/paper/2018/06/20/vae2/"
    onclick="window.open(this.href, 'facebook-share','width=580,height=296');return false;">
            <span class="icon-facebook">
            </span>
        </a>
    </div>
  </div>
  <br/>






  <aside class="author" role="complementary">
    <div class="author">
  <h2 class="page-title hr">
    About
  </h2>
<div class="author-body">
  
    
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="woongwon lee"
    srcset="/assets/img/me.jpeg 1x,/assets/img/me.jpeg 2x"
    
  />


  
  <div class="author-body-description">
    <p>Machine Learning Engineer</p>

  </div>
</div>
</div>

  </aside>





<aside class="related" role="complementary">
  <h2 class="hr">Related Posts</h2>

  <ul class="related-posts">
    
      
      
      
        
        
          


<li class="h4">
  <a href="/paper/2018/10/14/arxiv_new/" data-flip="title">
    <span>Arxiv New 2018.10.14</span>
  </a>
  <small><time datetime="2018-10-14T00:00:00+09:00">
    14 Oct 2018
  </time></small>
</li>

        
      
        
        
          


<li class="h4">
  <a href="/paper/2018/07/29/nkn/" data-flip="title">
    <span>Neural Kinematic Networks for Unsupervised Motion Retargetting</span>
  </a>
  <small><time datetime="2018-07-29T00:00:00+09:00">
    29 Jul 2018
  </time></small>
</li>

        
      
        
        
          


<li class="h4">
  <a href="/paper/2018/07/19/youtube/" data-flip="title">
    <span>Playing hard exploration games by watching YouTube</span>
  </a>
  <small><time datetime="2018-07-19T00:00:00+09:00">
    19 Jul 2018
  </time></small>
</li>

        
      
        
        
      
    
  </ul>
</aside>



      
        <aside class="comments" role="complementary">
  <h2>Comments</h2>
  <hr/>

  <div id="disqus_thread"></div>

  <script>
    !function(s,i){function e(e){var t=s.pageYOffset||i.body.scrollTop;s.DISQUS&&!s._disqusThis&&!s._disqusFirst&&t+s.innerHeight>=s._disqusThreadOffsetTop&&(s._disqusThis=!0,s.DISQUS.reset({reload:!0,config:d}))}var d=function(){this.page.title="VAE Tutorial 2",this.page.identifier="/paper/2018/06/20/vae2",this.page.url="http://localhost:4003/paper/2018/06/20/vae2/"};s._disqusFirst=void 0===s._disqusFirst||s._disqusFirst,s._disqusLoading=void 0!==s._disqusLoading&&s._disqusLoading,s._disqusThis=!1,s._disqusThreadOffsetTop=i.getElementById("disqus_thread").offsetTop,s._disqusLoading?s._disqusFirst=!1:(s._disqusLoading=!0,loadJSDeferred("//dnddnjs.disqus.com/embed.js"),s.addEventListener?s.addEventListener("scroll",e,{passive:!0}):s.attachEvent?s.attachEvent("onscroll",e):s.onscroll=e)}(window,document);

  </script>
  <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
</aside>

      

      <footer>
  <hr/>
  
    <p>© 2018. by Woongwon Lee</p>

  
  <p>
    <code>Powered by <a href="https://dnddnjs.github.io/">dnddnjs</a></code>
  </p>
</footer>

    </main>
    <!--<div class="right-side">
  <div class="ad-first">
    <!--
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <!-- 블로그-상단-모바일
    <ins class="adsbygoogle"
         style="display:inline-block;width:100%;"
         data-ad-client="ca-pub-5105555504153863"
         data-ad-slot="9090558636"
         data-ad-format="auto"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
    -->
  </div>
<br/>
<br/>
  <!--
  <div class="ad-second">
    <!-- 블로그-스카이스크래퍼
    <ins class="adsbygoogle"
         style="display:inline-block;max-width:320px;width:100%;height:600px"
         data-ad-client="ca-pub-5105555504153863"
         data-ad-slot="3646660262"></ins>
    <script>
    (adsbygoogle = window.adsbygoogle || []).push({});
    </script>
    -->
  </div>
</div>

-->
  </div>
  <div id="_yDrawer">
  <div id="_sidebar" class="sidebar">
    <div class="sidebar-bg" style="background-color:#4f86aa;background-image:url(/assets/img/nap.jpg)"></div>
    <header class="sidebar-sticky" role="banner">
      <br/>
      <div class="sidebar-about">
        <h1><a id="_title" href="/">Woongwon Lee</a></h1>
        <p>Machine Learning Engineer</p>

      </div>

      <br/>
      <br/>
      <nav class="sidebar-nav heading" role="navigation">
        <span class="sr-only">Navigation:</span>
<ul>
  

  

  
  
  
  
  
    <li>
      <input type="checkbox" id="list-item-1"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/category/paper/">Paper</a>
       <label class="folder" for="list-item-1">▾</label>
    </div>
     <ul class="list-body">
       
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/paper-rl/">Reinforcement-Learning</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/paper-dl/">Deep-Learning</a>
             </li>
           
         
           
         
           
             <li>
               <a class="sidebar-nav-subitem" href="/tag/paper-thinking/">Thinking</a>
             </li>
           
         
     </ul>
    </li>

  
  
    <li>
      <input type="checkbox" id="list-item-2"/>
      <div  class="list-wrapper">
      <a class="sidebar-nav-item" href="/about/">About</a>
       
    </div>
     <ul class="list-body">
       
           
         
           
         
           
         
           
         
           
         
           
         
           
         
     </ul>
    </li>

  
</ul>

      </nav>
    <br/>
    <br/>
      <div class="sidebar-box">
        
          
  

  

  <img
    src="/assets/img/me.jpeg"
    class="me"
    alt="woongwon lee"
    srcset="/assets/img/me.jpeg 1x,/assets/img/me.jpeg 2x"
    
  />


        
      </div>
      <p>Reinforce Yourself</p>

      
      
        <div class="sidebar-social">
          <span class="sr-only">Social:</span>
<ul>
  
    









<li>
  <a href="https://facebook.com/dnddnjs">
    <span class="icon-facebook" title="Facebook"></span>
    <span class="sr-only">Facebook</span>
  </a>
</li>

  
    









<li>
  <a href="https://github.com/dnddnjs">
    <span class="icon-github" title="GitHub"></span>
    <span class="sr-only">GitHub</span>
  </a>
</li>

  
    









<li>
  <a href="mailto:dnddnjs11@naver.com">
    <span class="icon-mail" title="Email"></span>
    <span class="sr-only">Email</span>
  </a>
</li>

  
</ul>

        </div>
      
    </header>
  </div>
</div>

</div>

<!-- =============== -->
<!-- SCRIPTS         -->
<!-- =============== -->

<script>
  window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
  ga('create', 'UA-120080215-1', 'auto');
  ga('send', 'pageview');
  loadJSDeferred('https://www.google-analytics.com/analytics.js');
</script>





<!--[if gt IE 8]><!---->
<script src="//ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js"></script>
<script>
  WebFont.load({
    
    google: {
      families: 'Lato'.split('|')
    },
    

    custom: {
      families: ['icomoon'],
      urls: ['/assets/icomoon/style.css']
    }
  });
</script>
<!--<![endif]-->


  <!--[if gt IE 9]><!---->
  
  <script>loadJSDeferred('/assets/js/hydejack.js?v=6.4.0');</script>

  
  <!--<![endif]-->



</body>

</html>
